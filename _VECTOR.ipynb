{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "## Options:\n",
    "### Pretrained model:\n",
    "- word2vec-ruscorpora-300\t\n",
    "- glove-twitter-200\n",
    "\n",
    "### Tokenization: How to manage OOV?\n",
    "- Link: '&'\n",
    "- Tag: '#'\n",
    "- Mention: '@' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collective Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "from nltk.corpus import stopwords as stpdfa\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import gensim\n",
    "import gensim.models.word2vec as w2v\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load('glove-twitter-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(line):\n",
    "    words = []\n",
    "    for word in line:  # line - iterable, for example list of tokens\n",
    "        try:\n",
    "            w2v_idx = w2v_indices[word]\n",
    "        except KeyError:  # if you does not have a vector for this word in your w2v model, continue\n",
    "            words.append(list(np.zeros(200,)))\n",
    "            continue\n",
    "        words.append(list(w2v_vectors[w2v_idx]))\n",
    "        if not word:\n",
    "            words.append(None)\n",
    "\n",
    "        if len(line) > len(words):\n",
    "            continue\n",
    "    return np.asarray(words)\n",
    "\n",
    "\n",
    "def get_W2V_AVG(raw_data):\n",
    "    tweet_tokenizer = TweetTokenizer()\n",
    "    tweet_tokens = []\n",
    "\n",
    "    for sent in raw_data.text:\n",
    "        sent = re.sub(r\"http\\S+\", \"&\", sent)\n",
    "        sent = re.sub(r\"@\\S+\", \"@\", sent)\n",
    "        sent = re.sub(r\"#\\S+\", \"#\", sent)\n",
    "        sent = re.sub(r'([^\\s\\w@#&]|_)+', '', sent)\n",
    "        sent = re.sub('', '', sent.lower())\n",
    "        # print(tweet_tokenizer.tokenize(sent))\n",
    "        sent = [tweet_tokenizer.tokenize(sent)]\n",
    "        # sent = [tweet_tokenizer.tokenize(sent.lower())]\n",
    "        tweet_tokens.append(sent)\n",
    "        # tweet_tokens.append(tweet_tokenizer.tokenize(sent))\n",
    "    df_tokens = pd.DataFrame(tweet_tokens, columns=['token'])\n",
    "    df_tokens['token_vec'] = copy.deepcopy(df_tokens['token'])\n",
    "\n",
    "    for index, sent in enumerate(df_tokens['token_vec']):\n",
    "        df_tokens['token_vec'][index] = vectorize(sent).mean(axis=0)\n",
    "\n",
    "    # df_test[['text_token','text_token_vec']].head()\n",
    "\n",
    "    df_temp = pd.DataFrame(\n",
    "        df_tokens['token_vec'].values.tolist()).add_prefix('vec_avg')\n",
    "\n",
    "    df_tokens = df_tokens.join(df_temp).drop('token_vec', axis=1)\n",
    "    # df_test.drop('text_token_vec',axis=1, inplace=True)\n",
    "    return df_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"./data/_PHEME_text.csv\")\n",
    "raw_RHI = pd.read_csv(\"./data/_RHS_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHEME_W2V_AVG = get_W2V_AVG(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"./data/_PHEME_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5802, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>Event</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1080</th>\n      <td>#ICYMI, Christopher Hitchens on the case for m...</td>\n      <td>charliehebdo</td>\n    </tr>\n    <tr>\n      <th>4120</th>\n      <td>DEVELOPING NEWS: Soldier shot at War Memorial....</td>\n      <td>ottawashooting</td>\n    </tr>\n    <tr>\n      <th>5004</th>\n      <td>RT @tomsteinfort: Terrifying photo of hostages...</td>\n      <td>sydneysiege</td>\n    </tr>\n    <tr>\n      <th>2305</th>\n      <td>#Ferguson chief said the officer was unaware o...</td>\n      <td>ferguson</td>\n    </tr>\n    <tr>\n      <th>4898</th>\n      <td>BREAKING: 2 people have run out of Sydney buil...</td>\n      <td>sydneysiege</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                                   text           Event\n1080  #ICYMI, Christopher Hitchens on the case for m...    charliehebdo\n4120  DEVELOPING NEWS: Soldier shot at War Memorial....  ottawashooting\n5004  RT @tomsteinfort: Terrifying photo of hostages...     sydneysiege\n2305  #Ferguson chief said the officer was unaware o...        ferguson\n4898  BREAKING: 2 people have run out of Sydney buil...     sydneysiege"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(raw_data.shape)\n",
    "raw_data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = []\n",
    "\n",
    "for sent in raw_data.text:\n",
    "    sent = re.sub(r\"http\\S+\", \"&\", sent)\n",
    "    sent = re.sub(r\"@\\S+\", \"@\", sent)\n",
    "    sent = re.sub(r\"#\\S+\", \"#\", sent)\n",
    "    sent = re.sub(r'([^\\s\\w@#&]|_)+','', sent)\n",
    "    sent = re.sub('','', sent.lower())\n",
    "    # print(tweet_tokenizer.tokenize(sent))\n",
    "    sent = [tweet_tokenizer.tokenize(sent)]\n",
    "    # sent = [tweet_tokenizer.tokenize(sent.lower())]\n",
    "    tweet_tokens.append(sent)\n",
    "    # tweet_tokens.append(tweet_tokenizer.tokenize(sent))\n",
    "df_tokens = pd.DataFrame(tweet_tokens, columns=['token'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[breaking, armed, man, takes, hostage, in, kosher, grocery, east, of, paris, &amp;]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[#, killers, dead, confirmed, by, gendarmerie]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[top, french, cartoonists, charb, cabu, wolinski, tignous, confirmed, among, dead, in, #, #, attack, editor, is, critically, wounded]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[police, have, surrounded, the, area, where, the, #, attack, suspects, are, believed, to, be, &amp;, &amp;]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[photo, armed, gunmen, face, police, officers, near, #, hq, in, paris, &amp;, &amp;]</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                                                                                                                   token\n0                                                        [breaking, armed, man, takes, hostage, in, kosher, grocery, east, of, paris, &]\n1                                                                                         [#, killers, dead, confirmed, by, gendarmerie]\n2  [top, french, cartoonists, charb, cabu, wolinski, tignous, confirmed, among, dead, in, #, #, attack, editor, is, critically, wounded]\n3                                    [police, have, surrounded, the, area, where, the, #, attack, suspects, are, believed, to, be, &, &]\n4                                                           [photo, armed, gunmen, face, police, officers, near, #, hq, in, paris, &, &]"
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Pretrained model for Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching pretrained Model and Convert the Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "# corpus = api.load('text8')\n",
    "# wv = api.load('word2vec-google-news-300')\n",
    "# fasttext-wiki-news-subwords-300'\n",
    "#  'glove-twitter-200',\n",
    "model = api.load('glove-twitter-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7fa0739797f0>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import inspect\n",
    "# print(inspect.getsource(wv.__class__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_object = model.wv\n",
    "w2v_vectors = w2v_object.vectors # here you load vectors for each word in your model\n",
    "w2v_indices = {word: w2v_object.vocab[word].index for word in w2v_object.vocab} # here you load indices - with whom you can find an index of the particular word in your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(line): \n",
    "    words = []\n",
    "    for word in line: # line - iterable, for example list of tokens \n",
    "        try:\n",
    "            w2v_idx = w2v_indices[word]\n",
    "        except KeyError: # if you does not have a vector for this word in your w2v model, continue \n",
    "            words.append(list(np.zeros(200,)))\n",
    "            continue\n",
    "        words.append(list(w2v_vectors[w2v_idx]))\n",
    "        if not word:\n",
    "            words.append(None)\n",
    "\n",
    "        if len(line) > len(words):\n",
    "            continue\n",
    "    return np.asarray(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Tweet 1: \", raw_data['text'][1])\n",
    "# print(\"Indice of '{}': {}\".format(df_tokens['token'][1][0], w2v_indices[df_tokens['token'][1][0]]))\n",
    "# # print(\"Indice of '{}': {}\".format(raw_data['text_token'][1][0], w2v_vectors[w2v_indices[raw_data['text_token'][1][0]]]))\n",
    "# # print(\"Indice of '{}': {}\".format(raw_data['text_token'][1][1], w2v_indices[raw_data['text_token'][1][1]]))\n",
    "# # print(\"Indice of '{}': {}\".format(raw_data['text_token'][1][1], w2v_vectors[w2v_indices[raw_data['text_token'][1][1]]]))\n",
    "# # print(\"\\nVector of the first headline:\\n\", vectorize(raw_data['text_token'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "df_tokens['token_vec'] = copy.deepcopy(df_tokens['token'])\n",
    "\n",
    "for index, sent in enumerate(df_tokens['token_vec']):\n",
    "    df_tokens['token_vec'][index] = vectorize(sent).mean(axis=0)\n",
    "\n",
    "# df_test[['text_token','text_token_vec']].head()\n",
    "\n",
    "df_temp = pd.DataFrame(df_tokens['token_vec'].values.tolist()).add_prefix('vec_avg')\n",
    "\n",
    "df_tokens = df_tokens.join(df_temp).drop('token_vec',axis=1)\n",
    "# df_test.drop('text_token_vec',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token</th>\n      <th>vec_avg0</th>\n      <th>vec_avg1</th>\n      <th>vec_avg2</th>\n      <th>vec_avg3</th>\n      <th>vec_avg4</th>\n      <th>vec_avg5</th>\n      <th>vec_avg6</th>\n      <th>vec_avg7</th>\n      <th>vec_avg8</th>\n      <th>vec_avg9</th>\n      <th>vec_avg10</th>\n      <th>vec_avg11</th>\n      <th>vec_avg12</th>\n      <th>vec_avg13</th>\n      <th>vec_avg14</th>\n      <th>vec_avg15</th>\n      <th>vec_avg16</th>\n      <th>vec_avg17</th>\n      <th>vec_avg18</th>\n      <th>vec_avg19</th>\n      <th>vec_avg20</th>\n      <th>vec_avg21</th>\n      <th>vec_avg22</th>\n      <th>vec_avg23</th>\n      <th>vec_avg24</th>\n      <th>vec_avg25</th>\n      <th>vec_avg26</th>\n      <th>vec_avg27</th>\n      <th>vec_avg28</th>\n      <th>vec_avg29</th>\n      <th>vec_avg30</th>\n      <th>vec_avg31</th>\n      <th>vec_avg32</th>\n      <th>vec_avg33</th>\n      <th>vec_avg34</th>\n      <th>vec_avg35</th>\n      <th>vec_avg36</th>\n      <th>vec_avg37</th>\n      <th>vec_avg38</th>\n      <th>vec_avg39</th>\n      <th>vec_avg40</th>\n      <th>vec_avg41</th>\n      <th>vec_avg42</th>\n      <th>vec_avg43</th>\n      <th>vec_avg44</th>\n      <th>vec_avg45</th>\n      <th>vec_avg46</th>\n      <th>vec_avg47</th>\n      <th>vec_avg48</th>\n      <th>vec_avg49</th>\n      <th>vec_avg50</th>\n      <th>vec_avg51</th>\n      <th>vec_avg52</th>\n      <th>vec_avg53</th>\n      <th>vec_avg54</th>\n      <th>vec_avg55</th>\n      <th>vec_avg56</th>\n      <th>vec_avg57</th>\n      <th>vec_avg58</th>\n      <th>vec_avg59</th>\n      <th>vec_avg60</th>\n      <th>vec_avg61</th>\n      <th>vec_avg62</th>\n      <th>vec_avg63</th>\n      <th>vec_avg64</th>\n      <th>vec_avg65</th>\n      <th>vec_avg66</th>\n      <th>vec_avg67</th>\n      <th>vec_avg68</th>\n      <th>vec_avg69</th>\n      <th>vec_avg70</th>\n      <th>vec_avg71</th>\n      <th>vec_avg72</th>\n      <th>vec_avg73</th>\n      <th>vec_avg74</th>\n      <th>vec_avg75</th>\n      <th>vec_avg76</th>\n      <th>vec_avg77</th>\n      <th>vec_avg78</th>\n      <th>vec_avg79</th>\n      <th>vec_avg80</th>\n      <th>vec_avg81</th>\n      <th>vec_avg82</th>\n      <th>vec_avg83</th>\n      <th>vec_avg84</th>\n      <th>vec_avg85</th>\n      <th>vec_avg86</th>\n      <th>vec_avg87</th>\n      <th>vec_avg88</th>\n      <th>vec_avg89</th>\n      <th>vec_avg90</th>\n      <th>vec_avg91</th>\n      <th>vec_avg92</th>\n      <th>vec_avg93</th>\n      <th>vec_avg94</th>\n      <th>vec_avg95</th>\n      <th>vec_avg96</th>\n      <th>vec_avg97</th>\n      <th>vec_avg98</th>\n      <th>vec_avg99</th>\n      <th>vec_avg100</th>\n      <th>vec_avg101</th>\n      <th>vec_avg102</th>\n      <th>vec_avg103</th>\n      <th>vec_avg104</th>\n      <th>vec_avg105</th>\n      <th>vec_avg106</th>\n      <th>vec_avg107</th>\n      <th>vec_avg108</th>\n      <th>vec_avg109</th>\n      <th>vec_avg110</th>\n      <th>vec_avg111</th>\n      <th>vec_avg112</th>\n      <th>vec_avg113</th>\n      <th>vec_avg114</th>\n      <th>vec_avg115</th>\n      <th>vec_avg116</th>\n      <th>vec_avg117</th>\n      <th>vec_avg118</th>\n      <th>vec_avg119</th>\n      <th>vec_avg120</th>\n      <th>vec_avg121</th>\n      <th>vec_avg122</th>\n      <th>vec_avg123</th>\n      <th>vec_avg124</th>\n      <th>vec_avg125</th>\n      <th>vec_avg126</th>\n      <th>vec_avg127</th>\n      <th>vec_avg128</th>\n      <th>vec_avg129</th>\n      <th>vec_avg130</th>\n      <th>vec_avg131</th>\n      <th>vec_avg132</th>\n      <th>vec_avg133</th>\n      <th>vec_avg134</th>\n      <th>vec_avg135</th>\n      <th>vec_avg136</th>\n      <th>vec_avg137</th>\n      <th>vec_avg138</th>\n      <th>vec_avg139</th>\n      <th>vec_avg140</th>\n      <th>vec_avg141</th>\n      <th>vec_avg142</th>\n      <th>vec_avg143</th>\n      <th>vec_avg144</th>\n      <th>vec_avg145</th>\n      <th>vec_avg146</th>\n      <th>vec_avg147</th>\n      <th>vec_avg148</th>\n      <th>vec_avg149</th>\n      <th>vec_avg150</th>\n      <th>vec_avg151</th>\n      <th>vec_avg152</th>\n      <th>vec_avg153</th>\n      <th>vec_avg154</th>\n      <th>vec_avg155</th>\n      <th>vec_avg156</th>\n      <th>vec_avg157</th>\n      <th>vec_avg158</th>\n      <th>vec_avg159</th>\n      <th>vec_avg160</th>\n      <th>vec_avg161</th>\n      <th>vec_avg162</th>\n      <th>vec_avg163</th>\n      <th>vec_avg164</th>\n      <th>vec_avg165</th>\n      <th>vec_avg166</th>\n      <th>vec_avg167</th>\n      <th>vec_avg168</th>\n      <th>vec_avg169</th>\n      <th>vec_avg170</th>\n      <th>vec_avg171</th>\n      <th>vec_avg172</th>\n      <th>vec_avg173</th>\n      <th>vec_avg174</th>\n      <th>vec_avg175</th>\n      <th>vec_avg176</th>\n      <th>vec_avg177</th>\n      <th>vec_avg178</th>\n      <th>vec_avg179</th>\n      <th>vec_avg180</th>\n      <th>vec_avg181</th>\n      <th>vec_avg182</th>\n      <th>vec_avg183</th>\n      <th>vec_avg184</th>\n      <th>vec_avg185</th>\n      <th>vec_avg186</th>\n      <th>vec_avg187</th>\n      <th>vec_avg188</th>\n      <th>vec_avg189</th>\n      <th>vec_avg190</th>\n      <th>vec_avg191</th>\n      <th>vec_avg192</th>\n      <th>vec_avg193</th>\n      <th>vec_avg194</th>\n      <th>vec_avg195</th>\n      <th>vec_avg196</th>\n      <th>vec_avg197</th>\n      <th>vec_avg198</th>\n      <th>vec_avg199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[breaking, armed, man, takes, hostage, in, kosher, grocery, east, of, paris, &amp;]</td>\n      <td>-0.200159</td>\n      <td>-0.022104</td>\n      <td>0.169316</td>\n      <td>0.162104</td>\n      <td>-0.079669</td>\n      <td>-0.067645</td>\n      <td>0.134522</td>\n      <td>-0.100513</td>\n      <td>0.025423</td>\n      <td>-0.129884</td>\n      <td>-0.088205</td>\n      <td>0.168558</td>\n      <td>-0.766855</td>\n      <td>0.259602</td>\n      <td>0.113695</td>\n      <td>-0.058642</td>\n      <td>0.080902</td>\n      <td>-0.153879</td>\n      <td>-0.066224</td>\n      <td>-0.192514</td>\n      <td>0.070593</td>\n      <td>-0.136879</td>\n      <td>-0.091076</td>\n      <td>-0.039435</td>\n      <td>0.044066</td>\n      <td>0.837574</td>\n      <td>0.195193</td>\n      <td>-0.062630</td>\n      <td>-0.134976</td>\n      <td>-0.005924</td>\n      <td>-0.172016</td>\n      <td>-0.142005</td>\n      <td>0.004670</td>\n      <td>-0.190249</td>\n      <td>0.158722</td>\n      <td>0.286581</td>\n      <td>-0.055702</td>\n      <td>0.136685</td>\n      <td>0.286701</td>\n      <td>-0.113899</td>\n      <td>0.078847</td>\n      <td>-0.054876</td>\n      <td>-0.010237</td>\n      <td>-0.012731</td>\n      <td>0.089428</td>\n      <td>0.088139</td>\n      <td>0.042980</td>\n      <td>-0.051393</td>\n      <td>-0.054021</td>\n      <td>-0.008542</td>\n      <td>-0.460845</td>\n      <td>-0.054243</td>\n      <td>-0.056661</td>\n      <td>0.030779</td>\n      <td>-0.312091</td>\n      <td>0.075869</td>\n      <td>-0.159379</td>\n      <td>-0.002325</td>\n      <td>-0.121169</td>\n      <td>0.090237</td>\n      <td>0.037765</td>\n      <td>0.096103</td>\n      <td>-0.017929</td>\n      <td>-0.071445</td>\n      <td>0.214892</td>\n      <td>0.046556</td>\n      <td>0.197175</td>\n      <td>0.061560</td>\n      <td>-0.024521</td>\n      <td>-0.056524</td>\n      <td>-0.190097</td>\n      <td>-0.023057</td>\n      <td>0.099621</td>\n      <td>0.064687</td>\n      <td>0.193431</td>\n      <td>0.111219</td>\n      <td>-0.071651</td>\n      <td>-0.083748</td>\n      <td>-0.017425</td>\n      <td>-0.042494</td>\n      <td>0.402205</td>\n      <td>-0.050672</td>\n      <td>0.189620</td>\n      <td>0.146298</td>\n      <td>-0.077849</td>\n      <td>-0.114381</td>\n      <td>-0.103333</td>\n      <td>0.153981</td>\n      <td>-0.006182</td>\n      <td>-0.100918</td>\n      <td>-0.001299</td>\n      <td>-0.060657</td>\n      <td>0.167663</td>\n      <td>0.015330</td>\n      <td>-0.077277</td>\n      <td>0.054065</td>\n      <td>0.090626</td>\n      <td>-0.056983</td>\n      <td>0.122849</td>\n      <td>0.071176</td>\n      <td>0.229443</td>\n      <td>-0.101146</td>\n      <td>-0.038592</td>\n      <td>-0.199690</td>\n      <td>0.206279</td>\n      <td>-0.309849</td>\n      <td>0.093074</td>\n      <td>0.038219</td>\n      <td>-0.019591</td>\n      <td>-0.223114</td>\n      <td>0.124421</td>\n      <td>-0.049038</td>\n      <td>0.065194</td>\n      <td>-0.059252</td>\n      <td>0.020795</td>\n      <td>-0.216016</td>\n      <td>-0.058488</td>\n      <td>0.106349</td>\n      <td>-0.200736</td>\n      <td>0.281002</td>\n      <td>0.012990</td>\n      <td>-0.030815</td>\n      <td>-0.038219</td>\n      <td>-0.002121</td>\n      <td>-0.177230</td>\n      <td>-0.099608</td>\n      <td>0.058361</td>\n      <td>0.116085</td>\n      <td>0.389631</td>\n      <td>0.341496</td>\n      <td>-0.210511</td>\n      <td>0.076919</td>\n      <td>-0.145283</td>\n      <td>-0.323431</td>\n      <td>0.018985</td>\n      <td>-0.262398</td>\n      <td>0.133821</td>\n      <td>0.083993</td>\n      <td>0.109703</td>\n      <td>-0.085828</td>\n      <td>0.019022</td>\n      <td>0.165334</td>\n      <td>-0.211419</td>\n      <td>-0.107355</td>\n      <td>-0.183889</td>\n      <td>0.018511</td>\n      <td>-0.025599</td>\n      <td>-0.037882</td>\n      <td>-0.033412</td>\n      <td>0.171158</td>\n      <td>0.078859</td>\n      <td>-0.005609</td>\n      <td>-3.652662</td>\n      <td>-0.108474</td>\n      <td>0.244953</td>\n      <td>-0.066086</td>\n      <td>0.015625</td>\n      <td>0.100949</td>\n      <td>0.034245</td>\n      <td>-0.002728</td>\n      <td>-0.086317</td>\n      <td>0.241056</td>\n      <td>-0.008944</td>\n      <td>0.000277</td>\n      <td>0.242234</td>\n      <td>-0.132559</td>\n      <td>-0.143875</td>\n      <td>0.003110</td>\n      <td>0.350015</td>\n      <td>-0.237144</td>\n      <td>-0.169752</td>\n      <td>0.072902</td>\n      <td>0.052614</td>\n      <td>0.212688</td>\n      <td>-0.025674</td>\n      <td>-0.455150</td>\n      <td>0.175094</td>\n      <td>-0.065420</td>\n      <td>-0.309130</td>\n      <td>0.095305</td>\n      <td>-0.082957</td>\n      <td>-0.135087</td>\n      <td>-0.187487</td>\n      <td>0.070619</td>\n      <td>0.023726</td>\n      <td>0.096496</td>\n      <td>0.136371</td>\n      <td>-0.261459</td>\n      <td>0.068401</td>\n      <td>0.110441</td>\n      <td>0.053726</td>\n      <td>-0.198940</td>\n      <td>0.047774</td>\n      <td>-0.053581</td>\n      <td>-0.094088</td>\n      <td>-0.061137</td>\n      <td>0.137244</td>\n      <td>-0.009833</td>\n      <td>0.229133</td>\n      <td>-0.072766</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[#, killers, dead, confirmed, by, gendarmerie]</td>\n      <td>-0.291447</td>\n      <td>-0.047147</td>\n      <td>-0.267108</td>\n      <td>0.092101</td>\n      <td>-0.037705</td>\n      <td>0.023588</td>\n      <td>-0.052686</td>\n      <td>0.091274</td>\n      <td>0.292993</td>\n      <td>-0.009616</td>\n      <td>0.221990</td>\n      <td>0.189858</td>\n      <td>-0.411915</td>\n      <td>-0.186630</td>\n      <td>0.049495</td>\n      <td>-0.117179</td>\n      <td>-0.095865</td>\n      <td>-0.085227</td>\n      <td>0.099359</td>\n      <td>0.086961</td>\n      <td>0.051027</td>\n      <td>-0.091937</td>\n      <td>-0.075948</td>\n      <td>-0.090393</td>\n      <td>0.268013</td>\n      <td>0.371418</td>\n      <td>-0.149622</td>\n      <td>-0.076155</td>\n      <td>0.170415</td>\n      <td>0.160535</td>\n      <td>0.049072</td>\n      <td>-0.181991</td>\n      <td>-0.316626</td>\n      <td>-0.106580</td>\n      <td>0.040932</td>\n      <td>0.023259</td>\n      <td>-0.103865</td>\n      <td>-0.069906</td>\n      <td>-0.014556</td>\n      <td>-0.134547</td>\n      <td>0.102421</td>\n      <td>0.118352</td>\n      <td>0.207198</td>\n      <td>0.038608</td>\n      <td>-0.010468</td>\n      <td>0.165599</td>\n      <td>-0.199971</td>\n      <td>-0.110026</td>\n      <td>0.154007</td>\n      <td>0.211806</td>\n      <td>-0.347255</td>\n      <td>0.122397</td>\n      <td>0.028775</td>\n      <td>0.126227</td>\n      <td>0.025135</td>\n      <td>0.094557</td>\n      <td>-0.019438</td>\n      <td>-0.172741</td>\n      <td>0.008326</td>\n      <td>0.007106</td>\n      <td>-0.083359</td>\n      <td>0.331628</td>\n      <td>-0.123738</td>\n      <td>0.106306</td>\n      <td>-0.236729</td>\n      <td>-0.049108</td>\n      <td>0.151732</td>\n      <td>0.343700</td>\n      <td>0.036511</td>\n      <td>0.057186</td>\n      <td>0.176933</td>\n      <td>0.038154</td>\n      <td>-0.027135</td>\n      <td>-0.118448</td>\n      <td>0.174952</td>\n      <td>0.137749</td>\n      <td>-0.553584</td>\n      <td>0.072005</td>\n      <td>-0.121769</td>\n      <td>-0.135583</td>\n      <td>0.366000</td>\n      <td>-0.234510</td>\n      <td>0.186655</td>\n      <td>0.271335</td>\n      <td>0.030054</td>\n      <td>-0.072998</td>\n      <td>0.003283</td>\n      <td>0.150198</td>\n      <td>0.031441</td>\n      <td>0.012118</td>\n      <td>0.047725</td>\n      <td>0.119394</td>\n      <td>-0.038298</td>\n      <td>0.091326</td>\n      <td>-0.191887</td>\n      <td>0.311467</td>\n      <td>-0.159037</td>\n      <td>0.102393</td>\n      <td>0.265661</td>\n      <td>0.096408</td>\n      <td>0.261859</td>\n      <td>-0.002567</td>\n      <td>-0.108688</td>\n      <td>-0.057877</td>\n      <td>0.218483</td>\n      <td>0.036741</td>\n      <td>0.190465</td>\n      <td>-0.097270</td>\n      <td>0.025708</td>\n      <td>-0.055757</td>\n      <td>-0.057588</td>\n      <td>0.110868</td>\n      <td>0.032927</td>\n      <td>-0.053181</td>\n      <td>-0.086862</td>\n      <td>0.061490</td>\n      <td>0.038102</td>\n      <td>-0.170783</td>\n      <td>-0.002632</td>\n      <td>0.469709</td>\n      <td>0.305140</td>\n      <td>-0.170614</td>\n      <td>0.291319</td>\n      <td>-0.093086</td>\n      <td>0.051555</td>\n      <td>-0.037307</td>\n      <td>-0.011772</td>\n      <td>0.043757</td>\n      <td>0.351159</td>\n      <td>0.511241</td>\n      <td>0.152218</td>\n      <td>0.375050</td>\n      <td>-0.122599</td>\n      <td>0.098447</td>\n      <td>0.149115</td>\n      <td>-0.227462</td>\n      <td>0.044728</td>\n      <td>-0.126458</td>\n      <td>0.194042</td>\n      <td>-0.000292</td>\n      <td>0.064712</td>\n      <td>0.202020</td>\n      <td>-0.190563</td>\n      <td>-0.073598</td>\n      <td>0.237187</td>\n      <td>-0.084508</td>\n      <td>-0.358217</td>\n      <td>0.076222</td>\n      <td>-0.256618</td>\n      <td>0.117232</td>\n      <td>-0.196861</td>\n      <td>0.026854</td>\n      <td>-2.962860</td>\n      <td>-0.518403</td>\n      <td>-0.188975</td>\n      <td>-0.074493</td>\n      <td>0.002392</td>\n      <td>-0.076053</td>\n      <td>-0.079644</td>\n      <td>-0.311332</td>\n      <td>0.173143</td>\n      <td>-0.123788</td>\n      <td>-0.040849</td>\n      <td>-0.246912</td>\n      <td>0.100828</td>\n      <td>-0.060143</td>\n      <td>-0.036658</td>\n      <td>-0.203946</td>\n      <td>-0.017081</td>\n      <td>-0.254058</td>\n      <td>-0.097048</td>\n      <td>0.254555</td>\n      <td>0.183961</td>\n      <td>0.065330</td>\n      <td>0.117125</td>\n      <td>-0.010142</td>\n      <td>0.184769</td>\n      <td>-0.051489</td>\n      <td>-0.022970</td>\n      <td>-0.104466</td>\n      <td>0.363580</td>\n      <td>-0.183098</td>\n      <td>-0.056571</td>\n      <td>0.200610</td>\n      <td>-0.154833</td>\n      <td>0.275617</td>\n      <td>0.123637</td>\n      <td>-0.330188</td>\n      <td>0.113578</td>\n      <td>-0.025138</td>\n      <td>-0.033641</td>\n      <td>-0.027175</td>\n      <td>0.012455</td>\n      <td>0.155015</td>\n      <td>-0.060791</td>\n      <td>-0.054061</td>\n      <td>-0.175392</td>\n      <td>-0.169289</td>\n      <td>0.116426</td>\n      <td>-0.062778</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[top, french, cartoonists, charb, cabu, wolinski, tignous, confirmed, among, dead, in, #, #, attack, editor, is, critically, wounded]</td>\n      <td>-0.124081</td>\n      <td>-0.137474</td>\n      <td>0.030006</td>\n      <td>-0.013569</td>\n      <td>0.016013</td>\n      <td>-0.125182</td>\n      <td>0.008073</td>\n      <td>0.061128</td>\n      <td>0.217008</td>\n      <td>-0.239182</td>\n      <td>0.115239</td>\n      <td>0.017089</td>\n      <td>-0.394143</td>\n      <td>-0.023061</td>\n      <td>0.069772</td>\n      <td>-0.186543</td>\n      <td>0.036199</td>\n      <td>-0.115991</td>\n      <td>0.045852</td>\n      <td>-0.143186</td>\n      <td>-0.006309</td>\n      <td>-0.003129</td>\n      <td>-0.042088</td>\n      <td>-0.018240</td>\n      <td>0.114085</td>\n      <td>0.454470</td>\n      <td>-0.045393</td>\n      <td>-0.008617</td>\n      <td>0.022398</td>\n      <td>0.273532</td>\n      <td>0.121448</td>\n      <td>-0.043538</td>\n      <td>-0.092379</td>\n      <td>-0.171287</td>\n      <td>0.140103</td>\n      <td>-0.024756</td>\n      <td>0.048007</td>\n      <td>0.091645</td>\n      <td>0.081164</td>\n      <td>-0.066025</td>\n      <td>0.163180</td>\n      <td>-0.074186</td>\n      <td>0.196179</td>\n      <td>0.070356</td>\n      <td>-0.018209</td>\n      <td>0.004733</td>\n      <td>0.004330</td>\n      <td>-0.047791</td>\n      <td>0.020418</td>\n      <td>-0.085431</td>\n      <td>-0.208374</td>\n      <td>0.123452</td>\n      <td>-0.005033</td>\n      <td>0.118297</td>\n      <td>-0.109516</td>\n      <td>0.032269</td>\n      <td>-0.177515</td>\n      <td>0.065345</td>\n      <td>-0.050680</td>\n      <td>-0.112080</td>\n      <td>-0.172437</td>\n      <td>0.074254</td>\n      <td>-0.044654</td>\n      <td>0.052944</td>\n      <td>-0.033867</td>\n      <td>0.084796</td>\n      <td>0.230846</td>\n      <td>0.026173</td>\n      <td>-0.082763</td>\n      <td>-0.123594</td>\n      <td>0.010171</td>\n      <td>-0.074129</td>\n      <td>-0.027996</td>\n      <td>0.002844</td>\n      <td>0.203796</td>\n      <td>0.197250</td>\n      <td>-0.246616</td>\n      <td>-0.043212</td>\n      <td>0.169967</td>\n      <td>-0.056704</td>\n      <td>0.432683</td>\n      <td>0.006495</td>\n      <td>0.196270</td>\n      <td>0.157043</td>\n      <td>-0.046917</td>\n      <td>0.005368</td>\n      <td>0.060184</td>\n      <td>0.126982</td>\n      <td>-0.082208</td>\n      <td>0.039695</td>\n      <td>0.046603</td>\n      <td>0.102262</td>\n      <td>0.027852</td>\n      <td>0.040618</td>\n      <td>0.046582</td>\n      <td>0.076568</td>\n      <td>-0.004826</td>\n      <td>0.096296</td>\n      <td>0.196519</td>\n      <td>0.047819</td>\n      <td>0.229760</td>\n      <td>-0.067784</td>\n      <td>-0.045062</td>\n      <td>-0.039305</td>\n      <td>-0.044017</td>\n      <td>0.020600</td>\n      <td>0.153518</td>\n      <td>-0.008868</td>\n      <td>0.068827</td>\n      <td>-0.004371</td>\n      <td>0.054789</td>\n      <td>0.014423</td>\n      <td>0.113711</td>\n      <td>-0.010962</td>\n      <td>0.004176</td>\n      <td>0.009541</td>\n      <td>0.167206</td>\n      <td>0.037619</td>\n      <td>-0.037014</td>\n      <td>0.144857</td>\n      <td>-0.089358</td>\n      <td>0.087452</td>\n      <td>0.146390</td>\n      <td>-0.121956</td>\n      <td>-0.128659</td>\n      <td>-0.137246</td>\n      <td>-0.161709</td>\n      <td>-0.028931</td>\n      <td>0.304540</td>\n      <td>0.138103</td>\n      <td>-0.064078</td>\n      <td>0.271916</td>\n      <td>-0.027124</td>\n      <td>0.151723</td>\n      <td>-0.046157</td>\n      <td>-0.225742</td>\n      <td>0.097829</td>\n      <td>-0.042130</td>\n      <td>0.016488</td>\n      <td>-0.057565</td>\n      <td>0.105558</td>\n      <td>0.016687</td>\n      <td>-0.099526</td>\n      <td>0.037413</td>\n      <td>-0.002978</td>\n      <td>-0.039033</td>\n      <td>-0.206725</td>\n      <td>0.065943</td>\n      <td>-0.136590</td>\n      <td>0.023886</td>\n      <td>0.013609</td>\n      <td>0.059459</td>\n      <td>-2.477128</td>\n      <td>-0.157780</td>\n      <td>0.031101</td>\n      <td>-0.158550</td>\n      <td>0.104843</td>\n      <td>0.057554</td>\n      <td>0.035800</td>\n      <td>-0.046132</td>\n      <td>0.056592</td>\n      <td>-0.003764</td>\n      <td>-0.154249</td>\n      <td>-0.065640</td>\n      <td>0.072087</td>\n      <td>-0.024396</td>\n      <td>-0.059638</td>\n      <td>-0.065377</td>\n      <td>0.064898</td>\n      <td>-0.095154</td>\n      <td>0.017193</td>\n      <td>0.086545</td>\n      <td>0.146687</td>\n      <td>0.261685</td>\n      <td>0.137251</td>\n      <td>0.026177</td>\n      <td>0.088901</td>\n      <td>-0.022877</td>\n      <td>-0.097679</td>\n      <td>-0.214167</td>\n      <td>0.208215</td>\n      <td>-0.057922</td>\n      <td>0.029581</td>\n      <td>0.009956</td>\n      <td>-0.010301</td>\n      <td>0.268196</td>\n      <td>0.029751</td>\n      <td>-0.167665</td>\n      <td>0.037720</td>\n      <td>-0.131911</td>\n      <td>0.028179</td>\n      <td>-0.228036</td>\n      <td>-0.063890</td>\n      <td>0.109678</td>\n      <td>-0.124332</td>\n      <td>-0.090901</td>\n      <td>0.032937</td>\n      <td>-0.061539</td>\n      <td>0.094249</td>\n      <td>-0.030732</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[police, have, surrounded, the, area, where, the, #, attack, suspects, are, believed, to, be, &amp;, &amp;]</td>\n      <td>0.016080</td>\n      <td>0.078431</td>\n      <td>-0.110218</td>\n      <td>0.303813</td>\n      <td>0.059625</td>\n      <td>0.013453</td>\n      <td>0.314505</td>\n      <td>0.111926</td>\n      <td>0.043341</td>\n      <td>-0.170851</td>\n      <td>-0.022760</td>\n      <td>0.013789</td>\n      <td>-0.899021</td>\n      <td>-0.055298</td>\n      <td>0.126146</td>\n      <td>-0.097923</td>\n      <td>-0.081069</td>\n      <td>-0.156138</td>\n      <td>-0.086749</td>\n      <td>-0.040230</td>\n      <td>0.024305</td>\n      <td>0.013127</td>\n      <td>-0.086284</td>\n      <td>-0.008302</td>\n      <td>0.063472</td>\n      <td>0.681631</td>\n      <td>0.131836</td>\n      <td>0.066302</td>\n      <td>0.154466</td>\n      <td>-0.015468</td>\n      <td>-0.053289</td>\n      <td>-0.135291</td>\n      <td>-0.158850</td>\n      <td>-0.034731</td>\n      <td>0.044537</td>\n      <td>0.067163</td>\n      <td>0.109494</td>\n      <td>0.207618</td>\n      <td>0.048071</td>\n      <td>-0.199204</td>\n      <td>0.203148</td>\n      <td>0.114692</td>\n      <td>0.152150</td>\n      <td>-0.097975</td>\n      <td>0.175243</td>\n      <td>-0.164511</td>\n      <td>0.115373</td>\n      <td>-0.171556</td>\n      <td>0.032598</td>\n      <td>-0.144781</td>\n      <td>-0.288489</td>\n      <td>-0.013903</td>\n      <td>-0.006118</td>\n      <td>0.045020</td>\n      <td>-0.047302</td>\n      <td>-0.004346</td>\n      <td>-0.461157</td>\n      <td>0.086607</td>\n      <td>-0.095087</td>\n      <td>-0.053627</td>\n      <td>-0.141073</td>\n      <td>0.209282</td>\n      <td>-0.062922</td>\n      <td>-0.010459</td>\n      <td>0.280100</td>\n      <td>0.215512</td>\n      <td>-0.033449</td>\n      <td>0.073647</td>\n      <td>-0.108292</td>\n      <td>0.101428</td>\n      <td>0.167875</td>\n      <td>-0.017829</td>\n      <td>0.050450</td>\n      <td>-0.146227</td>\n      <td>0.318351</td>\n      <td>0.211351</td>\n      <td>-0.095372</td>\n      <td>-0.231178</td>\n      <td>-0.153265</td>\n      <td>-0.165001</td>\n      <td>0.630719</td>\n      <td>-0.143387</td>\n      <td>0.229400</td>\n      <td>0.244026</td>\n      <td>0.044495</td>\n      <td>0.016795</td>\n      <td>0.018669</td>\n      <td>0.137908</td>\n      <td>-0.039542</td>\n      <td>0.024099</td>\n      <td>0.045067</td>\n      <td>0.103815</td>\n      <td>0.243002</td>\n      <td>0.020649</td>\n      <td>-0.007903</td>\n      <td>-0.043981</td>\n      <td>-0.150695</td>\n      <td>-0.124405</td>\n      <td>0.083274</td>\n      <td>0.136321</td>\n      <td>0.199454</td>\n      <td>-0.008222</td>\n      <td>-0.089893</td>\n      <td>-0.099319</td>\n      <td>0.233111</td>\n      <td>-0.246459</td>\n      <td>0.276907</td>\n      <td>0.036149</td>\n      <td>-0.113199</td>\n      <td>-0.201612</td>\n      <td>-0.045858</td>\n      <td>-0.058983</td>\n      <td>0.176819</td>\n      <td>-0.091657</td>\n      <td>-0.079428</td>\n      <td>-0.122428</td>\n      <td>-0.154935</td>\n      <td>0.010727</td>\n      <td>0.024021</td>\n      <td>0.321716</td>\n      <td>0.043609</td>\n      <td>-0.122900</td>\n      <td>0.104929</td>\n      <td>0.090259</td>\n      <td>0.002390</td>\n      <td>-0.093069</td>\n      <td>0.053720</td>\n      <td>0.051303</td>\n      <td>0.378969</td>\n      <td>0.209787</td>\n      <td>0.039028</td>\n      <td>0.014369</td>\n      <td>-0.091526</td>\n      <td>-0.022759</td>\n      <td>-0.114298</td>\n      <td>-0.137538</td>\n      <td>0.114162</td>\n      <td>-0.071896</td>\n      <td>0.131572</td>\n      <td>-0.345326</td>\n      <td>0.107361</td>\n      <td>0.117503</td>\n      <td>-0.193023</td>\n      <td>0.191446</td>\n      <td>0.151031</td>\n      <td>0.050976</td>\n      <td>-0.114699</td>\n      <td>-0.115518</td>\n      <td>-0.131883</td>\n      <td>-0.020880</td>\n      <td>0.044928</td>\n      <td>0.288451</td>\n      <td>-4.639482</td>\n      <td>-0.194060</td>\n      <td>-0.029095</td>\n      <td>0.066211</td>\n      <td>0.162330</td>\n      <td>0.046667</td>\n      <td>0.028176</td>\n      <td>0.080879</td>\n      <td>0.073220</td>\n      <td>0.054116</td>\n      <td>-0.196138</td>\n      <td>0.007172</td>\n      <td>-0.089389</td>\n      <td>-0.049007</td>\n      <td>-0.196198</td>\n      <td>-0.102555</td>\n      <td>0.073917</td>\n      <td>-0.056248</td>\n      <td>-0.303220</td>\n      <td>-0.020301</td>\n      <td>-0.023727</td>\n      <td>0.014697</td>\n      <td>0.046984</td>\n      <td>-0.068068</td>\n      <td>0.104679</td>\n      <td>-0.007430</td>\n      <td>-0.152839</td>\n      <td>0.143305</td>\n      <td>0.157657</td>\n      <td>-0.212905</td>\n      <td>-0.145295</td>\n      <td>0.017875</td>\n      <td>-0.058830</td>\n      <td>-0.110899</td>\n      <td>0.035500</td>\n      <td>-0.254278</td>\n      <td>0.011819</td>\n      <td>-0.093445</td>\n      <td>0.054530</td>\n      <td>-0.107643</td>\n      <td>0.124141</td>\n      <td>-0.082329</td>\n      <td>-0.169564</td>\n      <td>-0.157198</td>\n      <td>-0.045242</td>\n      <td>-0.158567</td>\n      <td>-0.078366</td>\n      <td>-0.048285</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[photo, armed, gunmen, face, police, officers, near, #, hq, in, paris, &amp;, &amp;]</td>\n      <td>-0.207173</td>\n      <td>-0.284736</td>\n      <td>-0.180337</td>\n      <td>0.331091</td>\n      <td>0.084479</td>\n      <td>-0.096168</td>\n      <td>-0.030399</td>\n      <td>-0.038830</td>\n      <td>0.247730</td>\n      <td>-0.173032</td>\n      <td>0.029569</td>\n      <td>0.146954</td>\n      <td>-0.431276</td>\n      <td>0.169803</td>\n      <td>0.177460</td>\n      <td>-0.209317</td>\n      <td>0.098112</td>\n      <td>-0.308670</td>\n      <td>-0.062750</td>\n      <td>-0.219135</td>\n      <td>-0.018042</td>\n      <td>-0.201846</td>\n      <td>-0.079689</td>\n      <td>0.015235</td>\n      <td>0.059654</td>\n      <td>0.429946</td>\n      <td>0.097664</td>\n      <td>0.003530</td>\n      <td>0.083342</td>\n      <td>0.110105</td>\n      <td>-0.136993</td>\n      <td>-0.183289</td>\n      <td>-0.075494</td>\n      <td>-0.157378</td>\n      <td>0.136129</td>\n      <td>0.259056</td>\n      <td>-0.066369</td>\n      <td>0.106052</td>\n      <td>0.268468</td>\n      <td>-0.028946</td>\n      <td>-0.030336</td>\n      <td>-0.066822</td>\n      <td>-0.022377</td>\n      <td>0.200566</td>\n      <td>0.014297</td>\n      <td>-0.155254</td>\n      <td>-0.013665</td>\n      <td>0.018132</td>\n      <td>-0.052884</td>\n      <td>-0.197649</td>\n      <td>-0.346654</td>\n      <td>-0.128158</td>\n      <td>-0.098670</td>\n      <td>0.115297</td>\n      <td>-0.216069</td>\n      <td>0.044271</td>\n      <td>-0.361232</td>\n      <td>-0.024501</td>\n      <td>-0.177754</td>\n      <td>-0.015872</td>\n      <td>0.081900</td>\n      <td>0.109551</td>\n      <td>0.095234</td>\n      <td>0.022344</td>\n      <td>0.162784</td>\n      <td>0.132312</td>\n      <td>0.131826</td>\n      <td>0.090832</td>\n      <td>-0.069306</td>\n      <td>-0.031829</td>\n      <td>0.068374</td>\n      <td>-0.126602</td>\n      <td>0.180925</td>\n      <td>0.065657</td>\n      <td>0.167764</td>\n      <td>0.155912</td>\n      <td>-0.227649</td>\n      <td>-0.073613</td>\n      <td>-0.132992</td>\n      <td>-0.190452</td>\n      <td>0.595159</td>\n      <td>-0.205311</td>\n      <td>0.212720</td>\n      <td>0.091716</td>\n      <td>0.076286</td>\n      <td>-0.115902</td>\n      <td>0.018058</td>\n      <td>0.422530</td>\n      <td>-0.006155</td>\n      <td>-0.030718</td>\n      <td>0.015542</td>\n      <td>0.208234</td>\n      <td>0.168398</td>\n      <td>0.124937</td>\n      <td>0.189802</td>\n      <td>0.163881</td>\n      <td>-0.120342</td>\n      <td>-0.058847</td>\n      <td>0.190904</td>\n      <td>0.043595</td>\n      <td>0.237599</td>\n      <td>-0.011928</td>\n      <td>-0.082374</td>\n      <td>-0.143324</td>\n      <td>0.234996</td>\n      <td>-0.172422</td>\n      <td>0.240791</td>\n      <td>-0.012201</td>\n      <td>-0.167034</td>\n      <td>-0.379698</td>\n      <td>0.176999</td>\n      <td>-0.049022</td>\n      <td>-0.082175</td>\n      <td>-0.182909</td>\n      <td>0.084153</td>\n      <td>-0.269685</td>\n      <td>-0.038029</td>\n      <td>-0.186103</td>\n      <td>-0.089783</td>\n      <td>0.438459</td>\n      <td>0.036945</td>\n      <td>-0.195045</td>\n      <td>0.205958</td>\n      <td>0.138946</td>\n      <td>-0.118581</td>\n      <td>-0.027392</td>\n      <td>-0.003957</td>\n      <td>0.217607</td>\n      <td>0.259886</td>\n      <td>0.598758</td>\n      <td>0.098899</td>\n      <td>-0.006222</td>\n      <td>-0.185399</td>\n      <td>-0.026316</td>\n      <td>-0.022326</td>\n      <td>-0.235028</td>\n      <td>0.029866</td>\n      <td>-0.095477</td>\n      <td>-0.020755</td>\n      <td>-0.085532</td>\n      <td>0.128229</td>\n      <td>0.123329</td>\n      <td>-0.128823</td>\n      <td>0.245971</td>\n      <td>0.111507</td>\n      <td>0.082959</td>\n      <td>-0.258967</td>\n      <td>-0.051208</td>\n      <td>0.042649</td>\n      <td>-0.022223</td>\n      <td>0.122469</td>\n      <td>0.170334</td>\n      <td>-3.615378</td>\n      <td>-0.305068</td>\n      <td>0.014872</td>\n      <td>0.013386</td>\n      <td>0.050109</td>\n      <td>0.100291</td>\n      <td>0.123855</td>\n      <td>0.098400</td>\n      <td>0.029230</td>\n      <td>-0.031395</td>\n      <td>-0.030448</td>\n      <td>-0.118268</td>\n      <td>0.012326</td>\n      <td>-0.109288</td>\n      <td>-0.374059</td>\n      <td>-0.426128</td>\n      <td>-0.015599</td>\n      <td>-0.111202</td>\n      <td>-0.350197</td>\n      <td>0.040030</td>\n      <td>-0.052255</td>\n      <td>0.154375</td>\n      <td>0.177052</td>\n      <td>-0.199524</td>\n      <td>0.402980</td>\n      <td>0.205654</td>\n      <td>-0.353022</td>\n      <td>0.043944</td>\n      <td>-0.034190</td>\n      <td>-0.118590</td>\n      <td>-0.065440</td>\n      <td>0.070915</td>\n      <td>-0.125772</td>\n      <td>-0.119960</td>\n      <td>0.005299</td>\n      <td>-0.432786</td>\n      <td>0.207072</td>\n      <td>-0.142483</td>\n      <td>0.151164</td>\n      <td>-0.178209</td>\n      <td>-0.083584</td>\n      <td>0.091386</td>\n      <td>-0.191965</td>\n      <td>-0.063621</td>\n      <td>0.063793</td>\n      <td>-0.429122</td>\n      <td>0.105709</td>\n      <td>-0.134280</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                                                                                                                   token  \\\n0                                                        [breaking, armed, man, takes, hostage, in, kosher, grocery, east, of, paris, &]   \n1                                                                                         [#, killers, dead, confirmed, by, gendarmerie]   \n2  [top, french, cartoonists, charb, cabu, wolinski, tignous, confirmed, among, dead, in, #, #, attack, editor, is, critically, wounded]   \n3                                    [police, have, surrounded, the, area, where, the, #, attack, suspects, are, believed, to, be, &, &]   \n4                                                           [photo, armed, gunmen, face, police, officers, near, #, hq, in, paris, &, &]   \n\n   vec_avg0  vec_avg1  vec_avg2  vec_avg3  vec_avg4  vec_avg5  vec_avg6  \\\n0 -0.200159 -0.022104  0.169316  0.162104 -0.079669 -0.067645  0.134522   \n1 -0.291447 -0.047147 -0.267108  0.092101 -0.037705  0.023588 -0.052686   \n2 -0.124081 -0.137474  0.030006 -0.013569  0.016013 -0.125182  0.008073   \n3  0.016080  0.078431 -0.110218  0.303813  0.059625  0.013453  0.314505   \n4 -0.207173 -0.284736 -0.180337  0.331091  0.084479 -0.096168 -0.030399   \n\n   vec_avg7  vec_avg8  vec_avg9  vec_avg10  vec_avg11  vec_avg12  vec_avg13  \\\n0 -0.100513  0.025423 -0.129884  -0.088205   0.168558  -0.766855   0.259602   \n1  0.091274  0.292993 -0.009616   0.221990   0.189858  -0.411915  -0.186630   \n2  0.061128  0.217008 -0.239182   0.115239   0.017089  -0.394143  -0.023061   \n3  0.111926  0.043341 -0.170851  -0.022760   0.013789  -0.899021  -0.055298   \n4 -0.038830  0.247730 -0.173032   0.029569   0.146954  -0.431276   0.169803   \n\n   vec_avg14  vec_avg15  vec_avg16  vec_avg17  vec_avg18  vec_avg19  \\\n0   0.113695  -0.058642   0.080902  -0.153879  -0.066224  -0.192514   \n1   0.049495  -0.117179  -0.095865  -0.085227   0.099359   0.086961   \n2   0.069772  -0.186543   0.036199  -0.115991   0.045852  -0.143186   \n3   0.126146  -0.097923  -0.081069  -0.156138  -0.086749  -0.040230   \n4   0.177460  -0.209317   0.098112  -0.308670  -0.062750  -0.219135   \n\n   vec_avg20  vec_avg21  vec_avg22  vec_avg23  vec_avg24  vec_avg25  \\\n0   0.070593  -0.136879  -0.091076  -0.039435   0.044066   0.837574   \n1   0.051027  -0.091937  -0.075948  -0.090393   0.268013   0.371418   \n2  -0.006309  -0.003129  -0.042088  -0.018240   0.114085   0.454470   \n3   0.024305   0.013127  -0.086284  -0.008302   0.063472   0.681631   \n4  -0.018042  -0.201846  -0.079689   0.015235   0.059654   0.429946   \n\n   vec_avg26  vec_avg27  vec_avg28  vec_avg29  vec_avg30  vec_avg31  \\\n0   0.195193  -0.062630  -0.134976  -0.005924  -0.172016  -0.142005   \n1  -0.149622  -0.076155   0.170415   0.160535   0.049072  -0.181991   \n2  -0.045393  -0.008617   0.022398   0.273532   0.121448  -0.043538   \n3   0.131836   0.066302   0.154466  -0.015468  -0.053289  -0.135291   \n4   0.097664   0.003530   0.083342   0.110105  -0.136993  -0.183289   \n\n   vec_avg32  vec_avg33  vec_avg34  vec_avg35  vec_avg36  vec_avg37  \\\n0   0.004670  -0.190249   0.158722   0.286581  -0.055702   0.136685   \n1  -0.316626  -0.106580   0.040932   0.023259  -0.103865  -0.069906   \n2  -0.092379  -0.171287   0.140103  -0.024756   0.048007   0.091645   \n3  -0.158850  -0.034731   0.044537   0.067163   0.109494   0.207618   \n4  -0.075494  -0.157378   0.136129   0.259056  -0.066369   0.106052   \n\n   vec_avg38  vec_avg39  vec_avg40  vec_avg41  vec_avg42  vec_avg43  \\\n0   0.286701  -0.113899   0.078847  -0.054876  -0.010237  -0.012731   \n1  -0.014556  -0.134547   0.102421   0.118352   0.207198   0.038608   \n2   0.081164  -0.066025   0.163180  -0.074186   0.196179   0.070356   \n3   0.048071  -0.199204   0.203148   0.114692   0.152150  -0.097975   \n4   0.268468  -0.028946  -0.030336  -0.066822  -0.022377   0.200566   \n\n   vec_avg44  vec_avg45  vec_avg46  vec_avg47  vec_avg48  vec_avg49  \\\n0   0.089428   0.088139   0.042980  -0.051393  -0.054021  -0.008542   \n1  -0.010468   0.165599  -0.199971  -0.110026   0.154007   0.211806   \n2  -0.018209   0.004733   0.004330  -0.047791   0.020418  -0.085431   \n3   0.175243  -0.164511   0.115373  -0.171556   0.032598  -0.144781   \n4   0.014297  -0.155254  -0.013665   0.018132  -0.052884  -0.197649   \n\n   vec_avg50  vec_avg51  vec_avg52  vec_avg53  vec_avg54  vec_avg55  \\\n0  -0.460845  -0.054243  -0.056661   0.030779  -0.312091   0.075869   \n1  -0.347255   0.122397   0.028775   0.126227   0.025135   0.094557   \n2  -0.208374   0.123452  -0.005033   0.118297  -0.109516   0.032269   \n3  -0.288489  -0.013903  -0.006118   0.045020  -0.047302  -0.004346   \n4  -0.346654  -0.128158  -0.098670   0.115297  -0.216069   0.044271   \n\n   vec_avg56  vec_avg57  vec_avg58  vec_avg59  vec_avg60  vec_avg61  \\\n0  -0.159379  -0.002325  -0.121169   0.090237   0.037765   0.096103   \n1  -0.019438  -0.172741   0.008326   0.007106  -0.083359   0.331628   \n2  -0.177515   0.065345  -0.050680  -0.112080  -0.172437   0.074254   \n3  -0.461157   0.086607  -0.095087  -0.053627  -0.141073   0.209282   \n4  -0.361232  -0.024501  -0.177754  -0.015872   0.081900   0.109551   \n\n   vec_avg62  vec_avg63  vec_avg64  vec_avg65  vec_avg66  vec_avg67  \\\n0  -0.017929  -0.071445   0.214892   0.046556   0.197175   0.061560   \n1  -0.123738   0.106306  -0.236729  -0.049108   0.151732   0.343700   \n2  -0.044654   0.052944  -0.033867   0.084796   0.230846   0.026173   \n3  -0.062922  -0.010459   0.280100   0.215512  -0.033449   0.073647   \n4   0.095234   0.022344   0.162784   0.132312   0.131826   0.090832   \n\n   vec_avg68  vec_avg69  vec_avg70  vec_avg71  vec_avg72  vec_avg73  \\\n0  -0.024521  -0.056524  -0.190097  -0.023057   0.099621   0.064687   \n1   0.036511   0.057186   0.176933   0.038154  -0.027135  -0.118448   \n2  -0.082763  -0.123594   0.010171  -0.074129  -0.027996   0.002844   \n3  -0.108292   0.101428   0.167875  -0.017829   0.050450  -0.146227   \n4  -0.069306  -0.031829   0.068374  -0.126602   0.180925   0.065657   \n\n   vec_avg74  vec_avg75  vec_avg76  vec_avg77  vec_avg78  vec_avg79  \\\n0   0.193431   0.111219  -0.071651  -0.083748  -0.017425  -0.042494   \n1   0.174952   0.137749  -0.553584   0.072005  -0.121769  -0.135583   \n2   0.203796   0.197250  -0.246616  -0.043212   0.169967  -0.056704   \n3   0.318351   0.211351  -0.095372  -0.231178  -0.153265  -0.165001   \n4   0.167764   0.155912  -0.227649  -0.073613  -0.132992  -0.190452   \n\n   vec_avg80  vec_avg81  vec_avg82  vec_avg83  vec_avg84  vec_avg85  \\\n0   0.402205  -0.050672   0.189620   0.146298  -0.077849  -0.114381   \n1   0.366000  -0.234510   0.186655   0.271335   0.030054  -0.072998   \n2   0.432683   0.006495   0.196270   0.157043  -0.046917   0.005368   \n3   0.630719  -0.143387   0.229400   0.244026   0.044495   0.016795   \n4   0.595159  -0.205311   0.212720   0.091716   0.076286  -0.115902   \n\n   vec_avg86  vec_avg87  vec_avg88  vec_avg89  vec_avg90  vec_avg91  \\\n0  -0.103333   0.153981  -0.006182  -0.100918  -0.001299  -0.060657   \n1   0.003283   0.150198   0.031441   0.012118   0.047725   0.119394   \n2   0.060184   0.126982  -0.082208   0.039695   0.046603   0.102262   \n3   0.018669   0.137908  -0.039542   0.024099   0.045067   0.103815   \n4   0.018058   0.422530  -0.006155  -0.030718   0.015542   0.208234   \n\n   vec_avg92  vec_avg93  vec_avg94  vec_avg95  vec_avg96  vec_avg97  \\\n0   0.167663   0.015330  -0.077277   0.054065   0.090626  -0.056983   \n1  -0.038298   0.091326  -0.191887   0.311467  -0.159037   0.102393   \n2   0.027852   0.040618   0.046582   0.076568  -0.004826   0.096296   \n3   0.243002   0.020649  -0.007903  -0.043981  -0.150695  -0.124405   \n4   0.168398   0.124937   0.189802   0.163881  -0.120342  -0.058847   \n\n   vec_avg98  vec_avg99  vec_avg100  vec_avg101  vec_avg102  vec_avg103  \\\n0   0.122849   0.071176    0.229443   -0.101146   -0.038592   -0.199690   \n1   0.265661   0.096408    0.261859   -0.002567   -0.108688   -0.057877   \n2   0.196519   0.047819    0.229760   -0.067784   -0.045062   -0.039305   \n3   0.083274   0.136321    0.199454   -0.008222   -0.089893   -0.099319   \n4   0.190904   0.043595    0.237599   -0.011928   -0.082374   -0.143324   \n\n   vec_avg104  vec_avg105  vec_avg106  vec_avg107  vec_avg108  vec_avg109  \\\n0    0.206279   -0.309849    0.093074    0.038219   -0.019591   -0.223114   \n1    0.218483    0.036741    0.190465   -0.097270    0.025708   -0.055757   \n2   -0.044017    0.020600    0.153518   -0.008868    0.068827   -0.004371   \n3    0.233111   -0.246459    0.276907    0.036149   -0.113199   -0.201612   \n4    0.234996   -0.172422    0.240791   -0.012201   -0.167034   -0.379698   \n\n   vec_avg110  vec_avg111  vec_avg112  vec_avg113  vec_avg114  vec_avg115  \\\n0    0.124421   -0.049038    0.065194   -0.059252    0.020795   -0.216016   \n1   -0.057588    0.110868    0.032927   -0.053181   -0.086862    0.061490   \n2    0.054789    0.014423    0.113711   -0.010962    0.004176    0.009541   \n3   -0.045858   -0.058983    0.176819   -0.091657   -0.079428   -0.122428   \n4    0.176999   -0.049022   -0.082175   -0.182909    0.084153   -0.269685   \n\n   vec_avg116  vec_avg117  vec_avg118  vec_avg119  vec_avg120  vec_avg121  \\\n0   -0.058488    0.106349   -0.200736    0.281002    0.012990   -0.030815   \n1    0.038102   -0.170783   -0.002632    0.469709    0.305140   -0.170614   \n2    0.167206    0.037619   -0.037014    0.144857   -0.089358    0.087452   \n3   -0.154935    0.010727    0.024021    0.321716    0.043609   -0.122900   \n4   -0.038029   -0.186103   -0.089783    0.438459    0.036945   -0.195045   \n\n   vec_avg122  vec_avg123  vec_avg124  vec_avg125  vec_avg126  vec_avg127  \\\n0   -0.038219   -0.002121   -0.177230   -0.099608    0.058361    0.116085   \n1    0.291319   -0.093086    0.051555   -0.037307   -0.011772    0.043757   \n2    0.146390   -0.121956   -0.128659   -0.137246   -0.161709   -0.028931   \n3    0.104929    0.090259    0.002390   -0.093069    0.053720    0.051303   \n4    0.205958    0.138946   -0.118581   -0.027392   -0.003957    0.217607   \n\n   vec_avg128  vec_avg129  vec_avg130  vec_avg131  vec_avg132  vec_avg133  \\\n0    0.389631    0.341496   -0.210511    0.076919   -0.145283   -0.323431   \n1    0.351159    0.511241    0.152218    0.375050   -0.122599    0.098447   \n2    0.304540    0.138103   -0.064078    0.271916   -0.027124    0.151723   \n3    0.378969    0.209787    0.039028    0.014369   -0.091526   -0.022759   \n4    0.259886    0.598758    0.098899   -0.006222   -0.185399   -0.026316   \n\n   vec_avg134  vec_avg135  vec_avg136  vec_avg137  vec_avg138  vec_avg139  \\\n0    0.018985   -0.262398    0.133821    0.083993    0.109703   -0.085828   \n1    0.149115   -0.227462    0.044728   -0.126458    0.194042   -0.000292   \n2   -0.046157   -0.225742    0.097829   -0.042130    0.016488   -0.057565   \n3   -0.114298   -0.137538    0.114162   -0.071896    0.131572   -0.345326   \n4   -0.022326   -0.235028    0.029866   -0.095477   -0.020755   -0.085532   \n\n   vec_avg140  vec_avg141  vec_avg142  vec_avg143  vec_avg144  vec_avg145  \\\n0    0.019022    0.165334   -0.211419   -0.107355   -0.183889    0.018511   \n1    0.064712    0.202020   -0.190563   -0.073598    0.237187   -0.084508   \n2    0.105558    0.016687   -0.099526    0.037413   -0.002978   -0.039033   \n3    0.107361    0.117503   -0.193023    0.191446    0.151031    0.050976   \n4    0.128229    0.123329   -0.128823    0.245971    0.111507    0.082959   \n\n   vec_avg146  vec_avg147  vec_avg148  vec_avg149  vec_avg150  vec_avg151  \\\n0   -0.025599   -0.037882   -0.033412    0.171158    0.078859   -0.005609   \n1   -0.358217    0.076222   -0.256618    0.117232   -0.196861    0.026854   \n2   -0.206725    0.065943   -0.136590    0.023886    0.013609    0.059459   \n3   -0.114699   -0.115518   -0.131883   -0.020880    0.044928    0.288451   \n4   -0.258967   -0.051208    0.042649   -0.022223    0.122469    0.170334   \n\n   vec_avg152  vec_avg153  vec_avg154  vec_avg155  vec_avg156  vec_avg157  \\\n0   -3.652662   -0.108474    0.244953   -0.066086    0.015625    0.100949   \n1   -2.962860   -0.518403   -0.188975   -0.074493    0.002392   -0.076053   \n2   -2.477128   -0.157780    0.031101   -0.158550    0.104843    0.057554   \n3   -4.639482   -0.194060   -0.029095    0.066211    0.162330    0.046667   \n4   -3.615378   -0.305068    0.014872    0.013386    0.050109    0.100291   \n\n   vec_avg158  vec_avg159  vec_avg160  vec_avg161  vec_avg162  vec_avg163  \\\n0    0.034245   -0.002728   -0.086317    0.241056   -0.008944    0.000277   \n1   -0.079644   -0.311332    0.173143   -0.123788   -0.040849   -0.246912   \n2    0.035800   -0.046132    0.056592   -0.003764   -0.154249   -0.065640   \n3    0.028176    0.080879    0.073220    0.054116   -0.196138    0.007172   \n4    0.123855    0.098400    0.029230   -0.031395   -0.030448   -0.118268   \n\n   vec_avg164  vec_avg165  vec_avg166  vec_avg167  vec_avg168  vec_avg169  \\\n0    0.242234   -0.132559   -0.143875    0.003110    0.350015   -0.237144   \n1    0.100828   -0.060143   -0.036658   -0.203946   -0.017081   -0.254058   \n2    0.072087   -0.024396   -0.059638   -0.065377    0.064898   -0.095154   \n3   -0.089389   -0.049007   -0.196198   -0.102555    0.073917   -0.056248   \n4    0.012326   -0.109288   -0.374059   -0.426128   -0.015599   -0.111202   \n\n   vec_avg170  vec_avg171  vec_avg172  vec_avg173  vec_avg174  vec_avg175  \\\n0   -0.169752    0.072902    0.052614    0.212688   -0.025674   -0.455150   \n1   -0.097048    0.254555    0.183961    0.065330    0.117125   -0.010142   \n2    0.017193    0.086545    0.146687    0.261685    0.137251    0.026177   \n3   -0.303220   -0.020301   -0.023727    0.014697    0.046984   -0.068068   \n4   -0.350197    0.040030   -0.052255    0.154375    0.177052   -0.199524   \n\n   vec_avg176  vec_avg177  vec_avg178  vec_avg179  vec_avg180  vec_avg181  \\\n0    0.175094   -0.065420   -0.309130    0.095305   -0.082957   -0.135087   \n1    0.184769   -0.051489   -0.022970   -0.104466    0.363580   -0.183098   \n2    0.088901   -0.022877   -0.097679   -0.214167    0.208215   -0.057922   \n3    0.104679   -0.007430   -0.152839    0.143305    0.157657   -0.212905   \n4    0.402980    0.205654   -0.353022    0.043944   -0.034190   -0.118590   \n\n   vec_avg182  vec_avg183  vec_avg184  vec_avg185  vec_avg186  vec_avg187  \\\n0   -0.187487    0.070619    0.023726    0.096496    0.136371   -0.261459   \n1   -0.056571    0.200610   -0.154833    0.275617    0.123637   -0.330188   \n2    0.029581    0.009956   -0.010301    0.268196    0.029751   -0.167665   \n3   -0.145295    0.017875   -0.058830   -0.110899    0.035500   -0.254278   \n4   -0.065440    0.070915   -0.125772   -0.119960    0.005299   -0.432786   \n\n   vec_avg188  vec_avg189  vec_avg190  vec_avg191  vec_avg192  vec_avg193  \\\n0    0.068401    0.110441    0.053726   -0.198940    0.047774   -0.053581   \n1    0.113578   -0.025138   -0.033641   -0.027175    0.012455    0.155015   \n2    0.037720   -0.131911    0.028179   -0.228036   -0.063890    0.109678   \n3    0.011819   -0.093445    0.054530   -0.107643    0.124141   -0.082329   \n4    0.207072   -0.142483    0.151164   -0.178209   -0.083584    0.091386   \n\n   vec_avg194  vec_avg195  vec_avg196  vec_avg197  vec_avg198  vec_avg199  \n0   -0.094088   -0.061137    0.137244   -0.009833    0.229133   -0.072766  \n1   -0.060791   -0.054061   -0.175392   -0.169289    0.116426   -0.062778  \n2   -0.124332   -0.090901    0.032937   -0.061539    0.094249   -0.030732  \n3   -0.169564   -0.157198   -0.045242   -0.158567   -0.078366   -0.048285  \n4   -0.191965   -0.063621    0.063793   -0.429122    0.105709   -0.134280  "
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rosetta",
   "language": "python",
   "name": "rosetta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}