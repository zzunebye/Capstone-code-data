{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzunebye/Capstone-code-data/blob/main/_RumorEval_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5pLteIxD88o",
        "outputId": "696261be-017b-4f19-d295-4b690761b03f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Mounted at ./MyDrive\n"
          ]
        }
      ],
      "source": [
        "# 실행시 등장하는 URL을 클릭하여 허용해주면 인증KEY가 나타난다. 복사하여 URL아래 빈칸에 붙여넣으면 마운트에 성공하게된다.\n",
        "from google.colab import drive\n",
        "drive.mount('./MyDrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSU42bHqFeZ2",
        "outputId": "d4dbf91f-a385-4bae-dff1-243f3e877933"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'MyDrive/MyDrive/Capstone/code_data'\n",
            "/content/MyDrive/MyDrive/Capstone/code_data\n"
          ]
        }
      ],
      "source": [
        "cd MyDrive/MyDrive/Capstone/code_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kzqWy-ubCmUn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from glob2 import glob\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "class data_loader():\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "\n",
        "    def getdata(self, source=True, split=True):\n",
        "        jsons = glob('%s/*/*/*/source-tweet/*.json' % self.path)\n",
        "        print(\"The number of json files are:\", len(jsons))\n",
        "        return [jsons]\n",
        "\n",
        "    def extract_data(self, data):\n",
        "        count = 0  # help var\n",
        "        data_lists = []\n",
        "        isRumorLists = []\n",
        "        for index, dataset in enumerate(data):\n",
        "            data_list = []\n",
        "            isRumorList = []\n",
        "            count = 0  # help var\n",
        "\n",
        "            for jsonFile in dataset:\n",
        "                count += 1\n",
        "                if jsonFile.find(\"non-rumours\") == -1:\n",
        "                    isRumorList.append(1)\n",
        "                else:\n",
        "                    isRumorList.append(0)\n",
        "\n",
        "                with open(jsonFile, 'r') as f:\n",
        "                    for l in f.readlines():\n",
        "                        if not l.strip():  # skip empty lines\n",
        "                            continue\n",
        "                        json_data = json.loads(l)\n",
        "                        # print (json_data,\"\\n\\n\")\n",
        "                        data_list.append(json_data)\n",
        "\n",
        "            isRumorLists.append(pd.DataFrame(isRumorList, columns=['isRumor']))\n",
        "            data_lists.append(data_list)\n",
        "        return data_lists, isRumorLists\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tweets2tokens(tweet_text):\n",
        "    # Tokenizing\n",
        "    urls = []\n",
        "    tokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+','', tweet_text.lower()))\n",
        "    tweet_text = re.sub(r\"http\\S+\", \"\", tweet_text)\n",
        "    # tokens = nltk.TweetTokenizer().tokenize(re.sub(r'([\\d,.]+)','', tweet_text.lower()))\n",
        "\n",
        "    # Setting url value (whether the tweet contains http link) and filter http links\n",
        "    url=0\n",
        "    for token in tokens:\n",
        "        if token.startswith('http'):\n",
        "            url=1\n",
        "    tokens = [token for token in tokens if not token.startswith('http')]\n",
        "\n",
        "    ## Stemming\n",
        "    # porter = PorterStemmer()\n",
        "    # tokens = [porter.stem(token) for token in tokens]\n",
        "\n",
        "    # Filtering Stop words\n",
        "    # from nltk.corpus import stopwords\n",
        "    # stop_words = set(stopwords.words('english'))\n",
        "    # tokens = [token for token in tokens if not token in stop_words]\n",
        "\n",
        "    return tokens, url\n",
        "\n",
        "def extract_urls(entities_dicts):\n",
        "    if len(entities_dicts) < 1:\n",
        "        return 0,[],[]\n",
        "\n",
        "    urls = []\n",
        "    urls_expanded = []\n",
        "\n",
        "    key = 'url'\n",
        "    key2 = 'expanded_url'\n",
        "    # print(len(entities_dict))\n",
        "    for i in entities_dicts:\n",
        "        urls.append(i[key])\n",
        "        urls_expanded.append(i[key2])\n",
        "    return 1, urls, urls_expanded\n",
        "\n",
        "def flatten_tweets(tweets):\n",
        "    \"\"\" Flattens out tweet dictionaries so relevant JSON is in a top-level dictionary. \"\"\"\n",
        "    tweets_list = []\n",
        "    total_tokens_l = []\n",
        "\n",
        "    # Iterate through each tweet\n",
        "    for tweet_obj in tweets:\n",
        "        output_f = dict()\n",
        "        # print(tweet_obj['text'])\n",
        "        output_f['text']= tweet_obj['text']\n",
        "        \n",
        "        urls_dicts = tweet_obj['entities']['urls']\n",
        "\n",
        "        output_f['verified'] = tweet_obj['user']['verified']\n",
        "\n",
        "        tweets_list.append(output_f)\n",
        "\n",
        "    unk_tokens_l = list(set(total_tokens_l))\n",
        "    # print(\"Number of total tokens appeared: {}\\nNumber of unique tokens appeared: {}\\n\".format(len(total_tokens_l), len(unk_tokens_l))) # number of tokens and unique tokens\n",
        "\n",
        "    return tweets_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "vuButUYPFxuw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of json files are: 5802\n"
          ]
        }
      ],
      "source": [
        "data_path = \"../pheme-rnr-dataset\"\n",
        "loader = data_loader(data_path)\n",
        "jsonFiles_data = loader.getdata()\n",
        "# dev_path = \"./Other's code/rumoureval-master/data/dev\"\n",
        "# jsonFiles_dev = data_loader().getdata(dev_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# len(jsonFiles_data[0][0])\n",
        "# jsonFiles_data[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Data of Root tweets) X: 1 y: 1\n"
          ]
        }
      ],
      "source": [
        "data_lists, y_lists = loader.extract_data(jsonFiles_data)\n",
        "print(\"(Data of Root tweets) X: {} y: {}\".format(len(data_lists),len(y_lists)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame(flatten_tweets(data_lists[0]))\n",
        "y = pd.DataFrame(y_lists[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "concat = pd.concat([data,y], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "isRumor\n0    3830\n1    1972\ndtype: int64"
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "concat.value_counts('isRumor')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "concat.to_csv('./data/_PHEME_text.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# d = pd.read_csv(\"./data/_PHEME_text.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = \"../pheme-rnr-dataset\"\n",
        "events = ['charliehebdo', 'ferguson',\n",
        "          'germanwings-crash', 'ottawashooting', 'sydneysiege']\n",
        "jsons = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, event in enumerate(events):\n",
        "    # print(i, event)\n",
        "    jsons.append(glob('%s/%s/*/*/source-tweet/*.json' % (path, event)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "5"
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(jsons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 charliehebdo 2079 2079\n",
            "1 ferguson 1143 1143\n",
            "2 germanwings-crash 469 469\n",
            "3 ottawashooting 890 890\n",
            "4 sydneysiege 1221 1221\n"
          ]
        }
      ],
      "source": [
        "targets = []\n",
        "features = []\n",
        "for index, dataset in enumerate(jsons):\n",
        "    targetEvent = []\n",
        "    dataEvent = []\n",
        "    count = 0  # help var\n",
        "    for jsonFile in dataset:\n",
        "        count += 1\n",
        "        if jsonFile.find(\"non-rumours\") == -1:\n",
        "            targetEvent.append(1)\n",
        "        else:\n",
        "            targetEvent.append(0)\n",
        "\n",
        "        with open(jsonFile, 'r') as f:\n",
        "            for l in f.readlines():\n",
        "                if not l.strip():  # skip empty lines\n",
        "                    continue\n",
        "                json_data = json.loads(l)\n",
        "                # print (json_data,\"\\n\\n\")\n",
        "                dataEvent.append(json_data)\n",
        "    print(index, events[index], len(targetEvent), len(dataEvent))\n",
        "    targets.append(targetEvent)\n",
        "    features.append(dataEvent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "extracted_features = []\n",
        "\n",
        "extracted = []\n",
        "# total_tokens_l = []\n",
        "\n",
        "# Iterate through each tweet\n",
        "for obj_list in features:\n",
        "    extracted_event = []\n",
        "    for obj in obj_list:\n",
        "        output_f = dict()\n",
        "        # print(tweet_obj['text'])\n",
        "        output_f['text'] = obj['text']\n",
        "        # urls_dicts = obj['entities']['urls']\n",
        "        # output_f['verified'] = obj['user']['verified']\n",
        "        extracted_event.append(output_f)\n",
        "    extracted_features.append(extracted_event)\n",
        "# unk_tokens_l = list(set(total_tokens_l))\n",
        "# print(\"Number of total tokens appeared: {}\\nNumber of unique tokens appeared: {}\\n\".format(\n",
        "# len(total_tokens_l), len(unk_tokens_l)))  # number of tokens and unique tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [],
      "source": [
        "extracted_df = []\n",
        "for i, data in enumerate(extracted_features):\n",
        "    temp = pd.DataFrame(data)\n",
        "    temp[\"Event\"] = events[i]\n",
        "    extracted_df.append(pd.DataFrame(temp))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [],
      "source": [
        "final = pd.concat(extracted_df, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>Event</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BREAKING: Armed man takes hostage in kosher gr...</td>\n      <td>charliehebdo</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>#CharlieHebdo killers dead, confirmed by genda...</td>\n      <td>charliehebdo</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Top French cartoonists Charb, Cabu, Wolinski, ...</td>\n      <td>charliehebdo</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Police have surrounded the area where the #Cha...</td>\n      <td>charliehebdo</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>PHOTO: Armed gunmen face police officers near ...</td>\n      <td>charliehebdo</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5797</th>\n      <td>'I'll ride with you' http://t.co/llZnuCAzg5 Au...</td>\n      <td>sydneysiege</td>\n    </tr>\n    <tr>\n      <th>5798</th>\n      <td>Canada's thoughts and prayers are with our Aus...</td>\n      <td>sydneysiege</td>\n    </tr>\n    <tr>\n      <th>5799</th>\n      <td>Every non-muslim in the world must watch this ...</td>\n      <td>sydneysiege</td>\n    </tr>\n    <tr>\n      <th>5800</th>\n      <td>Suspect in Sydney cafe siege identified as Man...</td>\n      <td>sydneysiege</td>\n    </tr>\n    <tr>\n      <th>5801</th>\n      <td>Australians respond to racism by telling #Musl...</td>\n      <td>sydneysiege</td>\n    </tr>\n  </tbody>\n</table>\n<p>5802 rows × 2 columns</p>\n</div>",
            "text/plain": "                                                   text         Event\n0     BREAKING: Armed man takes hostage in kosher gr...  charliehebdo\n1     #CharlieHebdo killers dead, confirmed by genda...  charliehebdo\n2     Top French cartoonists Charb, Cabu, Wolinski, ...  charliehebdo\n3     Police have surrounded the area where the #Cha...  charliehebdo\n4     PHOTO: Armed gunmen face police officers near ...  charliehebdo\n...                                                 ...           ...\n5797  'I'll ride with you' http://t.co/llZnuCAzg5 Au...   sydneysiege\n5798  Canada's thoughts and prayers are with our Aus...   sydneysiege\n5799  Every non-muslim in the world must watch this ...   sydneysiege\n5800  Suspect in Sydney cafe siege identified as Man...   sydneysiege\n5801  Australians respond to racism by telling #Musl...   sydneysiege\n\n[5802 rows x 2 columns]"
          },
          "execution_count": 158,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOqgxhgf7r7b1nXDjRfAzrw",
      "include_colab_link": true,
      "name": "_RumorEval_data.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "x86VenvTest",
      "language": "python",
      "name": "venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}