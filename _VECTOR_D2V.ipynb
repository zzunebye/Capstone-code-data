{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "## Options:\n",
    "### Pretrained model:\n",
    "- word2vec-ruscorpora-300\t\n",
    "- glove-twitter-200\n",
    "\n",
    "### Tokenization\n",
    "- How to deal with OOV?\n",
    "    - Mention -> '@'\n",
    "    - Hashtag -> '#'\n",
    "    - Link -> '&'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# nltk.download('punkt') # nltk.download('averaged_perceptron_tagger')\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import gensim\n",
    "import copy\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "model = api.load('glove-twitter-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(line):\n",
    "    words = []\n",
    "    for word in line:  # line - iterable, for example list of tokens\n",
    "        try:\n",
    "            w2v_idx = w2v_indices[word]\n",
    "        except KeyError:  # if you does not have a vector for this word in your w2v model, continue\n",
    "            words.append(list(np.zeros(200,)))\n",
    "            continue\n",
    "        words.append(list(w2v_vectors[w2v_idx]))\n",
    "        if not word:\n",
    "            # words.append(np.zeros((200,)))\n",
    "            words.append(None)\n",
    "        if len(line) > len(words):\n",
    "            continue\n",
    "    return np.asarray(words)\n",
    "\n",
    "# insert only text data\n",
    "def get_W2V_AVG(raw_data):\n",
    "    tweet_tokenizer = TweetTokenizer()\n",
    "    tweet_tokens = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    for sent in raw_data.text:\n",
    "        sent = re.sub('', '', sent.lower())\n",
    "        sent = re.sub(r'([^\\s\\w@#]|_)+', '', sent)\n",
    "        sent = re.sub(r\"http\\S+\", \"&\", sent)\n",
    "        sent = re.sub(r\"@\\S+\", '@', sent)\n",
    "        sent = re.sub(r\"#\\S+\", '#', sent)\n",
    "        sent = [tweet_tokenizer.tokenize(sent)]\n",
    "        sent = [token for token in sent[0] if not token in stop_words]\n",
    "        tweet_tokens.append([sent])\n",
    "    df_tokens = pd.DataFrame(tweet_tokens, columns=['token'])\n",
    "\n",
    "    w2v_object = model.wv\n",
    "    # here you load vectors for each word in your model\n",
    "    w2v_vectors = w2v_object.vectors\n",
    "    # here you load indices - with whom you can find an index of the particular word in your model\n",
    "    w2v_indices = {\n",
    "        word: w2v_object.vocab[word].index for word in w2v_object.vocab}\n",
    "\n",
    "    df_tokens['token_vec_avg'] = copy.deepcopy(df_tokens['token'])\n",
    "\n",
    "    # for index, sent in enumerate(df_tokens['token_vec_avg']):\n",
    "    for index, sent in enumerate(df_tokens['token_vec_avg']):\n",
    "        df_tokens['token_vec_avg'][index] = vectorize(sent).mean(axis=0)\n",
    "\n",
    "    df_temp = pd.DataFrame(df_tokens['token_vec_avg'].values.tolist()).add_prefix('W2V_avg')  # .join(df)\n",
    "    df_tokens = df_tokens.join(df_temp).drop('token_vec_avg', axis=1)\n",
    "    return df_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"./data/_PHEME_text.csv\")\n",
    "df_tokens = get_W2V_AVG(raw_data)\n",
    "df_tokens.head()\n",
    "\n",
    "raw_rumorhasit = pd.read_csv(\"./data/_RHS_text.csv\")\n",
    "rhi_tokens = get_W2V_AVG(raw_rumorhasit)\n",
    "# rhi_tokens.head()\n",
    "\n",
    "'''Word2Vec Averaged Vector Feature set'''\n",
    "raw_rumorhasit.to_csv('./data/_PHEME_text_AVGw2v.csv', index = False)\n",
    "rhi_tokens.to_csv('./data/_RHS_text_AVGw2v.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in df_tokens['token']:\n",
    "#     if len(i) < 5 :\n",
    "#         print(i)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "from nltk.corpus import stopwords as stpdfa\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import gensim\n",
    "import gensim.models.word2vec as w2v\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "# import matplotlib as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"./data/_PHEME_text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[breaking, armed, man, takes, hostage, kosher, grocery, east, paris]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[charliehebdo, killers, dead, confirmed, gendarmerie]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[top, french, cartoonists, charb, cabu, wolinski, tignous, confirmed, among, dead, paris, charliehebdo, attack, editor, critically, wounded]</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                                                                                                                          token\n0                                                                          [breaking, armed, man, takes, hostage, kosher, grocery, east, paris]\n1                                                                                         [charliehebdo, killers, dead, confirmed, gendarmerie]\n2  [top, french, cartoonists, charb, cabu, wolinski, tignous, confirmed, among, dead, paris, charliehebdo, attack, editor, critically, wounded]"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for sent in raw_data.text:\n",
    "    sent = re.sub(r\"http\\S+\", \"\", sent)\n",
    "    sent = re.sub(r'([^\\s\\w]|_)+', '', sent)\n",
    "    sent = re.sub('', '', sent.lower())\n",
    "    sent = [tweet_tokenizer.tokenize(sent)]\n",
    "    sent = [token for token in sent[0] if not token in stop_words]\n",
    "    # tweet_tokens.append(sent)\n",
    "    tweet_tokens.append([sent])\n",
    "# print(tweet_tokens)\n",
    "df_tokens = pd.DataFrame(tweet_tokens, columns=['token'])\n",
    "# [[['BREAKING', 'Armed', 'man', 'takes', 'hostage', 'in', 'kosher', 'grocery', 'east', 'of', 'Paris']],\n",
    "df_tokens.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Pretrained model for Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching pretrained Model and Convert the Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "# corpus = api.load('text8')\n",
    "# wv = api.load('word2vec-google-news-300')\n",
    "# fasttext-wiki-news-subwords-300'\n",
    "#  'glove-twitter-200',\n",
    "model = api.load('glove-twitter-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling the OOV\n",
    "# for doc in df_tokens:\n",
    "#     words = filter(lambda x: x in model.vocab, doc)\n",
    "#     print(words)\n",
    "# for word, i in t.word_index.items():\n",
    "#     try:\n",
    "#         embedding_vector = model.wv[word] except: print(word, 'not found')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_object = model.wv\n",
    "w2v_vectors = w2v_object.vectors # here you load vectors for each word in your model\n",
    "w2v_indices = {word: w2v_object.vocab[word].index for word in w2v_object.vocab} # here you load indices - with whom you can find an index of the particular word in your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(line):\n",
    "    # print(line)\n",
    "    # print(len(np.asarray(line)))\n",
    "    words = []\n",
    "    for word in line:  # line - iterable, for example list of tokens\n",
    "        try:\n",
    "            w2v_idx = w2v_indices[word]\n",
    "        except KeyError:  # if you does not have a vector for this word in your w2v model, continue\n",
    "            words.append(list(np.zeros(200,)))\n",
    "            continue\n",
    "        words.append(list(w2v_vectors[w2v_idx]))\n",
    "        if not word:\n",
    "            # words.append(np.zeros((200,)))\n",
    "            words.append(None)\n",
    "\n",
    "        if len(line) > len(words):\n",
    "            continue\n",
    "    return np.asarray(words)\n",
    "    # print(words)\n",
    "    # print(len(np.asarray(words)))\n",
    "    # return words\n",
    "\n",
    "\n",
    "def get_vectorOOV(s):\n",
    "    try:\n",
    "        return np.array(model.wv.get_vector(s))\n",
    "    except KeyError:\n",
    "        return np.zeros((300,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304\n"
     ]
    }
   ],
   "source": [
    "# print(\"Tweet: \", raw_data['text'][1])\n",
    "# print(\"Tweet Token: \", df_tokens['token'][10])\n",
    "# print(\"Indice of '{}': {}\".format(df_tokens['token'][10][0], w2v_indices[df_tokens['token'][10][0]]))\n",
    "# print(\"Indice of '{}': {}\".format(df_tokens['token'][10][0], w2v_vectors[w2v_indices[df_tokens['token'][10][0]]]))\n",
    "# print(\"Indice of '{}': {}\".format(raw_data['text_token'][1][1], w2v_indices[raw_data['text_token'][1][1]]))\n",
    "# # print(\"Indice of '{}': {}\".format(raw_data['text_token'][1][1], w2v_vectors[w2v_indices[raw_data['text_token'][1][1]]]))\n",
    "# print(\"\\nVector of the first headline can be avaraged:\\n\", vectorize(df_tokens['token'][10]).mean())\n",
    "# w2v_vectors[w2v_indices[df_tokens['token'][10][0]]].shape\n",
    "# print(w2v_indices['@'])\n",
    "# len(vectorize(df_tokens['token'][10]))\n",
    "# vectorize(df_tokens['token'][10]).mean(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[breaking, armed, man, takes, hostage, kosher, grocery, east, paris]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[charliehebdo, killers, dead, confirmed, gendarmerie]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[top, french, cartoonists, charb, cabu, wolinski, tignous, confirmed, among, dead, paris, charliehebdo, attack, editor, critically, wounded]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[police, surrounded, area, charliehebdo, attack, suspects, believed]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[photo, armed, gunmen, face, police, officers, near, charliehebdo, hq, paris]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5797</th>\n      <td>[ill, ride, australia, unites, sydneysiege]</td>\n    </tr>\n    <tr>\n      <th>5798</th>\n      <td>[canadas, thoughts, prayers, australian, friends, martinplace, sydneysiege]</td>\n    </tr>\n    <tr>\n      <th>5799</th>\n      <td>[every, nonmuslim, world, must, watch, video, amp, show, every, nonmuslim, sydneysiege]</td>\n    </tr>\n    <tr>\n      <th>5800</th>\n      <td>[suspect, sydney, cafe, siege, identified, man, haron, monis, iranian, granted, asylum, australia]</td>\n    </tr>\n    <tr>\n      <th>5801</th>\n      <td>[australians, respond, racism, telling, muslim, community, illridewithyou, sydneysiege, martinplace]</td>\n    </tr>\n  </tbody>\n</table>\n<p>5802 rows Ã— 1 columns</p>\n</div>",
      "text/plain": "                                                                                                                                             token\n0                                                                             [breaking, armed, man, takes, hostage, kosher, grocery, east, paris]\n1                                                                                            [charliehebdo, killers, dead, confirmed, gendarmerie]\n2     [top, french, cartoonists, charb, cabu, wolinski, tignous, confirmed, among, dead, paris, charliehebdo, attack, editor, critically, wounded]\n3                                                                             [police, surrounded, area, charliehebdo, attack, suspects, believed]\n4                                                                    [photo, armed, gunmen, face, police, officers, near, charliehebdo, hq, paris]\n...                                                                                                                                            ...\n5797                                                                                                   [ill, ride, australia, unites, sydneysiege]\n5798                                                                   [canadas, thoughts, prayers, australian, friends, martinplace, sydneysiege]\n5799                                                       [every, nonmuslim, world, must, watch, video, amp, show, every, nonmuslim, sydneysiege]\n5800                                            [suspect, sydney, cafe, siege, identified, man, haron, monis, iranian, granted, asylum, australia]\n5801                                          [australians, respond, racism, telling, muslim, community, illridewithyou, sydneysiege, martinplace]\n\n[5802 rows x 1 columns]"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens#.token[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token</th>\n      <th>W2V_avg0</th>\n      <th>W2V_avg1</th>\n      <th>W2V_avg2</th>\n      <th>W2V_avg3</th>\n      <th>W2V_avg4</th>\n      <th>W2V_avg5</th>\n      <th>W2V_avg6</th>\n      <th>W2V_avg7</th>\n      <th>W2V_avg8</th>\n      <th>W2V_avg9</th>\n      <th>W2V_avg10</th>\n      <th>W2V_avg11</th>\n      <th>W2V_avg12</th>\n      <th>W2V_avg13</th>\n      <th>W2V_avg14</th>\n      <th>W2V_avg15</th>\n      <th>W2V_avg16</th>\n      <th>W2V_avg17</th>\n      <th>W2V_avg18</th>\n      <th>W2V_avg19</th>\n      <th>W2V_avg20</th>\n      <th>W2V_avg21</th>\n      <th>W2V_avg22</th>\n      <th>W2V_avg23</th>\n      <th>W2V_avg24</th>\n      <th>W2V_avg25</th>\n      <th>W2V_avg26</th>\n      <th>W2V_avg27</th>\n      <th>W2V_avg28</th>\n      <th>W2V_avg29</th>\n      <th>W2V_avg30</th>\n      <th>W2V_avg31</th>\n      <th>W2V_avg32</th>\n      <th>W2V_avg33</th>\n      <th>W2V_avg34</th>\n      <th>W2V_avg35</th>\n      <th>W2V_avg36</th>\n      <th>W2V_avg37</th>\n      <th>W2V_avg38</th>\n      <th>W2V_avg39</th>\n      <th>W2V_avg40</th>\n      <th>W2V_avg41</th>\n      <th>W2V_avg42</th>\n      <th>W2V_avg43</th>\n      <th>W2V_avg44</th>\n      <th>W2V_avg45</th>\n      <th>W2V_avg46</th>\n      <th>W2V_avg47</th>\n      <th>W2V_avg48</th>\n      <th>W2V_avg49</th>\n      <th>W2V_avg50</th>\n      <th>W2V_avg51</th>\n      <th>W2V_avg52</th>\n      <th>W2V_avg53</th>\n      <th>W2V_avg54</th>\n      <th>W2V_avg55</th>\n      <th>W2V_avg56</th>\n      <th>W2V_avg57</th>\n      <th>W2V_avg58</th>\n      <th>W2V_avg59</th>\n      <th>W2V_avg60</th>\n      <th>W2V_avg61</th>\n      <th>W2V_avg62</th>\n      <th>W2V_avg63</th>\n      <th>W2V_avg64</th>\n      <th>W2V_avg65</th>\n      <th>W2V_avg66</th>\n      <th>W2V_avg67</th>\n      <th>W2V_avg68</th>\n      <th>W2V_avg69</th>\n      <th>W2V_avg70</th>\n      <th>W2V_avg71</th>\n      <th>W2V_avg72</th>\n      <th>W2V_avg73</th>\n      <th>W2V_avg74</th>\n      <th>W2V_avg75</th>\n      <th>W2V_avg76</th>\n      <th>W2V_avg77</th>\n      <th>W2V_avg78</th>\n      <th>W2V_avg79</th>\n      <th>W2V_avg80</th>\n      <th>W2V_avg81</th>\n      <th>W2V_avg82</th>\n      <th>W2V_avg83</th>\n      <th>W2V_avg84</th>\n      <th>W2V_avg85</th>\n      <th>W2V_avg86</th>\n      <th>W2V_avg87</th>\n      <th>W2V_avg88</th>\n      <th>W2V_avg89</th>\n      <th>W2V_avg90</th>\n      <th>W2V_avg91</th>\n      <th>W2V_avg92</th>\n      <th>W2V_avg93</th>\n      <th>W2V_avg94</th>\n      <th>W2V_avg95</th>\n      <th>W2V_avg96</th>\n      <th>W2V_avg97</th>\n      <th>W2V_avg98</th>\n      <th>W2V_avg99</th>\n      <th>W2V_avg100</th>\n      <th>W2V_avg101</th>\n      <th>W2V_avg102</th>\n      <th>W2V_avg103</th>\n      <th>W2V_avg104</th>\n      <th>W2V_avg105</th>\n      <th>W2V_avg106</th>\n      <th>W2V_avg107</th>\n      <th>W2V_avg108</th>\n      <th>W2V_avg109</th>\n      <th>W2V_avg110</th>\n      <th>W2V_avg111</th>\n      <th>W2V_avg112</th>\n      <th>W2V_avg113</th>\n      <th>W2V_avg114</th>\n      <th>W2V_avg115</th>\n      <th>W2V_avg116</th>\n      <th>W2V_avg117</th>\n      <th>W2V_avg118</th>\n      <th>W2V_avg119</th>\n      <th>W2V_avg120</th>\n      <th>W2V_avg121</th>\n      <th>W2V_avg122</th>\n      <th>W2V_avg123</th>\n      <th>W2V_avg124</th>\n      <th>W2V_avg125</th>\n      <th>W2V_avg126</th>\n      <th>W2V_avg127</th>\n      <th>W2V_avg128</th>\n      <th>W2V_avg129</th>\n      <th>W2V_avg130</th>\n      <th>W2V_avg131</th>\n      <th>W2V_avg132</th>\n      <th>W2V_avg133</th>\n      <th>W2V_avg134</th>\n      <th>W2V_avg135</th>\n      <th>W2V_avg136</th>\n      <th>W2V_avg137</th>\n      <th>W2V_avg138</th>\n      <th>W2V_avg139</th>\n      <th>W2V_avg140</th>\n      <th>W2V_avg141</th>\n      <th>W2V_avg142</th>\n      <th>W2V_avg143</th>\n      <th>W2V_avg144</th>\n      <th>W2V_avg145</th>\n      <th>W2V_avg146</th>\n      <th>W2V_avg147</th>\n      <th>W2V_avg148</th>\n      <th>W2V_avg149</th>\n      <th>W2V_avg150</th>\n      <th>W2V_avg151</th>\n      <th>W2V_avg152</th>\n      <th>W2V_avg153</th>\n      <th>W2V_avg154</th>\n      <th>W2V_avg155</th>\n      <th>W2V_avg156</th>\n      <th>W2V_avg157</th>\n      <th>W2V_avg158</th>\n      <th>W2V_avg159</th>\n      <th>W2V_avg160</th>\n      <th>W2V_avg161</th>\n      <th>W2V_avg162</th>\n      <th>W2V_avg163</th>\n      <th>W2V_avg164</th>\n      <th>W2V_avg165</th>\n      <th>W2V_avg166</th>\n      <th>W2V_avg167</th>\n      <th>W2V_avg168</th>\n      <th>W2V_avg169</th>\n      <th>W2V_avg170</th>\n      <th>W2V_avg171</th>\n      <th>W2V_avg172</th>\n      <th>W2V_avg173</th>\n      <th>W2V_avg174</th>\n      <th>W2V_avg175</th>\n      <th>W2V_avg176</th>\n      <th>W2V_avg177</th>\n      <th>W2V_avg178</th>\n      <th>W2V_avg179</th>\n      <th>W2V_avg180</th>\n      <th>W2V_avg181</th>\n      <th>W2V_avg182</th>\n      <th>W2V_avg183</th>\n      <th>W2V_avg184</th>\n      <th>W2V_avg185</th>\n      <th>W2V_avg186</th>\n      <th>W2V_avg187</th>\n      <th>W2V_avg188</th>\n      <th>W2V_avg189</th>\n      <th>W2V_avg190</th>\n      <th>W2V_avg191</th>\n      <th>W2V_avg192</th>\n      <th>W2V_avg193</th>\n      <th>W2V_avg194</th>\n      <th>W2V_avg195</th>\n      <th>W2V_avg196</th>\n      <th>W2V_avg197</th>\n      <th>W2V_avg198</th>\n      <th>W2V_avg199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[breaking, armed, man, takes, hostage, kosher, grocery, east, paris]</td>\n      <td>-0.214452</td>\n      <td>0.022334</td>\n      <td>0.199403</td>\n      <td>0.091531</td>\n      <td>-0.106364</td>\n      <td>-0.135615</td>\n      <td>0.224626</td>\n      <td>-0.100319</td>\n      <td>0.156632</td>\n      <td>-0.080552</td>\n      <td>-0.187510</td>\n      <td>0.085141</td>\n      <td>-0.692078</td>\n      <td>0.294819</td>\n      <td>0.190482</td>\n      <td>-0.000107</td>\n      <td>0.130387</td>\n      <td>-0.086283</td>\n      <td>0.000389</td>\n      <td>-0.256427</td>\n      <td>0.116084</td>\n      <td>-0.121319</td>\n      <td>-0.104006</td>\n      <td>-0.152930</td>\n      <td>0.023462</td>\n      <td>0.883121</td>\n      <td>0.178624</td>\n      <td>-0.068786</td>\n      <td>-0.162735</td>\n      <td>-0.025268</td>\n      <td>-0.081778</td>\n      <td>-0.072879</td>\n      <td>0.013534</td>\n      <td>-0.211057</td>\n      <td>0.161137</td>\n      <td>0.328094</td>\n      <td>-0.083515</td>\n      <td>0.074648</td>\n      <td>0.294723</td>\n      <td>-0.128192</td>\n      <td>0.191932</td>\n      <td>-0.004869</td>\n      <td>-0.009566</td>\n      <td>-0.111364</td>\n      <td>0.023465</td>\n      <td>0.179786</td>\n      <td>0.075096</td>\n      <td>-0.021256</td>\n      <td>-0.073478</td>\n      <td>0.147782</td>\n      <td>-0.470354</td>\n      <td>0.122131</td>\n      <td>-0.054976</td>\n      <td>0.097603</td>\n      <td>-0.216612</td>\n      <td>0.179325</td>\n      <td>-0.135947</td>\n      <td>0.018460</td>\n      <td>-0.087136</td>\n      <td>0.080001</td>\n      <td>0.104294</td>\n      <td>0.091111</td>\n      <td>0.128683</td>\n      <td>-0.013114</td>\n      <td>0.117404</td>\n      <td>-0.043849</td>\n      <td>0.204457</td>\n      <td>0.150360</td>\n      <td>-0.056318</td>\n      <td>-0.097586</td>\n      <td>-0.176027</td>\n      <td>-0.034084</td>\n      <td>0.131073</td>\n      <td>0.023279</td>\n      <td>0.092683</td>\n      <td>0.058691</td>\n      <td>-0.028683</td>\n      <td>-0.015163</td>\n      <td>0.025195</td>\n      <td>0.051936</td>\n      <td>0.374640</td>\n      <td>-0.091818</td>\n      <td>0.210236</td>\n      <td>0.206264</td>\n      <td>-0.073652</td>\n      <td>-0.212403</td>\n      <td>-0.131285</td>\n      <td>0.245320</td>\n      <td>-0.031517</td>\n      <td>-0.134470</td>\n      <td>-0.019555</td>\n      <td>-0.028470</td>\n      <td>0.130524</td>\n      <td>0.108279</td>\n      <td>-0.115213</td>\n      <td>0.134851</td>\n      <td>0.071441</td>\n      <td>0.061887</td>\n      <td>0.032899</td>\n      <td>-0.012026</td>\n      <td>0.299926</td>\n      <td>-0.140451</td>\n      <td>-0.122578</td>\n      <td>-0.307877</td>\n      <td>0.121167</td>\n      <td>-0.329101</td>\n      <td>0.065360</td>\n      <td>0.072052</td>\n      <td>0.156233</td>\n      <td>-0.119539</td>\n      <td>0.102322</td>\n      <td>-0.157276</td>\n      <td>0.073717</td>\n      <td>-0.007854</td>\n      <td>-0.000111</td>\n      <td>-0.240686</td>\n      <td>-0.007508</td>\n      <td>0.180650</td>\n      <td>-0.256752</td>\n      <td>0.362714</td>\n      <td>0.035268</td>\n      <td>0.046164</td>\n      <td>-0.035225</td>\n      <td>0.083667</td>\n      <td>-0.241622</td>\n      <td>-0.167557</td>\n      <td>0.108498</td>\n      <td>0.164607</td>\n      <td>0.431350</td>\n      <td>0.315716</td>\n      <td>-0.253180</td>\n      <td>0.140676</td>\n      <td>-0.126034</td>\n      <td>-0.474589</td>\n      <td>-0.084879</td>\n      <td>-0.358264</td>\n      <td>0.066130</td>\n      <td>0.110956</td>\n      <td>0.159029</td>\n      <td>-0.050851</td>\n      <td>-0.025120</td>\n      <td>0.258623</td>\n      <td>-0.202793</td>\n      <td>-0.207713</td>\n      <td>-0.221215</td>\n      <td>-0.004675</td>\n      <td>0.036970</td>\n      <td>-0.061748</td>\n      <td>0.001078</td>\n      <td>0.154303</td>\n      <td>0.091478</td>\n      <td>-0.095547</td>\n      <td>-2.876716</td>\n      <td>-0.078268</td>\n      <td>0.229267</td>\n      <td>-0.062095</td>\n      <td>-0.024084</td>\n      <td>0.122586</td>\n      <td>0.170145</td>\n      <td>0.006689</td>\n      <td>-0.100560</td>\n      <td>0.164260</td>\n      <td>-0.060955</td>\n      <td>0.046130</td>\n      <td>0.185162</td>\n      <td>-0.142742</td>\n      <td>-0.143107</td>\n      <td>-0.036575</td>\n      <td>0.335150</td>\n      <td>-0.313509</td>\n      <td>-0.235394</td>\n      <td>0.058141</td>\n      <td>-0.062775</td>\n      <td>0.226615</td>\n      <td>0.019887</td>\n      <td>-0.620922</td>\n      <td>0.119913</td>\n      <td>-0.163845</td>\n      <td>-0.269272</td>\n      <td>0.167251</td>\n      <td>-0.132802</td>\n      <td>-0.105065</td>\n      <td>-0.206132</td>\n      <td>0.153061</td>\n      <td>0.089953</td>\n      <td>0.093279</td>\n      <td>0.211769</td>\n      <td>-0.351812</td>\n      <td>0.060335</td>\n      <td>0.287951</td>\n      <td>0.113106</td>\n      <td>-0.158723</td>\n      <td>0.046337</td>\n      <td>-0.102808</td>\n      <td>-0.085297</td>\n      <td>-0.109255</td>\n      <td>0.093852</td>\n      <td>-0.004668</td>\n      <td>0.290299</td>\n      <td>-0.042824</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[charliehebdo, killers, dead, confirmed, gendarmerie]</td>\n      <td>-0.255756</td>\n      <td>0.098868</td>\n      <td>-0.311535</td>\n      <td>0.198814</td>\n      <td>-0.018441</td>\n      <td>-0.018610</td>\n      <td>-0.017218</td>\n      <td>0.087670</td>\n      <td>0.270094</td>\n      <td>-0.053273</td>\n      <td>0.310100</td>\n      <td>0.167015</td>\n      <td>-0.306440</td>\n      <td>-0.123198</td>\n      <td>0.087842</td>\n      <td>-0.002661</td>\n      <td>-0.128358</td>\n      <td>-0.082796</td>\n      <td>0.076324</td>\n      <td>0.020651</td>\n      <td>0.082407</td>\n      <td>0.066888</td>\n      <td>-0.098685</td>\n      <td>0.004973</td>\n      <td>0.129446</td>\n      <td>0.393180</td>\n      <td>-0.069406</td>\n      <td>-0.205070</td>\n      <td>-0.127408</td>\n      <td>0.155606</td>\n      <td>0.122610</td>\n      <td>-0.065847</td>\n      <td>-0.284126</td>\n      <td>-0.214486</td>\n      <td>-0.319644</td>\n      <td>0.070190</td>\n      <td>-0.213362</td>\n      <td>-0.141469</td>\n      <td>-0.050849</td>\n      <td>-0.080936</td>\n      <td>0.289188</td>\n      <td>-0.094120</td>\n      <td>0.216460</td>\n      <td>0.115527</td>\n      <td>-0.053166</td>\n      <td>0.149116</td>\n      <td>-0.058560</td>\n      <td>-0.150393</td>\n      <td>0.138688</td>\n      <td>0.325252</td>\n      <td>-0.202569</td>\n      <td>0.341700</td>\n      <td>-0.014998</td>\n      <td>0.007921</td>\n      <td>-0.120828</td>\n      <td>0.078157</td>\n      <td>-0.054125</td>\n      <td>-0.124825</td>\n      <td>-0.060872</td>\n      <td>0.051447</td>\n      <td>-0.002723</td>\n      <td>0.356174</td>\n      <td>-0.039212</td>\n      <td>0.157901</td>\n      <td>-0.315652</td>\n      <td>0.137474</td>\n      <td>0.153440</td>\n      <td>0.356670</td>\n      <td>0.050584</td>\n      <td>-0.026924</td>\n      <td>0.110450</td>\n      <td>0.282987</td>\n      <td>0.018974</td>\n      <td>-0.112940</td>\n      <td>0.055878</td>\n      <td>0.114597</td>\n      <td>-0.579636</td>\n      <td>-0.032342</td>\n      <td>-0.150035</td>\n      <td>-0.043010</td>\n      <td>0.231848</td>\n      <td>-0.311564</td>\n      <td>0.197215</td>\n      <td>0.410206</td>\n      <td>0.059377</td>\n      <td>-0.136214</td>\n      <td>-0.067408</td>\n      <td>-0.051222</td>\n      <td>0.049914</td>\n      <td>0.011236</td>\n      <td>-0.097720</td>\n      <td>0.023898</td>\n      <td>-0.117478</td>\n      <td>0.252436</td>\n      <td>-0.129452</td>\n      <td>0.280040</td>\n      <td>0.031374</td>\n      <td>0.093724</td>\n      <td>0.011174</td>\n      <td>0.171804</td>\n      <td>0.236370</td>\n      <td>0.039298</td>\n      <td>-0.119566</td>\n      <td>-0.180323</td>\n      <td>0.112074</td>\n      <td>-0.306025</td>\n      <td>0.075580</td>\n      <td>-0.168974</td>\n      <td>0.140822</td>\n      <td>-0.020892</td>\n      <td>-0.035424</td>\n      <td>0.040102</td>\n      <td>0.140417</td>\n      <td>-0.066569</td>\n      <td>0.038037</td>\n      <td>0.105540</td>\n      <td>0.040056</td>\n      <td>-0.016134</td>\n      <td>0.018342</td>\n      <td>0.383671</td>\n      <td>0.270512</td>\n      <td>-0.204871</td>\n      <td>0.254165</td>\n      <td>-0.181773</td>\n      <td>0.064248</td>\n      <td>0.067117</td>\n      <td>0.169530</td>\n      <td>-0.092278</td>\n      <td>0.262428</td>\n      <td>0.284424</td>\n      <td>0.255236</td>\n      <td>0.358990</td>\n      <td>-0.083502</td>\n      <td>0.066987</td>\n      <td>0.098118</td>\n      <td>-0.163968</td>\n      <td>-0.074293</td>\n      <td>-0.223430</td>\n      <td>0.205134</td>\n      <td>0.026182</td>\n      <td>-0.102120</td>\n      <td>0.148810</td>\n      <td>-0.131784</td>\n      <td>0.019180</td>\n      <td>0.249378</td>\n      <td>-0.049386</td>\n      <td>-0.381474</td>\n      <td>-0.034042</td>\n      <td>-0.132978</td>\n      <td>0.151890</td>\n      <td>-0.135441</td>\n      <td>0.137162</td>\n      <td>-1.810772</td>\n      <td>-0.657656</td>\n      <td>-0.300113</td>\n      <td>-0.057735</td>\n      <td>-0.108980</td>\n      <td>-0.122500</td>\n      <td>-0.126700</td>\n      <td>-0.186454</td>\n      <td>0.183002</td>\n      <td>-0.067376</td>\n      <td>-0.128471</td>\n      <td>-0.193728</td>\n      <td>0.209042</td>\n      <td>0.056675</td>\n      <td>-0.064868</td>\n      <td>-0.166222</td>\n      <td>-0.042119</td>\n      <td>-0.357104</td>\n      <td>-0.097194</td>\n      <td>0.289858</td>\n      <td>0.085809</td>\n      <td>-0.027096</td>\n      <td>0.155172</td>\n      <td>-0.055576</td>\n      <td>0.218576</td>\n      <td>-0.063891</td>\n      <td>0.080436</td>\n      <td>-0.028387</td>\n      <td>0.057260</td>\n      <td>-0.058414</td>\n      <td>-0.138078</td>\n      <td>0.246572</td>\n      <td>-0.268108</td>\n      <td>0.347104</td>\n      <td>0.056461</td>\n      <td>-0.317741</td>\n      <td>0.132710</td>\n      <td>0.016556</td>\n      <td>-0.035437</td>\n      <td>0.006688</td>\n      <td>-0.073772</td>\n      <td>0.061821</td>\n      <td>-0.062575</td>\n      <td>-0.122253</td>\n      <td>-0.172704</td>\n      <td>0.100435</td>\n      <td>0.167724</td>\n      <td>-0.180706</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[top, french, cartoonists, charb, cabu, wolinski, tignous, confirmed, among, dead, paris, charliehebdo, attack, editor, critically, wounded]</td>\n      <td>-0.204260</td>\n      <td>-0.128090</td>\n      <td>-0.056717</td>\n      <td>0.065545</td>\n      <td>-0.006312</td>\n      <td>-0.136742</td>\n      <td>-0.029758</td>\n      <td>0.072999</td>\n      <td>0.258633</td>\n      <td>-0.305450</td>\n      <td>0.191698</td>\n      <td>0.008591</td>\n      <td>-0.348024</td>\n      <td>-0.029128</td>\n      <td>0.134144</td>\n      <td>-0.201011</td>\n      <td>0.095839</td>\n      <td>-0.092672</td>\n      <td>0.042344</td>\n      <td>-0.112320</td>\n      <td>0.039853</td>\n      <td>-0.048555</td>\n      <td>-0.077608</td>\n      <td>-0.072290</td>\n      <td>0.013536</td>\n      <td>0.384435</td>\n      <td>-0.027090</td>\n      <td>-0.071353</td>\n      <td>-0.066789</td>\n      <td>0.197552</td>\n      <td>0.231422</td>\n      <td>0.011418</td>\n      <td>-0.117731</td>\n      <td>-0.170994</td>\n      <td>0.048690</td>\n      <td>0.028248</td>\n      <td>0.025336</td>\n      <td>0.014511</td>\n      <td>0.129919</td>\n      <td>0.020536</td>\n      <td>0.249458</td>\n      <td>-0.135115</td>\n      <td>0.171695</td>\n      <td>0.089334</td>\n      <td>-0.095055</td>\n      <td>-0.006512</td>\n      <td>0.015790</td>\n      <td>-0.072966</td>\n      <td>0.001627</td>\n      <td>-0.089184</td>\n      <td>-0.212376</td>\n      <td>0.210792</td>\n      <td>-0.116497</td>\n      <td>0.149019</td>\n      <td>-0.092482</td>\n      <td>0.079930</td>\n      <td>-0.177608</td>\n      <td>0.092087</td>\n      <td>-0.006942</td>\n      <td>-0.101734</td>\n      <td>0.005013</td>\n      <td>0.093513</td>\n      <td>0.044676</td>\n      <td>0.082593</td>\n      <td>-0.123470</td>\n      <td>0.220523</td>\n      <td>0.246703</td>\n      <td>0.027099</td>\n      <td>-0.074836</td>\n      <td>-0.096585</td>\n      <td>0.020706</td>\n      <td>0.007257</td>\n      <td>-0.051876</td>\n      <td>0.023567</td>\n      <td>0.145805</td>\n      <td>0.056249</td>\n      <td>-0.155700</td>\n      <td>-0.086659</td>\n      <td>0.213755</td>\n      <td>-0.037633</td>\n      <td>0.360630</td>\n      <td>0.009033</td>\n      <td>0.219562</td>\n      <td>0.255285</td>\n      <td>0.005478</td>\n      <td>-0.105667</td>\n      <td>0.158749</td>\n      <td>0.128628</td>\n      <td>-0.087835</td>\n      <td>-0.020529</td>\n      <td>-0.016638</td>\n      <td>0.093943</td>\n      <td>0.044070</td>\n      <td>0.084936</td>\n      <td>0.081614</td>\n      <td>0.113732</td>\n      <td>0.124580</td>\n      <td>0.051482</td>\n      <td>-0.073662</td>\n      <td>0.056604</td>\n      <td>0.243867</td>\n      <td>0.033156</td>\n      <td>-0.045085</td>\n      <td>-0.041383</td>\n      <td>-0.107638</td>\n      <td>-0.132201</td>\n      <td>0.115896</td>\n      <td>-0.070264</td>\n      <td>0.130904</td>\n      <td>0.079350</td>\n      <td>0.005973</td>\n      <td>-0.065917</td>\n      <td>0.065685</td>\n      <td>0.011467</td>\n      <td>0.047428</td>\n      <td>-0.021803</td>\n      <td>0.246528</td>\n      <td>0.115391</td>\n      <td>0.008002</td>\n      <td>0.107472</td>\n      <td>-0.051933</td>\n      <td>0.071811</td>\n      <td>0.168006</td>\n      <td>-0.132772</td>\n      <td>-0.079750</td>\n      <td>-0.149816</td>\n      <td>-0.036733</td>\n      <td>-0.120097</td>\n      <td>0.242908</td>\n      <td>0.084207</td>\n      <td>-0.007310</td>\n      <td>0.272619</td>\n      <td>0.019713</td>\n      <td>0.007194</td>\n      <td>-0.088377</td>\n      <td>-0.210656</td>\n      <td>0.062959</td>\n      <td>-0.126535</td>\n      <td>-0.005121</td>\n      <td>-0.047499</td>\n      <td>0.034975</td>\n      <td>0.042599</td>\n      <td>-0.071184</td>\n      <td>0.009752</td>\n      <td>-0.002292</td>\n      <td>0.002300</td>\n      <td>-0.239995</td>\n      <td>-0.043672</td>\n      <td>-0.035714</td>\n      <td>0.006066</td>\n      <td>0.054072</td>\n      <td>0.091986</td>\n      <td>-1.796350</td>\n      <td>-0.197658</td>\n      <td>-0.043255</td>\n      <td>-0.118952</td>\n      <td>0.038594</td>\n      <td>0.075118</td>\n      <td>0.098123</td>\n      <td>0.058687</td>\n      <td>0.065406</td>\n      <td>0.010183</td>\n      <td>-0.217738</td>\n      <td>-0.055123</td>\n      <td>0.066496</td>\n      <td>-0.052564</td>\n      <td>-0.059477</td>\n      <td>-0.085470</td>\n      <td>0.018326</td>\n      <td>-0.114870</td>\n      <td>0.002257</td>\n      <td>0.149768</td>\n      <td>0.016524</td>\n      <td>0.252526</td>\n      <td>0.145121</td>\n      <td>-0.033637</td>\n      <td>0.033257</td>\n      <td>-0.081941</td>\n      <td>-0.141422</td>\n      <td>-0.151762</td>\n      <td>0.087429</td>\n      <td>0.002064</td>\n      <td>-0.089263</td>\n      <td>0.032225</td>\n      <td>0.023097</td>\n      <td>0.255386</td>\n      <td>-0.009342</td>\n      <td>-0.161150</td>\n      <td>0.110728</td>\n      <td>-0.099069</td>\n      <td>0.080498</td>\n      <td>-0.169251</td>\n      <td>-0.068136</td>\n      <td>0.112692</td>\n      <td>-0.057449</td>\n      <td>-0.080394</td>\n      <td>0.086027</td>\n      <td>0.041923</td>\n      <td>0.135656</td>\n      <td>-0.139825</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[police, surrounded, area, charliehebdo, attack, suspects, believed]</td>\n      <td>-0.091529</td>\n      <td>0.074306</td>\n      <td>-0.349119</td>\n      <td>0.254383</td>\n      <td>0.098893</td>\n      <td>0.033803</td>\n      <td>0.152399</td>\n      <td>-0.016781</td>\n      <td>0.411479</td>\n      <td>-0.177106</td>\n      <td>0.010656</td>\n      <td>-0.092236</td>\n      <td>-0.792780</td>\n      <td>0.092821</td>\n      <td>0.485739</td>\n      <td>-0.044347</td>\n      <td>0.040502</td>\n      <td>-0.238916</td>\n      <td>0.004026</td>\n      <td>0.039599</td>\n      <td>0.095151</td>\n      <td>0.184304</td>\n      <td>-0.067949</td>\n      <td>-0.122141</td>\n      <td>0.170298</td>\n      <td>0.739074</td>\n      <td>-0.031164</td>\n      <td>-0.191431</td>\n      <td>-0.080367</td>\n      <td>0.027644</td>\n      <td>0.112307</td>\n      <td>-0.114560</td>\n      <td>-0.188454</td>\n      <td>-0.131394</td>\n      <td>0.071864</td>\n      <td>0.135815</td>\n      <td>0.041656</td>\n      <td>0.157148</td>\n      <td>0.030447</td>\n      <td>-0.227244</td>\n      <td>0.359573</td>\n      <td>0.144805</td>\n      <td>0.029679</td>\n      <td>-0.020854</td>\n      <td>-0.003681</td>\n      <td>-0.137450</td>\n      <td>0.146878</td>\n      <td>-0.066970</td>\n      <td>-0.076150</td>\n      <td>0.075238</td>\n      <td>-0.327932</td>\n      <td>0.278061</td>\n      <td>0.042507</td>\n      <td>0.062780</td>\n      <td>0.005333</td>\n      <td>-0.066320</td>\n      <td>-0.404699</td>\n      <td>0.111456</td>\n      <td>-0.322922</td>\n      <td>0.096678</td>\n      <td>-0.112638</td>\n      <td>0.254730</td>\n      <td>0.139415</td>\n      <td>0.038544</td>\n      <td>0.271289</td>\n      <td>0.417597</td>\n      <td>0.205006</td>\n      <td>0.293944</td>\n      <td>-0.232444</td>\n      <td>0.026245</td>\n      <td>0.141199</td>\n      <td>0.033603</td>\n      <td>0.146027</td>\n      <td>-0.233019</td>\n      <td>0.186124</td>\n      <td>0.353500</td>\n      <td>-0.260452</td>\n      <td>-0.149412</td>\n      <td>-0.144105</td>\n      <td>-0.145454</td>\n      <td>0.539500</td>\n      <td>-0.294331</td>\n      <td>0.331097</td>\n      <td>0.401216</td>\n      <td>0.239260</td>\n      <td>-0.213281</td>\n      <td>0.206687</td>\n      <td>0.308451</td>\n      <td>-0.008447</td>\n      <td>-0.031517</td>\n      <td>0.174419</td>\n      <td>0.226903</td>\n      <td>0.288066</td>\n      <td>-0.029635</td>\n      <td>-0.074718</td>\n      <td>0.187854</td>\n      <td>-0.087730</td>\n      <td>0.017700</td>\n      <td>-0.046782</td>\n      <td>0.158881</td>\n      <td>0.374892</td>\n      <td>0.092694</td>\n      <td>0.011169</td>\n      <td>-0.255748</td>\n      <td>0.076064</td>\n      <td>-0.419004</td>\n      <td>0.365144</td>\n      <td>-0.083649</td>\n      <td>-0.024558</td>\n      <td>-0.313577</td>\n      <td>0.054032</td>\n      <td>-0.215222</td>\n      <td>0.180728</td>\n      <td>-0.301823</td>\n      <td>-0.126776</td>\n      <td>-0.187461</td>\n      <td>-0.075795</td>\n      <td>0.032114</td>\n      <td>-0.254427</td>\n      <td>0.328794</td>\n      <td>-0.013954</td>\n      <td>-0.003521</td>\n      <td>0.278634</td>\n      <td>0.030840</td>\n      <td>-0.113024</td>\n      <td>-0.084466</td>\n      <td>0.125333</td>\n      <td>0.058204</td>\n      <td>0.500939</td>\n      <td>0.179490</td>\n      <td>0.144391</td>\n      <td>0.150003</td>\n      <td>-0.091909</td>\n      <td>0.037695</td>\n      <td>-0.272316</td>\n      <td>-0.303293</td>\n      <td>0.003937</td>\n      <td>-0.216797</td>\n      <td>0.235984</td>\n      <td>-0.120352</td>\n      <td>0.165883</td>\n      <td>0.118661</td>\n      <td>-0.156175</td>\n      <td>0.253869</td>\n      <td>0.017218</td>\n      <td>0.004304</td>\n      <td>-0.313520</td>\n      <td>-0.459581</td>\n      <td>-0.117953</td>\n      <td>-0.107097</td>\n      <td>0.135096</td>\n      <td>0.264246</td>\n      <td>-2.386029</td>\n      <td>-0.454163</td>\n      <td>-0.105226</td>\n      <td>0.005814</td>\n      <td>0.252415</td>\n      <td>0.168435</td>\n      <td>0.305153</td>\n      <td>0.315089</td>\n      <td>0.295643</td>\n      <td>-0.039107</td>\n      <td>-0.310119</td>\n      <td>0.017801</td>\n      <td>-0.162417</td>\n      <td>-0.124355</td>\n      <td>-0.465788</td>\n      <td>-0.300883</td>\n      <td>-0.146278</td>\n      <td>-0.310543</td>\n      <td>-0.329809</td>\n      <td>-0.040036</td>\n      <td>-0.355430</td>\n      <td>-0.167299</td>\n      <td>0.234479</td>\n      <td>-0.241876</td>\n      <td>0.099494</td>\n      <td>0.078331</td>\n      <td>-0.189893</td>\n      <td>0.145764</td>\n      <td>0.139479</td>\n      <td>-0.303840</td>\n      <td>0.010887</td>\n      <td>0.063227</td>\n      <td>-0.132455</td>\n      <td>-0.124537</td>\n      <td>0.125451</td>\n      <td>-0.273232</td>\n      <td>0.082262</td>\n      <td>0.028749</td>\n      <td>0.308469</td>\n      <td>-0.060860</td>\n      <td>-0.004719</td>\n      <td>-0.017028</td>\n      <td>-0.134082</td>\n      <td>-0.169111</td>\n      <td>-0.202496</td>\n      <td>-0.093122</td>\n      <td>0.131769</td>\n      <td>-0.179948</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[photo, armed, gunmen, face, police, officers, near, charliehebdo, hq, paris]</td>\n      <td>-0.181514</td>\n      <td>-0.233624</td>\n      <td>-0.185539</td>\n      <td>0.307293</td>\n      <td>0.101435</td>\n      <td>-0.181207</td>\n      <td>-0.068696</td>\n      <td>-0.100285</td>\n      <td>0.443494</td>\n      <td>-0.138506</td>\n      <td>0.008871</td>\n      <td>0.052462</td>\n      <td>-0.388869</td>\n      <td>0.242770</td>\n      <td>0.294437</td>\n      <td>-0.217079</td>\n      <td>0.303464</td>\n      <td>-0.269142</td>\n      <td>-0.067638</td>\n      <td>-0.188825</td>\n      <td>0.037606</td>\n      <td>-0.128639</td>\n      <td>-0.084335</td>\n      <td>-0.114790</td>\n      <td>0.013446</td>\n      <td>0.545790</td>\n      <td>0.095857</td>\n      <td>-0.034268</td>\n      <td>0.056252</td>\n      <td>0.122503</td>\n      <td>0.091112</td>\n      <td>-0.075636</td>\n      <td>-0.126743</td>\n      <td>-0.264947</td>\n      <td>0.015786</td>\n      <td>0.260063</td>\n      <td>-0.171628</td>\n      <td>0.015498</td>\n      <td>0.250616</td>\n      <td>-0.035133</td>\n      <td>0.172644</td>\n      <td>-0.115041</td>\n      <td>-0.121451</td>\n      <td>0.244705</td>\n      <td>-0.058950</td>\n      <td>-0.198002</td>\n      <td>-0.030571</td>\n      <td>0.064429</td>\n      <td>-0.096889</td>\n      <td>-0.066357</td>\n      <td>-0.226136</td>\n      <td>0.135488</td>\n      <td>-0.097689</td>\n      <td>0.177356</td>\n      <td>-0.131988</td>\n      <td>0.103360</td>\n      <td>-0.288738</td>\n      <td>-0.004288</td>\n      <td>-0.187023</td>\n      <td>-0.023119</td>\n      <td>0.207766</td>\n      <td>0.110615</td>\n      <td>0.251113</td>\n      <td>0.070365</td>\n      <td>0.012751</td>\n      <td>0.277987</td>\n      <td>0.107870</td>\n      <td>0.152652</td>\n      <td>-0.093270</td>\n      <td>-0.129866</td>\n      <td>0.030643</td>\n      <td>-0.098664</td>\n      <td>0.202450</td>\n      <td>0.005420</td>\n      <td>0.073005</td>\n      <td>0.090581</td>\n      <td>-0.227294</td>\n      <td>-0.076028</td>\n      <td>-0.099512</td>\n      <td>-0.067837</td>\n      <td>0.550273</td>\n      <td>-0.262349</td>\n      <td>0.250284</td>\n      <td>0.281190</td>\n      <td>0.184034</td>\n      <td>-0.313200</td>\n      <td>0.067767</td>\n      <td>0.506064</td>\n      <td>-0.032822</td>\n      <td>-0.080251</td>\n      <td>-0.038839</td>\n      <td>0.276725</td>\n      <td>0.098407</td>\n      <td>0.227532</td>\n      <td>0.228674</td>\n      <td>0.233592</td>\n      <td>-0.057346</td>\n      <td>0.022558</td>\n      <td>0.088295</td>\n      <td>-0.007663</td>\n      <td>0.252566</td>\n      <td>0.083812</td>\n      <td>-0.068791</td>\n      <td>-0.164280</td>\n      <td>0.102309</td>\n      <td>-0.336351</td>\n      <td>0.201921</td>\n      <td>-0.086287</td>\n      <td>-0.095157</td>\n      <td>-0.296421</td>\n      <td>0.193912</td>\n      <td>-0.168874</td>\n      <td>-0.157039</td>\n      <td>-0.211386</td>\n      <td>0.087633</td>\n      <td>-0.262306</td>\n      <td>0.081043</td>\n      <td>-0.094733</td>\n      <td>-0.156171</td>\n      <td>0.580461</td>\n      <td>0.161057</td>\n      <td>-0.084419</td>\n      <td>0.336935</td>\n      <td>0.099181</td>\n      <td>-0.117300</td>\n      <td>-0.023067</td>\n      <td>0.155470</td>\n      <td>0.192509</td>\n      <td>0.342587</td>\n      <td>0.610529</td>\n      <td>0.188050</td>\n      <td>0.034041</td>\n      <td>-0.180169</td>\n      <td>-0.016007</td>\n      <td>-0.188245</td>\n      <td>-0.308204</td>\n      <td>-0.050512</td>\n      <td>-0.051594</td>\n      <td>-0.002449</td>\n      <td>-0.048040</td>\n      <td>0.119384</td>\n      <td>0.211429</td>\n      <td>-0.049964</td>\n      <td>0.270258</td>\n      <td>0.098857</td>\n      <td>0.079164</td>\n      <td>-0.274561</td>\n      <td>-0.135679</td>\n      <td>0.139983</td>\n      <td>-0.088009</td>\n      <td>0.246854</td>\n      <td>0.228561</td>\n      <td>-2.727972</td>\n      <td>-0.388546</td>\n      <td>-0.129818</td>\n      <td>0.006184</td>\n      <td>-0.002321</td>\n      <td>0.163799</td>\n      <td>0.264902</td>\n      <td>0.249973</td>\n      <td>0.172393</td>\n      <td>-0.102475</td>\n      <td>-0.152801</td>\n      <td>-0.091078</td>\n      <td>-0.055599</td>\n      <td>-0.054531</td>\n      <td>-0.487143</td>\n      <td>-0.499725</td>\n      <td>-0.087250</td>\n      <td>-0.246568</td>\n      <td>-0.371306</td>\n      <td>0.085531</td>\n      <td>-0.241897</td>\n      <td>0.179933</td>\n      <td>0.306464</td>\n      <td>-0.248248</td>\n      <td>0.414034</td>\n      <td>0.175461</td>\n      <td>-0.273795</td>\n      <td>0.084443</td>\n      <td>-0.113869</td>\n      <td>0.007841</td>\n      <td>-0.030129</td>\n      <td>0.185586</td>\n      <td>-0.202589</td>\n      <td>-0.078614</td>\n      <td>0.079126</td>\n      <td>-0.543667</td>\n      <td>0.232708</td>\n      <td>0.012967</td>\n      <td>0.294798</td>\n      <td>-0.061288</td>\n      <td>-0.176865</td>\n      <td>0.117911</td>\n      <td>-0.152783</td>\n      <td>-0.086652</td>\n      <td>0.067214</td>\n      <td>-0.379722</td>\n      <td>0.129359</td>\n      <td>-0.237802</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                                                                                                                          token  \\\n0                                                                          [breaking, armed, man, takes, hostage, kosher, grocery, east, paris]   \n1                                                                                         [charliehebdo, killers, dead, confirmed, gendarmerie]   \n2  [top, french, cartoonists, charb, cabu, wolinski, tignous, confirmed, among, dead, paris, charliehebdo, attack, editor, critically, wounded]   \n3                                                                          [police, surrounded, area, charliehebdo, attack, suspects, believed]   \n4                                                                 [photo, armed, gunmen, face, police, officers, near, charliehebdo, hq, paris]   \n\n   W2V_avg0  W2V_avg1  W2V_avg2  W2V_avg3  W2V_avg4  W2V_avg5  W2V_avg6  \\\n0 -0.214452  0.022334  0.199403  0.091531 -0.106364 -0.135615  0.224626   \n1 -0.255756  0.098868 -0.311535  0.198814 -0.018441 -0.018610 -0.017218   \n2 -0.204260 -0.128090 -0.056717  0.065545 -0.006312 -0.136742 -0.029758   \n3 -0.091529  0.074306 -0.349119  0.254383  0.098893  0.033803  0.152399   \n4 -0.181514 -0.233624 -0.185539  0.307293  0.101435 -0.181207 -0.068696   \n\n   W2V_avg7  W2V_avg8  W2V_avg9  W2V_avg10  W2V_avg11  W2V_avg12  W2V_avg13  \\\n0 -0.100319  0.156632 -0.080552  -0.187510   0.085141  -0.692078   0.294819   \n1  0.087670  0.270094 -0.053273   0.310100   0.167015  -0.306440  -0.123198   \n2  0.072999  0.258633 -0.305450   0.191698   0.008591  -0.348024  -0.029128   \n3 -0.016781  0.411479 -0.177106   0.010656  -0.092236  -0.792780   0.092821   \n4 -0.100285  0.443494 -0.138506   0.008871   0.052462  -0.388869   0.242770   \n\n   W2V_avg14  W2V_avg15  W2V_avg16  W2V_avg17  W2V_avg18  W2V_avg19  \\\n0   0.190482  -0.000107   0.130387  -0.086283   0.000389  -0.256427   \n1   0.087842  -0.002661  -0.128358  -0.082796   0.076324   0.020651   \n2   0.134144  -0.201011   0.095839  -0.092672   0.042344  -0.112320   \n3   0.485739  -0.044347   0.040502  -0.238916   0.004026   0.039599   \n4   0.294437  -0.217079   0.303464  -0.269142  -0.067638  -0.188825   \n\n   W2V_avg20  W2V_avg21  W2V_avg22  W2V_avg23  W2V_avg24  W2V_avg25  \\\n0   0.116084  -0.121319  -0.104006  -0.152930   0.023462   0.883121   \n1   0.082407   0.066888  -0.098685   0.004973   0.129446   0.393180   \n2   0.039853  -0.048555  -0.077608  -0.072290   0.013536   0.384435   \n3   0.095151   0.184304  -0.067949  -0.122141   0.170298   0.739074   \n4   0.037606  -0.128639  -0.084335  -0.114790   0.013446   0.545790   \n\n   W2V_avg26  W2V_avg27  W2V_avg28  W2V_avg29  W2V_avg30  W2V_avg31  \\\n0   0.178624  -0.068786  -0.162735  -0.025268  -0.081778  -0.072879   \n1  -0.069406  -0.205070  -0.127408   0.155606   0.122610  -0.065847   \n2  -0.027090  -0.071353  -0.066789   0.197552   0.231422   0.011418   \n3  -0.031164  -0.191431  -0.080367   0.027644   0.112307  -0.114560   \n4   0.095857  -0.034268   0.056252   0.122503   0.091112  -0.075636   \n\n   W2V_avg32  W2V_avg33  W2V_avg34  W2V_avg35  W2V_avg36  W2V_avg37  \\\n0   0.013534  -0.211057   0.161137   0.328094  -0.083515   0.074648   \n1  -0.284126  -0.214486  -0.319644   0.070190  -0.213362  -0.141469   \n2  -0.117731  -0.170994   0.048690   0.028248   0.025336   0.014511   \n3  -0.188454  -0.131394   0.071864   0.135815   0.041656   0.157148   \n4  -0.126743  -0.264947   0.015786   0.260063  -0.171628   0.015498   \n\n   W2V_avg38  W2V_avg39  W2V_avg40  W2V_avg41  W2V_avg42  W2V_avg43  \\\n0   0.294723  -0.128192   0.191932  -0.004869  -0.009566  -0.111364   \n1  -0.050849  -0.080936   0.289188  -0.094120   0.216460   0.115527   \n2   0.129919   0.020536   0.249458  -0.135115   0.171695   0.089334   \n3   0.030447  -0.227244   0.359573   0.144805   0.029679  -0.020854   \n4   0.250616  -0.035133   0.172644  -0.115041  -0.121451   0.244705   \n\n   W2V_avg44  W2V_avg45  W2V_avg46  W2V_avg47  W2V_avg48  W2V_avg49  \\\n0   0.023465   0.179786   0.075096  -0.021256  -0.073478   0.147782   \n1  -0.053166   0.149116  -0.058560  -0.150393   0.138688   0.325252   \n2  -0.095055  -0.006512   0.015790  -0.072966   0.001627  -0.089184   \n3  -0.003681  -0.137450   0.146878  -0.066970  -0.076150   0.075238   \n4  -0.058950  -0.198002  -0.030571   0.064429  -0.096889  -0.066357   \n\n   W2V_avg50  W2V_avg51  W2V_avg52  W2V_avg53  W2V_avg54  W2V_avg55  \\\n0  -0.470354   0.122131  -0.054976   0.097603  -0.216612   0.179325   \n1  -0.202569   0.341700  -0.014998   0.007921  -0.120828   0.078157   \n2  -0.212376   0.210792  -0.116497   0.149019  -0.092482   0.079930   \n3  -0.327932   0.278061   0.042507   0.062780   0.005333  -0.066320   \n4  -0.226136   0.135488  -0.097689   0.177356  -0.131988   0.103360   \n\n   W2V_avg56  W2V_avg57  W2V_avg58  W2V_avg59  W2V_avg60  W2V_avg61  \\\n0  -0.135947   0.018460  -0.087136   0.080001   0.104294   0.091111   \n1  -0.054125  -0.124825  -0.060872   0.051447  -0.002723   0.356174   \n2  -0.177608   0.092087  -0.006942  -0.101734   0.005013   0.093513   \n3  -0.404699   0.111456  -0.322922   0.096678  -0.112638   0.254730   \n4  -0.288738  -0.004288  -0.187023  -0.023119   0.207766   0.110615   \n\n   W2V_avg62  W2V_avg63  W2V_avg64  W2V_avg65  W2V_avg66  W2V_avg67  \\\n0   0.128683  -0.013114   0.117404  -0.043849   0.204457   0.150360   \n1  -0.039212   0.157901  -0.315652   0.137474   0.153440   0.356670   \n2   0.044676   0.082593  -0.123470   0.220523   0.246703   0.027099   \n3   0.139415   0.038544   0.271289   0.417597   0.205006   0.293944   \n4   0.251113   0.070365   0.012751   0.277987   0.107870   0.152652   \n\n   W2V_avg68  W2V_avg69  W2V_avg70  W2V_avg71  W2V_avg72  W2V_avg73  \\\n0  -0.056318  -0.097586  -0.176027  -0.034084   0.131073   0.023279   \n1   0.050584  -0.026924   0.110450   0.282987   0.018974  -0.112940   \n2  -0.074836  -0.096585   0.020706   0.007257  -0.051876   0.023567   \n3  -0.232444   0.026245   0.141199   0.033603   0.146027  -0.233019   \n4  -0.093270  -0.129866   0.030643  -0.098664   0.202450   0.005420   \n\n   W2V_avg74  W2V_avg75  W2V_avg76  W2V_avg77  W2V_avg78  W2V_avg79  \\\n0   0.092683   0.058691  -0.028683  -0.015163   0.025195   0.051936   \n1   0.055878   0.114597  -0.579636  -0.032342  -0.150035  -0.043010   \n2   0.145805   0.056249  -0.155700  -0.086659   0.213755  -0.037633   \n3   0.186124   0.353500  -0.260452  -0.149412  -0.144105  -0.145454   \n4   0.073005   0.090581  -0.227294  -0.076028  -0.099512  -0.067837   \n\n   W2V_avg80  W2V_avg81  W2V_avg82  W2V_avg83  W2V_avg84  W2V_avg85  \\\n0   0.374640  -0.091818   0.210236   0.206264  -0.073652  -0.212403   \n1   0.231848  -0.311564   0.197215   0.410206   0.059377  -0.136214   \n2   0.360630   0.009033   0.219562   0.255285   0.005478  -0.105667   \n3   0.539500  -0.294331   0.331097   0.401216   0.239260  -0.213281   \n4   0.550273  -0.262349   0.250284   0.281190   0.184034  -0.313200   \n\n   W2V_avg86  W2V_avg87  W2V_avg88  W2V_avg89  W2V_avg90  W2V_avg91  \\\n0  -0.131285   0.245320  -0.031517  -0.134470  -0.019555  -0.028470   \n1  -0.067408  -0.051222   0.049914   0.011236  -0.097720   0.023898   \n2   0.158749   0.128628  -0.087835  -0.020529  -0.016638   0.093943   \n3   0.206687   0.308451  -0.008447  -0.031517   0.174419   0.226903   \n4   0.067767   0.506064  -0.032822  -0.080251  -0.038839   0.276725   \n\n   W2V_avg92  W2V_avg93  W2V_avg94  W2V_avg95  W2V_avg96  W2V_avg97  \\\n0   0.130524   0.108279  -0.115213   0.134851   0.071441   0.061887   \n1  -0.117478   0.252436  -0.129452   0.280040   0.031374   0.093724   \n2   0.044070   0.084936   0.081614   0.113732   0.124580   0.051482   \n3   0.288066  -0.029635  -0.074718   0.187854  -0.087730   0.017700   \n4   0.098407   0.227532   0.228674   0.233592  -0.057346   0.022558   \n\n   W2V_avg98  W2V_avg99  W2V_avg100  W2V_avg101  W2V_avg102  W2V_avg103  \\\n0   0.032899  -0.012026    0.299926   -0.140451   -0.122578   -0.307877   \n1   0.011174   0.171804    0.236370    0.039298   -0.119566   -0.180323   \n2  -0.073662   0.056604    0.243867    0.033156   -0.045085   -0.041383   \n3  -0.046782   0.158881    0.374892    0.092694    0.011169   -0.255748   \n4   0.088295  -0.007663    0.252566    0.083812   -0.068791   -0.164280   \n\n   W2V_avg104  W2V_avg105  W2V_avg106  W2V_avg107  W2V_avg108  W2V_avg109  \\\n0    0.121167   -0.329101    0.065360    0.072052    0.156233   -0.119539   \n1    0.112074   -0.306025    0.075580   -0.168974    0.140822   -0.020892   \n2   -0.107638   -0.132201    0.115896   -0.070264    0.130904    0.079350   \n3    0.076064   -0.419004    0.365144   -0.083649   -0.024558   -0.313577   \n4    0.102309   -0.336351    0.201921   -0.086287   -0.095157   -0.296421   \n\n   W2V_avg110  W2V_avg111  W2V_avg112  W2V_avg113  W2V_avg114  W2V_avg115  \\\n0    0.102322   -0.157276    0.073717   -0.007854   -0.000111   -0.240686   \n1   -0.035424    0.040102    0.140417   -0.066569    0.038037    0.105540   \n2    0.005973   -0.065917    0.065685    0.011467    0.047428   -0.021803   \n3    0.054032   -0.215222    0.180728   -0.301823   -0.126776   -0.187461   \n4    0.193912   -0.168874   -0.157039   -0.211386    0.087633   -0.262306   \n\n   W2V_avg116  W2V_avg117  W2V_avg118  W2V_avg119  W2V_avg120  W2V_avg121  \\\n0   -0.007508    0.180650   -0.256752    0.362714    0.035268    0.046164   \n1    0.040056   -0.016134    0.018342    0.383671    0.270512   -0.204871   \n2    0.246528    0.115391    0.008002    0.107472   -0.051933    0.071811   \n3   -0.075795    0.032114   -0.254427    0.328794   -0.013954   -0.003521   \n4    0.081043   -0.094733   -0.156171    0.580461    0.161057   -0.084419   \n\n   W2V_avg122  W2V_avg123  W2V_avg124  W2V_avg125  W2V_avg126  W2V_avg127  \\\n0   -0.035225    0.083667   -0.241622   -0.167557    0.108498    0.164607   \n1    0.254165   -0.181773    0.064248    0.067117    0.169530   -0.092278   \n2    0.168006   -0.132772   -0.079750   -0.149816   -0.036733   -0.120097   \n3    0.278634    0.030840   -0.113024   -0.084466    0.125333    0.058204   \n4    0.336935    0.099181   -0.117300   -0.023067    0.155470    0.192509   \n\n   W2V_avg128  W2V_avg129  W2V_avg130  W2V_avg131  W2V_avg132  W2V_avg133  \\\n0    0.431350    0.315716   -0.253180    0.140676   -0.126034   -0.474589   \n1    0.262428    0.284424    0.255236    0.358990   -0.083502    0.066987   \n2    0.242908    0.084207   -0.007310    0.272619    0.019713    0.007194   \n3    0.500939    0.179490    0.144391    0.150003   -0.091909    0.037695   \n4    0.342587    0.610529    0.188050    0.034041   -0.180169   -0.016007   \n\n   W2V_avg134  W2V_avg135  W2V_avg136  W2V_avg137  W2V_avg138  W2V_avg139  \\\n0   -0.084879   -0.358264    0.066130    0.110956    0.159029   -0.050851   \n1    0.098118   -0.163968   -0.074293   -0.223430    0.205134    0.026182   \n2   -0.088377   -0.210656    0.062959   -0.126535   -0.005121   -0.047499   \n3   -0.272316   -0.303293    0.003937   -0.216797    0.235984   -0.120352   \n4   -0.188245   -0.308204   -0.050512   -0.051594   -0.002449   -0.048040   \n\n   W2V_avg140  W2V_avg141  W2V_avg142  W2V_avg143  W2V_avg144  W2V_avg145  \\\n0   -0.025120    0.258623   -0.202793   -0.207713   -0.221215   -0.004675   \n1   -0.102120    0.148810   -0.131784    0.019180    0.249378   -0.049386   \n2    0.034975    0.042599   -0.071184    0.009752   -0.002292    0.002300   \n3    0.165883    0.118661   -0.156175    0.253869    0.017218    0.004304   \n4    0.119384    0.211429   -0.049964    0.270258    0.098857    0.079164   \n\n   W2V_avg146  W2V_avg147  W2V_avg148  W2V_avg149  W2V_avg150  W2V_avg151  \\\n0    0.036970   -0.061748    0.001078    0.154303    0.091478   -0.095547   \n1   -0.381474   -0.034042   -0.132978    0.151890   -0.135441    0.137162   \n2   -0.239995   -0.043672   -0.035714    0.006066    0.054072    0.091986   \n3   -0.313520   -0.459581   -0.117953   -0.107097    0.135096    0.264246   \n4   -0.274561   -0.135679    0.139983   -0.088009    0.246854    0.228561   \n\n   W2V_avg152  W2V_avg153  W2V_avg154  W2V_avg155  W2V_avg156  W2V_avg157  \\\n0   -2.876716   -0.078268    0.229267   -0.062095   -0.024084    0.122586   \n1   -1.810772   -0.657656   -0.300113   -0.057735   -0.108980   -0.122500   \n2   -1.796350   -0.197658   -0.043255   -0.118952    0.038594    0.075118   \n3   -2.386029   -0.454163   -0.105226    0.005814    0.252415    0.168435   \n4   -2.727972   -0.388546   -0.129818    0.006184   -0.002321    0.163799   \n\n   W2V_avg158  W2V_avg159  W2V_avg160  W2V_avg161  W2V_avg162  W2V_avg163  \\\n0    0.170145    0.006689   -0.100560    0.164260   -0.060955    0.046130   \n1   -0.126700   -0.186454    0.183002   -0.067376   -0.128471   -0.193728   \n2    0.098123    0.058687    0.065406    0.010183   -0.217738   -0.055123   \n3    0.305153    0.315089    0.295643   -0.039107   -0.310119    0.017801   \n4    0.264902    0.249973    0.172393   -0.102475   -0.152801   -0.091078   \n\n   W2V_avg164  W2V_avg165  W2V_avg166  W2V_avg167  W2V_avg168  W2V_avg169  \\\n0    0.185162   -0.142742   -0.143107   -0.036575    0.335150   -0.313509   \n1    0.209042    0.056675   -0.064868   -0.166222   -0.042119   -0.357104   \n2    0.066496   -0.052564   -0.059477   -0.085470    0.018326   -0.114870   \n3   -0.162417   -0.124355   -0.465788   -0.300883   -0.146278   -0.310543   \n4   -0.055599   -0.054531   -0.487143   -0.499725   -0.087250   -0.246568   \n\n   W2V_avg170  W2V_avg171  W2V_avg172  W2V_avg173  W2V_avg174  W2V_avg175  \\\n0   -0.235394    0.058141   -0.062775    0.226615    0.019887   -0.620922   \n1   -0.097194    0.289858    0.085809   -0.027096    0.155172   -0.055576   \n2    0.002257    0.149768    0.016524    0.252526    0.145121   -0.033637   \n3   -0.329809   -0.040036   -0.355430   -0.167299    0.234479   -0.241876   \n4   -0.371306    0.085531   -0.241897    0.179933    0.306464   -0.248248   \n\n   W2V_avg176  W2V_avg177  W2V_avg178  W2V_avg179  W2V_avg180  W2V_avg181  \\\n0    0.119913   -0.163845   -0.269272    0.167251   -0.132802   -0.105065   \n1    0.218576   -0.063891    0.080436   -0.028387    0.057260   -0.058414   \n2    0.033257   -0.081941   -0.141422   -0.151762    0.087429    0.002064   \n3    0.099494    0.078331   -0.189893    0.145764    0.139479   -0.303840   \n4    0.414034    0.175461   -0.273795    0.084443   -0.113869    0.007841   \n\n   W2V_avg182  W2V_avg183  W2V_avg184  W2V_avg185  W2V_avg186  W2V_avg187  \\\n0   -0.206132    0.153061    0.089953    0.093279    0.211769   -0.351812   \n1   -0.138078    0.246572   -0.268108    0.347104    0.056461   -0.317741   \n2   -0.089263    0.032225    0.023097    0.255386   -0.009342   -0.161150   \n3    0.010887    0.063227   -0.132455   -0.124537    0.125451   -0.273232   \n4   -0.030129    0.185586   -0.202589   -0.078614    0.079126   -0.543667   \n\n   W2V_avg188  W2V_avg189  W2V_avg190  W2V_avg191  W2V_avg192  W2V_avg193  \\\n0    0.060335    0.287951    0.113106   -0.158723    0.046337   -0.102808   \n1    0.132710    0.016556   -0.035437    0.006688   -0.073772    0.061821   \n2    0.110728   -0.099069    0.080498   -0.169251   -0.068136    0.112692   \n3    0.082262    0.028749    0.308469   -0.060860   -0.004719   -0.017028   \n4    0.232708    0.012967    0.294798   -0.061288   -0.176865    0.117911   \n\n   W2V_avg194  W2V_avg195  W2V_avg196  W2V_avg197  W2V_avg198  W2V_avg199  \n0   -0.085297   -0.109255    0.093852   -0.004668    0.290299   -0.042824  \n1   -0.062575   -0.122253   -0.172704    0.100435    0.167724   -0.180706  \n2   -0.057449   -0.080394    0.086027    0.041923    0.135656   -0.139825  \n3   -0.134082   -0.169111   -0.202496   -0.093122    0.131769   -0.179948  \n4   -0.152783   -0.086652    0.067214   -0.379722    0.129359   -0.237802  "
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "df_tokens['token_vec_avg'] = copy.deepcopy(df_tokens['token'])\n",
    "\n",
    "# for index, sent in enumerate(df_tokens['token_vec_avg']):\n",
    "for index, sent in enumerate(df_tokens['token_vec_avg']):\n",
    "    df_tokens['token_vec_avg'][index] = vectorize(sent).mean(axis=0)\n",
    "\n",
    "df_temp = pd.DataFrame(df_tokens['token_vec_avg'].values.tolist()).add_prefix(\n",
    "    'W2V_avg')  # .join(df)\n",
    "df_tokens = df_tokens.join(df_temp).drop('token_vec_avg', axis=1)\n",
    "df_tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Word2Vec Averaged Vector Feature set'''\n",
    "df_train_avg.to_csv('./data/train_avg.csv', index = False)\n",
    "df_test_avg.to_csv('./data/test_avg.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rosetta",
   "language": "python",
   "name": "rosetta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}