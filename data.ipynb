{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "ds",
   "display_name": "DS",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(model, doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in model.wv.vocab]\n",
    "    return np.mean(model[doc], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_.wv.most_similar('extended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob2 import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "jsonFiles = glob('../pheme-rnr-dataset/**/source-tweet/*.json') #Can be used absolute or relative paths\n",
    "print(jsonFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfList = []\n",
    "for jsonFile in jsonFiles:\n",
    "    df = pd.read_json(jsonFile)\n",
    "    dfList.append(df)\n",
    "    \n",
    "dfTrainingDF = pd.concat(dfList, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfList = []\n",
    "\n",
    "for jsonFile in jsonFiles[0:10]:\n",
    "    df=pd.read_json(jsonFile, orient='index')\n",
    "    dfList.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(dfList, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrainingDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrainingDF.to_csv()"
   ]
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list=[]\n",
    "for jsonFile in jsonFiles[0:2]:\n",
    "    with open(jsonFile) as json_file:\n",
    "            data_list = json.load(json_file)\n",
    "    json_file\n",
    "# data_list\n",
    "# tweet_data_frame = pd.DataFrame.from_dict(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data = []\n",
    "for file in jsonFiles[0:5]:\n",
    "    \n",
    "    tweets_file = open(file, \"r\", encoding = 'utf-8')\n",
    "\n",
    "    # Read in tweets and store in list: tweets_data\n",
    "    for line in tweets_file:\n",
    "        tweet = json.loads(line)\n",
    "        tweets_data.append(tweet)\n",
    "\n",
    "    tweets_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_tweets(tweets):\n",
    "    \"\"\" Flattens out tweet dictionaries so relevant JSON is in a top-level dictionary. \"\"\"\n",
    "    \n",
    "    tweets_list = []\n",
    "    \n",
    "    # Iterate through each tweet\n",
    "    for tweet_obj in tweets:\n",
    "    \n",
    "        ''' User info'''\n",
    "        # Store the user screen name in 'user-screen_name'\n",
    "        tweet_obj['user-screen_name'] = tweet_obj['user']['screen_name']\n",
    "        \n",
    "        # Store the user location\n",
    "        tweet_obj['user-location'] = tweet_obj['user']['location']\n",
    "    \n",
    "        ''' Text info'''\n",
    "        # Check if this is a 140+ character tweet\n",
    "        if 'extended_tweet' in tweet_obj:\n",
    "            # Store the extended tweet text in 'extended_tweet-full_text'\n",
    "            tweet_obj['extended_tweet-full_text'] = \\\n",
    "                                    tweet_obj['extended_tweet']['full_text']\n",
    "    \n",
    "        if 'retweeted_status' in tweet_obj:\n",
    "            # Store the retweet user screen name in \n",
    "            # 'retweeted_status-user-screen_name'\n",
    "            tweet_obj['retweeted_status-user-screen_name'] = \\\n",
    "                        tweet_obj['retweeted_status']['user']['screen_name']\n",
    "\n",
    "            # Store the retweet text in 'retweeted_status-text'\n",
    "            tweet_obj['retweeted_status-text'] = \\\n",
    "                                        tweet_obj['retweeted_status']['text']\n",
    "    \n",
    "            if 'extended_tweet' in tweet_obj['retweeted_status']:\n",
    "                # Store the extended retweet text in \n",
    "                #'retweeted_status-extended_tweet-full_text'\n",
    "                tweet_obj['retweeted_status-extended_tweet-full_text'] = \\\n",
    "                tweet_obj['retweeted_status']['extended_tweet']['full_text']\n",
    "                \n",
    "        if 'quoted_status' in tweet_obj:\n",
    "            # Store the retweet user screen name in \n",
    "            #'retweeted_status-user-screen_name'\n",
    "            tweet_obj['quoted_status-user-screen_name'] = \\\n",
    "                            tweet_obj['quoted_status']['user']['screen_name']\n",
    "\n",
    "            # Store the retweet text in 'retweeted_status-text'\n",
    "            tweet_obj['quoted_status-text'] = \\\n",
    "                                            tweet_obj['quoted_status']['text']\n",
    "    \n",
    "            if 'extended_tweet' in tweet_obj['quoted_status']:\n",
    "                # Store the extended retweet text in \n",
    "                #'retweeted_status-extended_tweet-full_text'\n",
    "                tweet_obj['quoted_status-extended_tweet-full_text'] = \\\n",
    "                    tweet_obj['quoted_status']['extended_tweet']['full_text']\n",
    "        \n",
    "        ''' Place info'''\n",
    "        if 'place' in tweet_obj:\n",
    "            # Store the country code in 'place-country_code'\n",
    "            try:\n",
    "                tweet_obj['place-country'] = \\\n",
    "                                            tweet_obj['place']['country']\n",
    "                \n",
    "                tweet_obj['place-country_code'] = \\\n",
    "                                            tweet_obj['place']['country_code']\n",
    "                \n",
    "                tweet_obj['location-coordinates'] = \\\n",
    "                            tweet_obj['place']['bounding_box']['coordinates']\n",
    "            except: pass\n",
    "        \n",
    "        tweets_list.append(tweet_obj)\n",
    "        \n",
    "    return tweets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_text(tweets):\n",
    "    ''' Assigns the main text to only one column depending\n",
    "        on whether the tweet is a RT/quote or not'''\n",
    "    \n",
    "    tweets_list = []\n",
    "    \n",
    "    # Iterate through each tweet\n",
    "    for tweet_obj in tweets:\n",
    "        \n",
    "        if 'retweeted_status-extended_tweet-full_text' in tweet_obj:\n",
    "            tweet_obj['text'] = \\\n",
    "                        tweet_obj['retweeted_status-extended_tweet-full_text']\n",
    "        \n",
    "        elif 'retweeted_status-text' in tweet_obj:\n",
    "            tweet_obj['text'] = tweet_obj['retweeted_status-text']\n",
    "            \n",
    "        elif 'extended_tweet-full_text' in tweet_obj:\n",
    "                    tweet_obj['text'] = tweet_obj['extended_tweet-full_text']\n",
    "                \n",
    "        tweets_list.append(tweet_obj)\n",
    "        \n",
    "    return tweets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten tweets\n",
    "tweets = flatten_tweets(tweets_data)\n",
    "columns_all_text = ['text', 'extended_tweet-full_text', 'retweeted_status-text', \n",
    "           'retweeted_status-extended_tweet-full_text', 'quoted_status-text', \n",
    "           'quoted_status-extended_tweet-full_text', 'lang', 'user-location', \n",
    "           'place-country_code']\n",
    "\n",
    "# select text\n",
    "tweets = select_text(tweets)\n",
    "columns = ['text', 'lang', 'user-location', 'place-country', \n",
    "           'place-country_code', 'location-coordinates', 'user-screen_name']\n",
    "\n",
    "# Create a DataFrame from `tweets`\n",
    "df_tweets = pd.DataFrame(tweets, columns=columns)\n",
    "# replaces NaNs by Nones\n",
    "# df_tweets.where(pd.notnull(df_tweets), None, inplace=True)\n",
    "#\n",
    "df_tweets.head()"
   ]
  },
  {
   "source": [
    "# 실행 코드"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob2 import glob\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "from nltk.corpus import stopwords as stp\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import gensim\n",
    "import gensim.models.word2vec as w2v\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "jsonFiles = glob('../pheme-rnr-dataset/**/source-tweet/*.json') #Can be used absolute or relative paths\n",
    "print(jsonFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list=[]\n",
    "for jsonFile in jsonFiles[0:300]:\n",
    "    with open (jsonFile, 'r') as f:\n",
    "        for l in f.readlines():\n",
    "            if not l.strip (): # skip empty lines\n",
    "                continue\n",
    "            json_data = json.loads(l)\n",
    "            # print (json_data,\"\\n\\n\")\n",
    "            data_list.append(json_data)\n",
    "tweet_data_frame = pd.DataFrame.from_dict(data_list)"
   ]
  },
  {
   "source": [
    "tweet_data_frame.info()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_tweets(tweets):\n",
    "\n",
    "    def capitalratio(tweet_text):\n",
    "        uppers = [l for l in tweet_text if l.isupper()]\n",
    "        capitalratio = len(uppers) / len(tweet_text)\n",
    "        return capitalratio \n",
    "\n",
    "    def tweets2tokens(tweet_text):\n",
    "\n",
    "        # Tokenizing\n",
    "        tokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+','', tweet_text.lower()))\n",
    "\n",
    "        # Setting url value (whether the tweet contains http link) and filter http links\n",
    "        url=0\n",
    "        for token in tokens:\n",
    "            if token.startswith( 'http' ):\n",
    "                url=1\n",
    "        tokens = [token for token in tokens if not token.startswith('http')]\n",
    "\n",
    "        ## Stemming\n",
    "        # porter = PorterStemmer()\n",
    "        # tokens = [porter.stem(token) for token in tokens]\n",
    "\n",
    "        # Filtering Stop words\n",
    "        # from nltk.corpus import stopwords\n",
    "        # stop_words = set(stopwords.words('english'))\n",
    "        # tokens = [token for token in tokens if not token in stop_words]\n",
    "\n",
    "        return tokens,url\n",
    "\n",
    "    def contentlength(words):\n",
    "        wordcount = len(words)\n",
    "        return wordcount\n",
    "\n",
    "    \"\"\" Flattens out tweet dictionaries so relevant JSON is in a top-level dictionary. \"\"\"\n",
    "    tweets_list = []\n",
    "    total_tokens_l = []\n",
    "\n",
    "    # Iterate through each tweet\n",
    "    for tweet_obj in tweets:\n",
    "        output_f = dict()\n",
    "\n",
    "        output_f['text']= tweet_obj['text']\n",
    "        \n",
    "        output_f['text_token'], _ = tweets2tokens(tweet_obj['text'])\n",
    "        total_tokens_l.extend(tweet_obj)\n",
    "\n",
    "\n",
    "        output_f['char_count'] = len(output_f['text'])\n",
    "        output_f['word_count'] = len(output_f['text_token'])\n",
    "\n",
    "        output_f['has_question'] = \"?\" in output_f[\"text\"]\n",
    "        output_f['has_exclaim'] = \"!\" in output_f[\"text\"]\n",
    "        output_f['has_period'] = \".\" in output_f[\"text\"]\n",
    "        # hasperiod=sum(c =='.' for c in tweet_text)\n",
    "            \n",
    "    \n",
    "        ''' User info'''\n",
    "        # Store the user screen name in 'user-screen_name'\n",
    "        # output_f['user-screen_name'] = tweet_obj['user']['screen_name']\n",
    "        \n",
    "        # Store the user location\n",
    "        # output_f['user-location'] = tweet_obj['user']['location']\n",
    "\n",
    "        acc_created = datetime.strptime(tweet_obj['user']['created_at'], '%a %b %d %H:%M:%S %z %Y')\n",
    "        tweet_created = datetime.strptime(tweet_obj['created_at'], '%a %b %d %H:%M:%S %z %Y')\n",
    "        age = (tweet_created - acc_created)\n",
    "        # print(type(timedelta.total_seconds(age)))\n",
    "\n",
    "        output_f['capital_ratio']=(capitalratio(tweet_obj['text']))\n",
    "\n",
    "        # features=(capitalratio(data_list[0]['user']))\n",
    "        output_f['tweet_count'] = np.log10(tweet_obj['user']['statuses_count'])\n",
    "        output_f['listed_count'] = np.log10(tweet_obj['user']['listed_count'])\n",
    "        output_f['follow_ratio'] = np.log10(tweet_obj['user']['followers_count'])\n",
    "        output_f['age'] = int(timedelta.total_seconds(age)/86400)\n",
    "        output_f['verified'] = tweet_obj['user']['verified']\n",
    "\n",
    "        tweets_list.append(output_f)\n",
    "\n",
    "    unk_tokens_l = list(set(total_tokens_l))\n",
    "    print(len(total_tokens_l), len(unk_tokens_l)) # number of tokens and unique tokens\n",
    "\n",
    "    return tweets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(flatten_tweets(data_list))\n",
    "df.head()\n",
    "# for index, row in df.iterrows():\n",
    "#     print(df.text[index],\"\\n\")\n",
    "# df.loc[df['has_period']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus():\n",
    "    def __init__(self, train_data):\n",
    "        self.train_data = train_data\n",
    "        \n",
    "    def __iter__(self):\n",
    "        p = PorterStemmer()\n",
    "        for i in range(len(self.train_data)):\n",
    "            doc = self.train_data['text'][i]\n",
    "            doc = re.sub(r'\\S*@\\S*\\s?', '', doc, flags=re.MULTILINE) # remove email\n",
    "            doc = re.sub(r'http\\S+', '', doc, flags=re.MULTILINE) # remove web addresses\n",
    "            doc = re.sub(\"\\'\", \"\", doc) # remove single quotes\n",
    "            doc = remove_stopwords(doc)\n",
    "            doc = p.stem_sentence(doc)\n",
    "            words = simple_preprocess(doc, deacc=True)\n",
    "            yield TaggedDocument(words=words, tags=[self.train_data['documentId'][i]])"
   ]
  },
  {
   "source": [
    "---\n",
    "### Experiment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for tweet_obj in data_list[0:3]:\n",
    "    test_tx = tweet_obj['text']\n",
    "    tokens = nltk.word_tokenize(re.sub(r'[([^\\s\\w]|_ |)','', test_tx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Soldier',\n",
       " 'killed',\n",
       " 'in',\n",
       " 'Ottawa',\n",
       " 'identified',\n",
       " 'as',\n",
       " 'Cpl',\n",
       " 'Nathan',\n",
       " 'Cirillo',\n",
       " 'httptcoAOT1ZKyAei',\n",
       " 'httptcon9NyPIQ5be']"
      ]
     },
     "metadata": {},
     "execution_count": 439
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = re.sub(r'http\\S+', '', doc, flags=re.MULTILINE) # remove web addresses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-427-fd03cf282620>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([^\\s\\w]|_)+'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/DS/lib/python3.7/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+','', test))"
   ]
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Word2vec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['text'])\n",
    "# brief_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_  = w2v.Word2Vec(\n",
    "    df['text_token'],\n",
    "    sg = 1, \n",
    "    seed = 1,\n",
    "    workers = 4,\n",
    "    size = 300,\n",
    "    min_count = 2,\n",
    "    window = 5,\n",
    "    sample = 1e-3 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec_.build_vocab(df['text_token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of word vectors: {}\".format(len(word2vec_.wv.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_token'][0][0] in word2vec_.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_.wv.most_similar('extended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_.train(df['text_token'], total_examples = word2vec_.corpus_count, epochs = word2vec_.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_.wv.vectors.shape # vocab size / window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_.wv.most_similar('news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = word2vec_.wv\n",
    "vocabs = word_vectors.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocabs))\n",
    "print(len(list(word_vectors.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = word_vectors[vocabs]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vectors = word_vectors.vectors # here you load vectors for each word in your model\n",
    "w2v_indices = {word: word_vectors.vocab[word].index for word in word_vectors.vocab} # here you load indices - with whom you can find an index of the particular word in your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(line): \n",
    "    words = []\n",
    "    \n",
    "    for word in line: # line - iterable, for example list of tokens \n",
    "        try:\n",
    "            w2v_idx = w2v_indices[word]\n",
    "        except KeyError: # if you does not have a vector for this word in your w2v model, continue \n",
    "            continue\n",
    "        words.append(list(w2v_vectors[w2v_idx]))\n",
    "        if not word:\n",
    "            words.append(None)\n",
    "        # print(words)\n",
    "\n",
    "        if len(line) > len(words):\n",
    "            continue\n",
    "        # if word: \n",
    "            # words = np.asarray(words)\n",
    "            # min_vec = words.min(axis=0)\n",
    "            # max_vec = words.max(axis=0)\n",
    "            # return np.concatenate((min_vec, max_vec))\n",
    "    return np.asarray(words)\n",
    "    # print(words)\n",
    "    # print(len(line))\n",
    "    # print(len(words))\n"
   ]
  },
  {
   "source": [
    "## Word2Vec AVG"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tweet 1:  Police have clarified that there were two shootings in Ottawa today, not three: at the War Memorial and Parliament Hill.\nIndice of 'police': 16\nIndice of 'police': [ 0.10139903 -0.03351727  0.02891129 -0.04591792 -0.02251872  0.03903986\n  0.00294215 -0.05639473  0.00608043  0.09522815  0.03273718  0.09814604\n  0.11461891  0.11613031 -0.03141282 -0.01166822 -0.0044847   0.00797959\n  0.00838375 -0.059598   -0.01984595 -0.01047864 -0.0394815  -0.01080784\n  0.12610394  0.04355714 -0.02825906  0.00034501 -0.05115823  0.04958455\n -0.09103469 -0.00092993  0.01341081 -0.22750261  0.00929687  0.0981482\n -0.01679917 -0.01934557 -0.08163718 -0.03059717  0.08441687 -0.1119382\n  0.1184299  -0.11451378 -0.03607124  0.06029971  0.00718947  0.0728694\n  0.13608205 -0.1507328   0.10506455  0.07593489 -0.09304969 -0.17574425\n -0.05550925 -0.06192505  0.08443025  0.0756253   0.05127217 -0.09241643\n  0.08867007  0.01600404 -0.0835479  -0.0095776  -0.06998151  0.13640194\n  0.00058157  0.17830689 -0.09330644  0.09400454  0.02003767  0.04147178\n  0.00608706  0.06413088 -0.03583673  0.00604776  0.00707683  0.04457815\n  0.07977836 -0.01347686  0.11633208 -0.00063414 -0.07215416 -0.04598237\n  0.03449161 -0.08724544 -0.03536257  0.01641312 -0.04444678 -0.06515048\n -0.04014331  0.1253719  -0.12222203  0.04695704 -0.0321393   0.0307487\n -0.01127432 -0.02819663  0.00510105  0.14553563 -0.07635996 -0.12251627\n -0.08327296 -0.05021607  0.05478496 -0.05255079  0.07244814  0.03873646\n -0.00438687 -0.03189956 -0.036646   -0.03621199  0.04189568 -0.07637863\n  0.05248904 -0.03786936  0.10855165 -0.046671    0.02327379  0.00997682\n  0.13734905  0.18383095 -0.02174036  0.02354313 -0.02791119  0.10038981\n  0.04501195 -0.03237994 -0.00774445  0.0181197  -0.08771975 -0.03079092\n -0.05274848  0.01111792  0.05902079 -0.10906895 -0.02738804  0.07367888\n  0.12578355  0.01093061  0.05124063  0.05184231  0.12032662  0.06914324\n  0.06560644  0.0482062  -0.01691116 -0.05284312 -0.08317649  0.0357936\n -0.09885192  0.05272679  0.04424573 -0.03384991 -0.02170757  0.02392272\n  0.01187778  0.02537999  0.00204127 -0.01028949  0.07714648 -0.06275848\n  0.10041143  0.0025515  -0.04730589 -0.02054412 -0.02222556 -0.16046944\n  0.04822743 -0.177801   -0.17866458 -0.0160384   0.03981381 -0.02889658\n -0.00509821  0.03535463 -0.12311532  0.05486952  0.04270176 -0.00601199\n  0.11674734  0.02902065 -0.04238727  0.07382292  0.01515758 -0.07810314\n -0.04013016 -0.08422536 -0.08409017  0.00920414 -0.0627747  -0.04452653\n -0.08085394  0.05850697 -0.04014425 -0.0799118   0.03706058 -0.05835726\n -0.05040121 -0.01801308 -0.00482929  0.08247945 -0.03894861 -0.05583709\n -0.00106827 -0.03563359 -0.06299003 -0.10287905  0.01597045 -0.13773109\n -0.14675821 -0.0218742   0.00453442 -0.03410386 -0.00648245 -0.09753839\n  0.09450275 -0.04615381 -0.00607294  0.12724294 -0.16332102 -0.06386429\n -0.06715417 -0.02379051 -0.01072284  0.01275165 -0.03938269  0.06602693\n -0.05700236 -0.01337771  0.0056977   0.0431242  -0.1155818  -0.01997086\n -0.06366149  0.12857988 -0.01842626 -0.08052238  0.03186996  0.03264958\n -0.10741393 -0.0058059  -0.06104826 -0.03378707  0.12300016  0.0979743\n -0.0516335  -0.04621011 -0.01635609 -0.07034881  0.07938169 -0.02560537\n  0.01779488  0.0104117  -0.04229305 -0.0734923   0.04160791 -0.01252235\n -0.05107788  0.02803731  0.04307639 -0.01766596  0.05802092  0.02850341\n  0.11061641  0.00696365  0.13993567 -0.10615466  0.00397315 -0.05238522\n  0.03386676  0.08764088 -0.05229479 -0.03386717 -0.00164269 -0.0499769\n -0.0150792   0.02149104 -0.01005263 -0.13240363 -0.01422768  0.05181985\n  0.04273227  0.06601609  0.02949834  0.02127824  0.10585546 -0.17849888\n  0.06332565  0.09852748 -0.08320826 -0.07470215 -0.01337119 -0.00694211\n  0.0949338   0.01795844 -0.04758315  0.01265301  0.12496839 -0.03145103]\nIndice of 'have': 60\nIndice of 'have': [ 0.10507923 -0.03426385  0.0265589  -0.04610852 -0.02373524  0.03963395\n  0.00331351 -0.05875077  0.00721416  0.09645782  0.03638347  0.10223506\n  0.11803252  0.12081088 -0.03152765 -0.0151798  -0.00505406  0.01087797\n  0.0097775  -0.06265521 -0.01913264 -0.01271835 -0.04240088 -0.01001437\n  0.13165613  0.0447017  -0.02967887  0.00069062 -0.05096054  0.04944508\n -0.09296937 -0.00139306  0.01350075 -0.23605075  0.00780272  0.10245317\n -0.01558392 -0.02033356 -0.08387695 -0.03058648  0.09051173 -0.11450721\n  0.12308379 -0.11754724 -0.03979185  0.06401186  0.00465092  0.07295408\n  0.14077997 -0.15657309  0.10846766  0.08020186 -0.09494305 -0.1804106\n -0.05534979 -0.05997091  0.08677323  0.07744297  0.05203961 -0.09534696\n  0.08999819  0.01775906 -0.08408829 -0.00965207 -0.07498482  0.14034043\n -0.00070069  0.18293858 -0.09559344  0.0962424   0.01955256  0.04494916\n  0.00653739  0.06971505 -0.03680712  0.0068125   0.00880736  0.04674566\n  0.08505133 -0.0174672   0.11775114 -0.00178639 -0.07579472 -0.04631229\n  0.03788773 -0.09014443 -0.03550499  0.02001392 -0.04558791 -0.06697758\n -0.0403372   0.1332609  -0.1281383   0.04928415 -0.03391301  0.03050861\n -0.01274474 -0.02802309  0.00301215  0.14701618 -0.07953814 -0.12900709\n -0.08845944 -0.051705    0.05590654 -0.05287635  0.07400356  0.03702993\n -0.0049351  -0.03183063 -0.03845899 -0.03711121  0.04161176 -0.08002929\n  0.05429283 -0.03836237  0.1098625  -0.04816666  0.02184262  0.01146741\n  0.14234044  0.19014907 -0.02151016  0.02533472 -0.03061423  0.10510705\n  0.04723774 -0.03439841 -0.00780347  0.01603364 -0.09140299 -0.03344112\n -0.05359757  0.00972926  0.06133129 -0.11317501 -0.02647615  0.07666778\n  0.13083227  0.01251464  0.0538216   0.0543375   0.1238742   0.0691836\n  0.06879675  0.04905864 -0.01322397 -0.05676846 -0.08729085  0.04044059\n -0.101358    0.05488366  0.04598466 -0.03455343 -0.02255596  0.02473836\n  0.01114697  0.02839217  0.00288343 -0.010115    0.0817853  -0.064326\n  0.10466974  0.00195544 -0.04895011 -0.01809115 -0.02306687 -0.16339242\n  0.04869484 -0.17969051 -0.18550491 -0.01409475  0.04000153 -0.03220075\n -0.00445258  0.03668506 -0.12519956  0.05560581  0.04648575 -0.00830054\n  0.12276278  0.02916812 -0.04156766  0.07564547  0.01667673 -0.08275224\n -0.0426893  -0.08528238 -0.0854243   0.01090901 -0.0635315  -0.04580073\n -0.08399262  0.06113472 -0.04241432 -0.0852162   0.0383039  -0.0587841\n -0.05175784 -0.02148981 -0.00276209  0.08674519 -0.03884133 -0.06055893\n -0.00119303 -0.03830131 -0.06198334 -0.10515318  0.01687691 -0.14269388\n -0.14945854 -0.01962388  0.00388072 -0.03441262 -0.00651178 -0.0997305\n  0.09838631 -0.04653633 -0.00454703  0.13083893 -0.16659954 -0.06485292\n -0.06804042 -0.02720377 -0.01355748  0.01458823 -0.04175176  0.07164129\n -0.0601463  -0.01005644  0.00503191  0.04525725 -0.12049484 -0.01995645\n -0.06322089  0.12884726 -0.02088534 -0.08135422  0.03175863  0.03257729\n -0.10987981 -0.00777264 -0.06321688 -0.03669475  0.12871897  0.10150749\n -0.05348461 -0.04578948 -0.01905839 -0.07074234  0.08014431 -0.02790604\n  0.02137703  0.01166777 -0.04442853 -0.07625015  0.04208382 -0.01166798\n -0.05096609  0.03287594  0.04746542 -0.01674624  0.05783616  0.03054689\n  0.11252568  0.00971505  0.14506933 -0.10767303  0.00403754 -0.05547889\n  0.03611729  0.09051982 -0.05467445 -0.03791147 -0.00237737 -0.05164969\n -0.0184255   0.02170477 -0.01016909 -0.1371105  -0.01237497  0.0499933\n  0.04770757  0.06735361  0.03289699  0.02471755  0.11085986 -0.18605869\n  0.06430265  0.10131468 -0.08550923 -0.07686603 -0.01389856 -0.00382425\n  0.09498207  0.01849922 -0.04798181  0.00942182  0.12863575 -0.03067814]\n\nVector of the first headline:\n [[ 0.10139903 -0.03351727  0.02891129 ...  0.01265301  0.12496839\n  -0.03145103]\n [ 0.10507923 -0.03426385  0.0265589  ...  0.00942182  0.12863575\n  -0.03067814]\n [ 0.10543728 -0.03539509  0.02672255 ...  0.00982836  0.12682562\n  -0.03005973]\n ...\n [ 0.10272889 -0.03396662  0.02802257 ...  0.00890979  0.12641141\n  -0.02906178]\n [ 0.09987485 -0.03281913  0.02781481 ...  0.00979358  0.12469253\n  -0.02991061]\n [ 0.10086752 -0.03174698  0.02682525 ...  0.00952376  0.1212582\n  -0.02803219]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tweet 1: \", df['text'][1])\n",
    "print(\"Indice of '{}': {}\".format(df['text_token'][1][0], w2v_indices[df['text_token'][1][0]]))\n",
    "print(\"Indice of '{}': {}\".format(df['text_token'][1][0], w2v_vectors[w2v_indices[df['text_token'][1][0]]]))\n",
    "print(\"Indice of '{}': {}\".format(df['text_token'][1][1], w2v_indices[df['text_token'][1][1]]))\n",
    "print(\"Indice of '{}': {}\".format(df['text_token'][1][1], w2v_vectors[w2v_indices[df['text_token'][1][1]]]))\n",
    "print(\"\\nVector of the first headline:\\n\", vectorize(df['text_token'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "300\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                            text_token  \\\n",
       "0    [extended, dramatic, video, of, gunfire, insid...   \n",
       "1    [police, have, clarified, that, there, were, t...   \n",
       "2    [soldier, killed, in, ottawa, identified, as, ...   \n",
       "3    [norad, increases, number, of, planes, on, hig...   \n",
       "4    [all, 3, patients, injured, in, ottawashooting...   \n",
       "..                                                 ...   \n",
       "295  [2, new, victims, en, route, to, ottawa, hospi...   \n",
       "296  [developing, story, here, gunman, killed, in, ...   \n",
       "297  [police, confirm, 3rd, shooting, in, area, of,...   \n",
       "298  [gun, fire, exchange, in, parliament, hill, bu...   \n",
       "299  [soldier, shot, at, national, war, memorial, i...   \n",
       "\n",
       "                                        text_token_vec  \n",
       "0    [0.09440833, -0.031573307, 0.024093332, -0.039...  \n",
       "1    [0.1016643, -0.03303066, 0.02730084, -0.044099...  \n",
       "2    [0.098197006, -0.032494348, 0.026898533, -0.04...  \n",
       "3    [0.0980182, -0.03213273, 0.025338322, -0.04129...  \n",
       "4    [0.10602436, -0.034144532, 0.027781254, -0.044...  \n",
       "..                                                 ...  \n",
       "295  [0.10297119, -0.033143934, 0.02643264, -0.0432...  \n",
       "296  [0.09752314, -0.031733863, 0.025908297, -0.042...  \n",
       "297  [0.100909874, -0.03269396, 0.02686235, -0.0435...  \n",
       "298  [0.099031426, -0.03231357, 0.026396746, -0.042...  \n",
       "299  [0.09642967, -0.031837694, 0.027852055, -0.042...  \n",
       "\n",
       "[300 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_token</th>\n      <th>text_token_vec</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[extended, dramatic, video, of, gunfire, insid...</td>\n      <td>[0.09440833, -0.031573307, 0.024093332, -0.039...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[police, have, clarified, that, there, were, t...</td>\n      <td>[0.1016643, -0.03303066, 0.02730084, -0.044099...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[soldier, killed, in, ottawa, identified, as, ...</td>\n      <td>[0.098197006, -0.032494348, 0.026898533, -0.04...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[norad, increases, number, of, planes, on, hig...</td>\n      <td>[0.0980182, -0.03213273, 0.025338322, -0.04129...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[all, 3, patients, injured, in, ottawashooting...</td>\n      <td>[0.10602436, -0.034144532, 0.027781254, -0.044...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>295</th>\n      <td>[2, new, victims, en, route, to, ottawa, hospi...</td>\n      <td>[0.10297119, -0.033143934, 0.02643264, -0.0432...</td>\n    </tr>\n    <tr>\n      <th>296</th>\n      <td>[developing, story, here, gunman, killed, in, ...</td>\n      <td>[0.09752314, -0.031733863, 0.025908297, -0.042...</td>\n    </tr>\n    <tr>\n      <th>297</th>\n      <td>[police, confirm, 3rd, shooting, in, area, of,...</td>\n      <td>[0.100909874, -0.03269396, 0.02686235, -0.0435...</td>\n    </tr>\n    <tr>\n      <th>298</th>\n      <td>[gun, fire, exchange, in, parliament, hill, bu...</td>\n      <td>[0.099031426, -0.03231357, 0.026396746, -0.042...</td>\n    </tr>\n    <tr>\n      <th>299</th>\n      <td>[soldier, shot, at, national, war, memorial, i...</td>\n      <td>[0.09642967, -0.031837694, 0.027852055, -0.042...</td>\n    </tr>\n  </tbody>\n</table>\n<p>300 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 521
    }
   ],
   "source": [
    "import copy\n",
    "df['text_token_vec'] = copy.deepcopy(df['text_token'])\n",
    "\n",
    "for index, sentence in enumerate(df['text_token_vec']):\n",
    "    df['text_token_vec'][index] = vectorize(sentence).mean(axis=0)\n",
    "    # df['text_token_vec'][0].mean(axis=0)\n",
    "\n",
    "print(len(df['text_token_vec'][0]))\n",
    "df[['text_token','text_token_vec']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     token_avg0  token_avg1  token_avg2  token_avg3  token_avg4  token_avg5  \\\n",
       "0      0.094408   -0.031573    0.024093   -0.039885   -0.020761    0.034463   \n",
       "1      0.101664   -0.033031    0.027301   -0.044099   -0.022456    0.037482   \n",
       "2      0.098197   -0.032494    0.026899   -0.042544   -0.023967    0.037725   \n",
       "3      0.098018   -0.032133    0.025338   -0.041297   -0.022105    0.034953   \n",
       "4      0.106024   -0.034145    0.027781   -0.044783   -0.023270    0.038521   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "295    0.102971   -0.033144    0.026433   -0.043297   -0.022824    0.036866   \n",
       "296    0.097523   -0.031734    0.025908   -0.042002   -0.021429    0.036109   \n",
       "297    0.100910   -0.032694    0.026862   -0.043547   -0.022546    0.036593   \n",
       "298    0.099031   -0.032314    0.026397   -0.042046   -0.022004    0.036505   \n",
       "299    0.096430   -0.031838    0.027852   -0.042624   -0.021418    0.037092   \n",
       "\n",
       "     token_avg6  token_avg7  token_avg8  token_avg9  ...  token_avg290  \\\n",
       "0      0.002402   -0.053406    0.005045    0.084720  ...     -0.078363   \n",
       "1      0.002724   -0.057728    0.006056    0.092435  ...     -0.084164   \n",
       "2      0.001601   -0.057752    0.005589    0.088875  ...     -0.082158   \n",
       "3      0.001899   -0.055017    0.004900    0.088602  ...     -0.080414   \n",
       "4      0.003041   -0.059217    0.006548    0.096233  ...     -0.086878   \n",
       "..          ...         ...         ...         ...  ...           ...   \n",
       "295    0.002729   -0.056809    0.005611    0.093265  ...     -0.084266   \n",
       "296    0.003090   -0.055747    0.005454    0.088506  ...     -0.081122   \n",
       "297    0.002126   -0.056812    0.004750    0.091745  ...     -0.083424   \n",
       "298    0.002222   -0.056095    0.004911    0.089784  ...     -0.082073   \n",
       "299    0.002252   -0.055731    0.006368    0.087782  ...     -0.081150   \n",
       "\n",
       "     token_avg291  token_avg292  token_avg293  token_avg294  token_avg295  \\\n",
       "0       -0.067360     -0.011084     -0.004991      0.086023      0.015997   \n",
       "1       -0.072562     -0.011624     -0.005453      0.094605      0.017698   \n",
       "2       -0.069797     -0.011306     -0.003078      0.090506      0.017486   \n",
       "3       -0.069475     -0.011104     -0.004861      0.088160      0.016586   \n",
       "4       -0.075664     -0.011687     -0.004443      0.096451      0.017912   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "295     -0.073245     -0.010711     -0.005136      0.093951      0.017129   \n",
       "296     -0.069181     -0.011394     -0.004345      0.090020      0.016362   \n",
       "297     -0.072458     -0.011005     -0.005760      0.092761      0.017546   \n",
       "298     -0.070634     -0.011375     -0.004912      0.090059      0.017514   \n",
       "299     -0.068662     -0.009489     -0.004912      0.092886      0.016679   \n",
       "\n",
       "     token_avg296  token_avg297  token_avg298  token_avg299  \n",
       "0       -0.042468      0.009034      0.114680     -0.027427  \n",
       "1       -0.046654      0.010436      0.123407     -0.028992  \n",
       "2       -0.047874      0.012308      0.119222     -0.027772  \n",
       "3       -0.044894      0.009660      0.117827     -0.028134  \n",
       "4       -0.048035      0.010790      0.127890     -0.030124  \n",
       "..            ...           ...           ...           ...  \n",
       "295     -0.045773      0.010227      0.123617     -0.029140  \n",
       "296     -0.045270      0.010196      0.117876     -0.027840  \n",
       "297     -0.045969      0.010313      0.122891     -0.028898  \n",
       "298     -0.044699      0.010109      0.119792     -0.028735  \n",
       "299     -0.045165      0.010060      0.117265     -0.027187  \n",
       "\n",
       "[300 rows x 300 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token_avg0</th>\n      <th>token_avg1</th>\n      <th>token_avg2</th>\n      <th>token_avg3</th>\n      <th>token_avg4</th>\n      <th>token_avg5</th>\n      <th>token_avg6</th>\n      <th>token_avg7</th>\n      <th>token_avg8</th>\n      <th>token_avg9</th>\n      <th>...</th>\n      <th>token_avg290</th>\n      <th>token_avg291</th>\n      <th>token_avg292</th>\n      <th>token_avg293</th>\n      <th>token_avg294</th>\n      <th>token_avg295</th>\n      <th>token_avg296</th>\n      <th>token_avg297</th>\n      <th>token_avg298</th>\n      <th>token_avg299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.094408</td>\n      <td>-0.031573</td>\n      <td>0.024093</td>\n      <td>-0.039885</td>\n      <td>-0.020761</td>\n      <td>0.034463</td>\n      <td>0.002402</td>\n      <td>-0.053406</td>\n      <td>0.005045</td>\n      <td>0.084720</td>\n      <td>...</td>\n      <td>-0.078363</td>\n      <td>-0.067360</td>\n      <td>-0.011084</td>\n      <td>-0.004991</td>\n      <td>0.086023</td>\n      <td>0.015997</td>\n      <td>-0.042468</td>\n      <td>0.009034</td>\n      <td>0.114680</td>\n      <td>-0.027427</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.101664</td>\n      <td>-0.033031</td>\n      <td>0.027301</td>\n      <td>-0.044099</td>\n      <td>-0.022456</td>\n      <td>0.037482</td>\n      <td>0.002724</td>\n      <td>-0.057728</td>\n      <td>0.006056</td>\n      <td>0.092435</td>\n      <td>...</td>\n      <td>-0.084164</td>\n      <td>-0.072562</td>\n      <td>-0.011624</td>\n      <td>-0.005453</td>\n      <td>0.094605</td>\n      <td>0.017698</td>\n      <td>-0.046654</td>\n      <td>0.010436</td>\n      <td>0.123407</td>\n      <td>-0.028992</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.098197</td>\n      <td>-0.032494</td>\n      <td>0.026899</td>\n      <td>-0.042544</td>\n      <td>-0.023967</td>\n      <td>0.037725</td>\n      <td>0.001601</td>\n      <td>-0.057752</td>\n      <td>0.005589</td>\n      <td>0.088875</td>\n      <td>...</td>\n      <td>-0.082158</td>\n      <td>-0.069797</td>\n      <td>-0.011306</td>\n      <td>-0.003078</td>\n      <td>0.090506</td>\n      <td>0.017486</td>\n      <td>-0.047874</td>\n      <td>0.012308</td>\n      <td>0.119222</td>\n      <td>-0.027772</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.098018</td>\n      <td>-0.032133</td>\n      <td>0.025338</td>\n      <td>-0.041297</td>\n      <td>-0.022105</td>\n      <td>0.034953</td>\n      <td>0.001899</td>\n      <td>-0.055017</td>\n      <td>0.004900</td>\n      <td>0.088602</td>\n      <td>...</td>\n      <td>-0.080414</td>\n      <td>-0.069475</td>\n      <td>-0.011104</td>\n      <td>-0.004861</td>\n      <td>0.088160</td>\n      <td>0.016586</td>\n      <td>-0.044894</td>\n      <td>0.009660</td>\n      <td>0.117827</td>\n      <td>-0.028134</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.106024</td>\n      <td>-0.034145</td>\n      <td>0.027781</td>\n      <td>-0.044783</td>\n      <td>-0.023270</td>\n      <td>0.038521</td>\n      <td>0.003041</td>\n      <td>-0.059217</td>\n      <td>0.006548</td>\n      <td>0.096233</td>\n      <td>...</td>\n      <td>-0.086878</td>\n      <td>-0.075664</td>\n      <td>-0.011687</td>\n      <td>-0.004443</td>\n      <td>0.096451</td>\n      <td>0.017912</td>\n      <td>-0.048035</td>\n      <td>0.010790</td>\n      <td>0.127890</td>\n      <td>-0.030124</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>295</th>\n      <td>0.102971</td>\n      <td>-0.033144</td>\n      <td>0.026433</td>\n      <td>-0.043297</td>\n      <td>-0.022824</td>\n      <td>0.036866</td>\n      <td>0.002729</td>\n      <td>-0.056809</td>\n      <td>0.005611</td>\n      <td>0.093265</td>\n      <td>...</td>\n      <td>-0.084266</td>\n      <td>-0.073245</td>\n      <td>-0.010711</td>\n      <td>-0.005136</td>\n      <td>0.093951</td>\n      <td>0.017129</td>\n      <td>-0.045773</td>\n      <td>0.010227</td>\n      <td>0.123617</td>\n      <td>-0.029140</td>\n    </tr>\n    <tr>\n      <th>296</th>\n      <td>0.097523</td>\n      <td>-0.031734</td>\n      <td>0.025908</td>\n      <td>-0.042002</td>\n      <td>-0.021429</td>\n      <td>0.036109</td>\n      <td>0.003090</td>\n      <td>-0.055747</td>\n      <td>0.005454</td>\n      <td>0.088506</td>\n      <td>...</td>\n      <td>-0.081122</td>\n      <td>-0.069181</td>\n      <td>-0.011394</td>\n      <td>-0.004345</td>\n      <td>0.090020</td>\n      <td>0.016362</td>\n      <td>-0.045270</td>\n      <td>0.010196</td>\n      <td>0.117876</td>\n      <td>-0.027840</td>\n    </tr>\n    <tr>\n      <th>297</th>\n      <td>0.100910</td>\n      <td>-0.032694</td>\n      <td>0.026862</td>\n      <td>-0.043547</td>\n      <td>-0.022546</td>\n      <td>0.036593</td>\n      <td>0.002126</td>\n      <td>-0.056812</td>\n      <td>0.004750</td>\n      <td>0.091745</td>\n      <td>...</td>\n      <td>-0.083424</td>\n      <td>-0.072458</td>\n      <td>-0.011005</td>\n      <td>-0.005760</td>\n      <td>0.092761</td>\n      <td>0.017546</td>\n      <td>-0.045969</td>\n      <td>0.010313</td>\n      <td>0.122891</td>\n      <td>-0.028898</td>\n    </tr>\n    <tr>\n      <th>298</th>\n      <td>0.099031</td>\n      <td>-0.032314</td>\n      <td>0.026397</td>\n      <td>-0.042046</td>\n      <td>-0.022004</td>\n      <td>0.036505</td>\n      <td>0.002222</td>\n      <td>-0.056095</td>\n      <td>0.004911</td>\n      <td>0.089784</td>\n      <td>...</td>\n      <td>-0.082073</td>\n      <td>-0.070634</td>\n      <td>-0.011375</td>\n      <td>-0.004912</td>\n      <td>0.090059</td>\n      <td>0.017514</td>\n      <td>-0.044699</td>\n      <td>0.010109</td>\n      <td>0.119792</td>\n      <td>-0.028735</td>\n    </tr>\n    <tr>\n      <th>299</th>\n      <td>0.096430</td>\n      <td>-0.031838</td>\n      <td>0.027852</td>\n      <td>-0.042624</td>\n      <td>-0.021418</td>\n      <td>0.037092</td>\n      <td>0.002252</td>\n      <td>-0.055731</td>\n      <td>0.006368</td>\n      <td>0.087782</td>\n      <td>...</td>\n      <td>-0.081150</td>\n      <td>-0.068662</td>\n      <td>-0.009489</td>\n      <td>-0.004912</td>\n      <td>0.092886</td>\n      <td>0.016679</td>\n      <td>-0.045165</td>\n      <td>0.010060</td>\n      <td>0.117265</td>\n      <td>-0.027187</td>\n    </tr>\n  </tbody>\n</table>\n<p>300 rows × 300 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 522
    }
   ],
   "source": [
    "df_avg = pd.DataFrame(df['text_token_vec'].values.tolist()).add_prefix('token_avg')#.join(df)\n",
    "df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(model, doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in model.wv.vocab]\n",
    "    return np.mean(model[doc], axis=0)"
   ]
  },
  {
   "source": [
    "## Doc2vec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0      [extended, dramatic, video, of, gunfire, insid...\n",
       "1      [police, have, clarified, that, there, were, t...\n",
       "2      [soldier, killed, in, ottawa, identified, as, ...\n",
       "3      [norad, increases, number, of, planes, on, hig...\n",
       "4      [all, 3, patients, injured, in, ottawashooting...\n",
       "                             ...                        \n",
       "295    [2, new, victims, en, route, to, ottawa, hospi...\n",
       "296    [developing, story, here, gunman, killed, in, ...\n",
       "297    [police, confirm, 3rd, shooting, in, area, of,...\n",
       "298    [gun, fire, exchange, in, parliament, hill, bu...\n",
       "299    [soldier, shot, at, national, war, memorial, i...\n",
       "Name: text_token, Length: 300, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 442
    }
   ],
   "source": [
    "df['text_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doc2vec 실행\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(df['text_token'])]\n",
    "model = Doc2Vec(documents, vector_size=300, min_alpha=0.025, window=10, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Now training epoch 0 \nNow training epoch 2 \nNow training epoch 4 \nNow training epoch 6 \nNow training epoch 8 \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    if epoch % 2 == 0:\n",
    "        print('Now training epoch %s '%epoch)\n",
    "    model.train(documents, total_examples=model.corpus_count, epochs=1)\n",
    "    model.alpha -= 0.002\n",
    "    model.min_alpha = model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TaggedDocument(words=['police', 'have', 'clarified', 'that', 'there', 'were', 'two', 'shootings', 'in', 'ottawa', 'today', 'not', 'three', 'at', 'the', 'war', 'memorial', 'and', 'parliament', 'hill'], tags=[1])"
      ]
     },
     "metadata": {},
     "execution_count": 485
    }
   ],
   "source": [
    "# 첫번째 도큐먼트\n",
    "documents[1]\n",
    "# documents[199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "metadata": {},
     "execution_count": 484
    }
   ],
   "source": [
    "# document vector의 정보\n",
    "len(model.docvecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 478
    }
   ],
   "source": [
    "doctags = model.docvecs.doctags.items()\n",
    "doctags = sorted(doctags, key=lambda x:x[1].offset)\n",
    "doctags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, doctag in sorted(model.docvecs.doctags.items(), key=lambda x:x[1].offset):\n",
    "    print(idx, doctag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_text(index):\n",
    "    similar = model.docvecs.most_similar(index)\n",
    "\n",
    "    print(\"The quried text: \\n\\n{} \\n\\nis most similar to the text:\\n\\n{}\".format(df['text'][index],df['text'][similar[0][0]]))\n",
    "\n",
    "similar = model.docvecs.most_similar(1)\n",
    "\n",
    "# # for i, y in enumerate(similar):\n",
    "#     # print(\"{}'s '\"y[1])\n",
    "# print(\"The quried text is most similar to the text\\n\\n{}\".format(df['text'][similar[0][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The quried text: \n\nSenior U.S. official: Canadian government has informed U.S. that one shooter is dead in Ottawa. Live blog: http://t.co/q98AMohu7T \n\nis most similar to the text:\n\nA Canadian soldier who was standing on guard for thee is gone. Our thoughts are w everyone affected by the shootings in Ottawa #CanadaStrong\n"
     ]
    }
   ],
   "source": [
    "most_similar_text(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "280"
      ]
     },
     "metadata": {},
     "execution_count": 590
    }
   ],
   "source": [
    "similar[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model.infer_vector([\"system\", \"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectors = model.docvecs\n",
    "vocabs = doc_vectors.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Doc2VecKeyedVectors' object has no attribute 'wv'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-483-8529ff69eadd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Doc2VecKeyedVectors' object has no attribute 'wv'"
     ]
    }
   ],
   "source": [
    "model.docvecs.wv"
   ]
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(model, doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [vec for vec in doc if vec in model.docvecs]\n",
    "    return model[doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vector(model, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [(i, model.docvecs[i]) for i, j in enumerate(df['text_token'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_doc = []\n",
    "for index, tokens in enumerate(df['text_token']):\n",
    "    list_doc.append(model.docvecs[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_doc = pd.DataFrame(list_doc).add_prefix('doc_vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     doc_vec0  doc_vec1  doc_vec2  doc_vec3  doc_vec4  doc_vec5  doc_vec6  \\\n",
       "0    0.001174 -0.002209  0.001276 -0.001821 -0.000796 -0.000253  0.002193   \n",
       "1    0.000072 -0.000643  0.001412 -0.000262 -0.001581 -0.000478  0.002305   \n",
       "2   -0.000422  0.000378  0.000531  0.000098  0.000671  0.001367  0.000605   \n",
       "3    0.003968 -0.001037 -0.000448 -0.000493 -0.000122  0.001622  0.002108   \n",
       "4    0.001743 -0.002971  0.002050  0.000865 -0.001889 -0.000670  0.001920   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "295  0.008864 -0.004854  0.002666  0.000149 -0.003610  0.003660  0.009176   \n",
       "296  0.005015 -0.002488  0.000023 -0.000325 -0.000288  0.001626  0.002102   \n",
       "297  0.008710 -0.005446 -0.000367 -0.000711 -0.003312  0.002145  0.007968   \n",
       "298  0.007198 -0.002597  0.000567 -0.000610 -0.001103  0.002521  0.004766   \n",
       "299 -0.001230 -0.000553 -0.001099 -0.001580 -0.000147 -0.000508  0.000226   \n",
       "\n",
       "     doc_vec7  doc_vec8  doc_vec9  ...  doc_vec290  doc_vec291  doc_vec292  \\\n",
       "0   -0.003003  0.000298  0.003488  ...   -0.001035   -0.000250    0.001629   \n",
       "1   -0.000424  0.000885  0.002826  ...   -0.002484   -0.001614   -0.000466   \n",
       "2    0.000540  0.000643 -0.001153  ...   -0.001668   -0.001710    0.001587   \n",
       "3   -0.002218  0.000243  0.003003  ...   -0.001377   -0.001745    0.001073   \n",
       "4   -0.003183 -0.000968  0.001418  ...   -0.001513   -0.001383    0.000214   \n",
       "..        ...       ...       ...  ...         ...         ...         ...   \n",
       "295 -0.013051 -0.004851  0.012053  ...   -0.008705   -0.006912    0.001525   \n",
       "296 -0.003017 -0.001379  0.002432  ...   -0.005066   -0.001973   -0.000018   \n",
       "297 -0.008544 -0.003830  0.007356  ...   -0.007832   -0.006358   -0.000395   \n",
       "298 -0.005498 -0.001745  0.004327  ...   -0.003289   -0.005891   -0.000416   \n",
       "299 -0.000922  0.000483  0.000170  ...    0.000982    0.001603   -0.000622   \n",
       "\n",
       "     doc_vec293  doc_vec294  doc_vec295  doc_vec296  doc_vec297  doc_vec298  \\\n",
       "0      0.000343    0.002200    0.000286   -0.001029    0.001021    0.004979   \n",
       "1     -0.001706    0.001469   -0.000877   -0.000901    0.000994    0.005010   \n",
       "2     -0.001262    0.000716   -0.000444    0.000353   -0.001152    0.000395   \n",
       "3     -0.001699    0.005931   -0.000040   -0.001468   -0.000238    0.008251   \n",
       "4      0.000514    0.005464   -0.000371   -0.001379   -0.000717    0.007901   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "295   -0.001280    0.019738   -0.004820   -0.009305   -0.003422    0.026781   \n",
       "296    0.000307    0.007681   -0.001885   -0.004186   -0.000287    0.010256   \n",
       "297    0.000080    0.012673   -0.002346   -0.004022   -0.004028    0.020328   \n",
       "298   -0.001990    0.008526   -0.000690   -0.005000   -0.000398    0.014628   \n",
       "299    0.000547    0.000781   -0.000932    0.000827   -0.001151   -0.000654   \n",
       "\n",
       "     doc_vec299  \n",
       "0     -0.003107  \n",
       "1     -0.000370  \n",
       "2      0.000124  \n",
       "3     -0.003180  \n",
       "4     -0.001406  \n",
       "..          ...  \n",
       "295   -0.008457  \n",
       "296   -0.002379  \n",
       "297   -0.006236  \n",
       "298   -0.004342  \n",
       "299    0.000413  \n",
       "\n",
       "[300 rows x 300 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>doc_vec0</th>\n      <th>doc_vec1</th>\n      <th>doc_vec2</th>\n      <th>doc_vec3</th>\n      <th>doc_vec4</th>\n      <th>doc_vec5</th>\n      <th>doc_vec6</th>\n      <th>doc_vec7</th>\n      <th>doc_vec8</th>\n      <th>doc_vec9</th>\n      <th>...</th>\n      <th>doc_vec290</th>\n      <th>doc_vec291</th>\n      <th>doc_vec292</th>\n      <th>doc_vec293</th>\n      <th>doc_vec294</th>\n      <th>doc_vec295</th>\n      <th>doc_vec296</th>\n      <th>doc_vec297</th>\n      <th>doc_vec298</th>\n      <th>doc_vec299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.001174</td>\n      <td>-0.002209</td>\n      <td>0.001276</td>\n      <td>-0.001821</td>\n      <td>-0.000796</td>\n      <td>-0.000253</td>\n      <td>0.002193</td>\n      <td>-0.003003</td>\n      <td>0.000298</td>\n      <td>0.003488</td>\n      <td>...</td>\n      <td>-0.001035</td>\n      <td>-0.000250</td>\n      <td>0.001629</td>\n      <td>0.000343</td>\n      <td>0.002200</td>\n      <td>0.000286</td>\n      <td>-0.001029</td>\n      <td>0.001021</td>\n      <td>0.004979</td>\n      <td>-0.003107</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000072</td>\n      <td>-0.000643</td>\n      <td>0.001412</td>\n      <td>-0.000262</td>\n      <td>-0.001581</td>\n      <td>-0.000478</td>\n      <td>0.002305</td>\n      <td>-0.000424</td>\n      <td>0.000885</td>\n      <td>0.002826</td>\n      <td>...</td>\n      <td>-0.002484</td>\n      <td>-0.001614</td>\n      <td>-0.000466</td>\n      <td>-0.001706</td>\n      <td>0.001469</td>\n      <td>-0.000877</td>\n      <td>-0.000901</td>\n      <td>0.000994</td>\n      <td>0.005010</td>\n      <td>-0.000370</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.000422</td>\n      <td>0.000378</td>\n      <td>0.000531</td>\n      <td>0.000098</td>\n      <td>0.000671</td>\n      <td>0.001367</td>\n      <td>0.000605</td>\n      <td>0.000540</td>\n      <td>0.000643</td>\n      <td>-0.001153</td>\n      <td>...</td>\n      <td>-0.001668</td>\n      <td>-0.001710</td>\n      <td>0.001587</td>\n      <td>-0.001262</td>\n      <td>0.000716</td>\n      <td>-0.000444</td>\n      <td>0.000353</td>\n      <td>-0.001152</td>\n      <td>0.000395</td>\n      <td>0.000124</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.003968</td>\n      <td>-0.001037</td>\n      <td>-0.000448</td>\n      <td>-0.000493</td>\n      <td>-0.000122</td>\n      <td>0.001622</td>\n      <td>0.002108</td>\n      <td>-0.002218</td>\n      <td>0.000243</td>\n      <td>0.003003</td>\n      <td>...</td>\n      <td>-0.001377</td>\n      <td>-0.001745</td>\n      <td>0.001073</td>\n      <td>-0.001699</td>\n      <td>0.005931</td>\n      <td>-0.000040</td>\n      <td>-0.001468</td>\n      <td>-0.000238</td>\n      <td>0.008251</td>\n      <td>-0.003180</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.001743</td>\n      <td>-0.002971</td>\n      <td>0.002050</td>\n      <td>0.000865</td>\n      <td>-0.001889</td>\n      <td>-0.000670</td>\n      <td>0.001920</td>\n      <td>-0.003183</td>\n      <td>-0.000968</td>\n      <td>0.001418</td>\n      <td>...</td>\n      <td>-0.001513</td>\n      <td>-0.001383</td>\n      <td>0.000214</td>\n      <td>0.000514</td>\n      <td>0.005464</td>\n      <td>-0.000371</td>\n      <td>-0.001379</td>\n      <td>-0.000717</td>\n      <td>0.007901</td>\n      <td>-0.001406</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>295</th>\n      <td>0.008864</td>\n      <td>-0.004854</td>\n      <td>0.002666</td>\n      <td>0.000149</td>\n      <td>-0.003610</td>\n      <td>0.003660</td>\n      <td>0.009176</td>\n      <td>-0.013051</td>\n      <td>-0.004851</td>\n      <td>0.012053</td>\n      <td>...</td>\n      <td>-0.008705</td>\n      <td>-0.006912</td>\n      <td>0.001525</td>\n      <td>-0.001280</td>\n      <td>0.019738</td>\n      <td>-0.004820</td>\n      <td>-0.009305</td>\n      <td>-0.003422</td>\n      <td>0.026781</td>\n      <td>-0.008457</td>\n    </tr>\n    <tr>\n      <th>296</th>\n      <td>0.005015</td>\n      <td>-0.002488</td>\n      <td>0.000023</td>\n      <td>-0.000325</td>\n      <td>-0.000288</td>\n      <td>0.001626</td>\n      <td>0.002102</td>\n      <td>-0.003017</td>\n      <td>-0.001379</td>\n      <td>0.002432</td>\n      <td>...</td>\n      <td>-0.005066</td>\n      <td>-0.001973</td>\n      <td>-0.000018</td>\n      <td>0.000307</td>\n      <td>0.007681</td>\n      <td>-0.001885</td>\n      <td>-0.004186</td>\n      <td>-0.000287</td>\n      <td>0.010256</td>\n      <td>-0.002379</td>\n    </tr>\n    <tr>\n      <th>297</th>\n      <td>0.008710</td>\n      <td>-0.005446</td>\n      <td>-0.000367</td>\n      <td>-0.000711</td>\n      <td>-0.003312</td>\n      <td>0.002145</td>\n      <td>0.007968</td>\n      <td>-0.008544</td>\n      <td>-0.003830</td>\n      <td>0.007356</td>\n      <td>...</td>\n      <td>-0.007832</td>\n      <td>-0.006358</td>\n      <td>-0.000395</td>\n      <td>0.000080</td>\n      <td>0.012673</td>\n      <td>-0.002346</td>\n      <td>-0.004022</td>\n      <td>-0.004028</td>\n      <td>0.020328</td>\n      <td>-0.006236</td>\n    </tr>\n    <tr>\n      <th>298</th>\n      <td>0.007198</td>\n      <td>-0.002597</td>\n      <td>0.000567</td>\n      <td>-0.000610</td>\n      <td>-0.001103</td>\n      <td>0.002521</td>\n      <td>0.004766</td>\n      <td>-0.005498</td>\n      <td>-0.001745</td>\n      <td>0.004327</td>\n      <td>...</td>\n      <td>-0.003289</td>\n      <td>-0.005891</td>\n      <td>-0.000416</td>\n      <td>-0.001990</td>\n      <td>0.008526</td>\n      <td>-0.000690</td>\n      <td>-0.005000</td>\n      <td>-0.000398</td>\n      <td>0.014628</td>\n      <td>-0.004342</td>\n    </tr>\n    <tr>\n      <th>299</th>\n      <td>-0.001230</td>\n      <td>-0.000553</td>\n      <td>-0.001099</td>\n      <td>-0.001580</td>\n      <td>-0.000147</td>\n      <td>-0.000508</td>\n      <td>0.000226</td>\n      <td>-0.000922</td>\n      <td>0.000483</td>\n      <td>0.000170</td>\n      <td>...</td>\n      <td>0.000982</td>\n      <td>0.001603</td>\n      <td>-0.000622</td>\n      <td>0.000547</td>\n      <td>0.000781</td>\n      <td>-0.000932</td>\n      <td>0.000827</td>\n      <td>-0.001151</td>\n      <td>-0.000654</td>\n      <td>0.000413</td>\n    </tr>\n  </tbody>\n</table>\n<p>300 rows × 300 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 575
    }
   ],
   "source": [
    "pd_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=pd.DataFrame(result[:][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Wrong number of items passed 2, placement implies 1",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/DS/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'doc_vec'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/DS/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3573\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3574\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3575\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DS/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'doc_vec'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-526-6f7721d41eeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doc_vec'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/DS/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3038\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3039\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3040\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DS/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3116\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3117\u001b[0;31m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3119\u001b[0m         \u001b[0;31m# check if we are modifying a copy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DS/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3575\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3576\u001b[0m             \u001b[0;31m# This item wasn't present, just insert at end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3577\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3578\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DS/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, item, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_reshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m         \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblkno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_fast_count_smallints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblknos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DS/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype)\u001b[0m\n\u001b[1;32m   2720\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatetimeArray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simple_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2722\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DS/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim)\u001b[0m\n\u001b[1;32m   2376\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2378\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2380\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DS/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_ndim\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             raise ValueError(\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0;34mf\"Wrong number of items passed {len(self.values)}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m                 \u001b[0;34mf\"placement implies {len(self.mgr_locs)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Wrong number of items passed 2, placement implies 1"
     ]
    }
   ],
   "source": [
    "df['doc_vec'] = np.asarray(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0      [1, [7.1845294e-05, -0.00064286514, 0.00141188...\n",
       "1      [0.1016643, -0.03303066, 0.02730084, -0.044099...\n",
       "2      [0.098197006, -0.032494348, 0.026898533, -0.04...\n",
       "3      [0.0980182, -0.03213273, 0.025338322, -0.04129...\n",
       "4      [0.10602436, -0.034144532, 0.027781254, -0.044...\n",
       "                             ...                        \n",
       "295    [0.10297119, -0.033143934, 0.02643264, -0.0432...\n",
       "296    [0.09752314, -0.031733863, 0.025908297, -0.042...\n",
       "297    [0.100909874, -0.03269396, 0.02686235, -0.0435...\n",
       "298    [0.099031426, -0.03231357, 0.026396746, -0.042...\n",
       "299    [0.09642967, -0.031837694, 0.027852055, -0.042...\n",
       "Name: text_token_vec, Length: 300, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 519
    }
   ],
   "source": [
    "df['text_token_vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "DataFrame constructor not properly called!",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-492-add0102ac0ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/DS/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DataFrame constructor not properly called!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: DataFrame constructor not properly called!"
     ]
    }
   ],
   "source": [
    "doc = [word for word in doc if word in model.docvecs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, sentence in enumerate(df['text_token_vec']):\n",
    "    df['text_token_vec'][index] = vectorize(sentence).mean(axis=0)\n",
    "    # df['text_token_vec'][0].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'doc_vec'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/DS/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'doc_vec'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-491-737c0130e57c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doc_vec'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/DS/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2902\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2903\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DS/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'doc_vec'"
     ]
    }
   ],
   "source": [
    "for index, sentence in enumerate(df['text']):\n",
    "    df['doc_vec'][index] = 0"
   ]
  },
  {
   "source": [
    "# Content based Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalratio(tweet_text):\n",
    "    uppers = [l for l in tweet_text if l.isupper()]\n",
    "    capitalratio = len(uppers) / len(tweet_text)\n",
    "    print(\"len of the text is: \",len(tweet_text))\n",
    "    print(\"len of the upper is: \",len(uppers))\n",
    "    return capitalratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=dict()\n",
    "twtx = data_list[1]['text']\n",
    "features = capitalratio(twtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "source": [
    "## Word tokens"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "from nltk.corpus import stopwords as stp\n",
    "# from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets2tokens(tweet_text):\n",
    "        tokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+','', tweet_text.lower()))\n",
    "        url=0\n",
    "        for token in tokens:\n",
    "            if token.startswith( 'http' ):\n",
    "                url=1\n",
    "\n",
    "        return tokens,url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens,url=tweets2tokens(twtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url"
   ]
  },
  {
   "source": [
    "Transfer Leraning을 해야하나? 300개 디멘션이라는데"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}