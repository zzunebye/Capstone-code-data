{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행시 등장하는 URL을 클릭하여 허용해주면 인증KEY가 나타난다. 복사하여 URL아래 빈칸에 붙여넣으면 마운트에 성공하게된다.\n",
    "from google.colab import drive\n",
    "drive.mount('./MyDrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd MyDrive/MyDrive/Capstone/code_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/june/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/june/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob2 import glob\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import gensim\n",
    "import gensim.models.word2vec as w2v\n",
    "from gensim.test.utils import common_texts\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "\n",
    "from nltk import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', 400)\n",
    "# pd.set_option('display.max_rowwidth', 100)\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmt = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "freqdist = nltk.FreqDist()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "\"\"\" Replaces contractions from a string to their equivalents \"\"\"\n",
    "contraction_patterns = [ (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "                         (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not'), \n",
    "                         (r'i\\'d', 'i would'), (r'I\\'d', 'I would'), (r'he\\'d', 'he would'), (r'she\\'d', 'she would'), (r'they\\'d', 'they would'), (r'we\\'d', 'we would')]\n",
    "def replaceContraction(text):\n",
    "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
    "    for (pattern, repl) in patterns:\n",
    "        (text, count) = re.subn(pattern, repl, text)\n",
    "    return text\n",
    "\n",
    "def capitalratio(tweet_text):\n",
    "    uppers = [l for l in tweet_text if l.isupper()]\n",
    "    capitalratio = len(uppers) / len(tweet_text)\n",
    "    return capitalratio \n",
    "\n",
    "def getTokenization(sent):\n",
    "    tweet_tokens = []\n",
    "    sent = sent.lower()\n",
    "    sent = replaceContraction(sent)\n",
    "\n",
    "    sent = re.sub(r\"http\\S+\", \"*\", sent) # http link -> '*'\n",
    "    # sent = re.sub(r\"@\\S+\", \"@\", sent)   # mention -> '@'\n",
    "    sent = re.sub(r\"@[^\\s]+\", \"@\", sent)   # mention -> '@'\n",
    "    sent = re.sub(r\"(#)(\\S+)\", r'\\1 \\2', sent) \n",
    "\n",
    "    sent = re.sub(r'([^\\s\\w@#\\*]|_)+', '', sent) # Erasing Special Characters\n",
    "    # sent = re.sub('@[^\\s]+','atUser',sent)\n",
    "    # sent = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','url',sent)\n",
    "    # sent = re.sub(r'#([^\\s]+)', r'\\1', sent)\n",
    "\n",
    "\n",
    "    # sent = re.sub('', '', sent.lower())\n",
    "    # sent = [tweet_tokenizer.tokenize(sent)]\n",
    "    sent = tweet_tokenizer.tokenize(sent)\n",
    "    sent = [stemmer.stem(token) for token in sent]\n",
    "    sent = [lmt.lemmatize(token) for token in sent]\n",
    "\n",
    "    temp = [token for token in sent if not token in stop_words]\n",
    "    url=0\n",
    "    for token in temp:\n",
    "        if token.startswith('*'):\n",
    "            url+=1\n",
    "    # tweet_tokens.append([temp])\n",
    "    # tweet_tokens.append(tweet_tokenizer.tokenize(sent))\n",
    "    # df_tokens = pd.DataFrame(tweet_tokens, columns=['token'])\n",
    "    return temp, url\n",
    "\n",
    "def extract_urls(entities_dicts):\n",
    "    if len(entities_dicts) < 1:\n",
    "        return 0\n",
    "    if len(entities_dicts) == 1:\n",
    "        return 1\n",
    "    if len(entities_dicts) == 2:\n",
    "        return 2\n",
    "\n",
    "    # urls = []\n",
    "    # urls_expanded = []\n",
    "\n",
    "    # key = 'url'\n",
    "    # key2 = 'expanded_url'\n",
    "    # # print(len(entities_dict))\n",
    "    # for i in entities_dicts:\n",
    "    #     urls.append(i[key])\n",
    "    #     urls_expanded.append(i[key2])\n",
    "    # return 1, urls, urls_expanded\n",
    "\n",
    "def getposcount(tokens):\n",
    "    postag = []\n",
    "    poscount = {}\n",
    "    poscount['Noun']=0\n",
    "    poscount['Verb']=0\n",
    "    poscount['Adjective'] = 0\n",
    "    poscount['Pronoun']=0\n",
    "    poscount['FirstPersonPronoun']=0\n",
    "    poscount['SecondPersonPronoun']=0\n",
    "    poscount['ThirdPersonPronoun']=0\n",
    "    poscount['Adverb']=0\n",
    "    poscount['Numeral']=0\n",
    "    poscount['Conjunction_inj']=0\n",
    "    poscount['Particle']=0\n",
    "    poscount['Determiner']=0\n",
    "    poscount['Modal']=0\n",
    "    poscount['Whs']=0\n",
    "\n",
    "    Nouns = {'NN','NNS','NNP','NNPS'}\n",
    "    Adverbs = {'RB','RBR','RBS'}\n",
    "    Whs = {'WDT','WP','WRB'} # Composition of wh-determiner(that,what), wh-pronoun(who), wh-adverb(how)\n",
    "    Verbs={'VB','VBP','VBZ','VBN','VBG','VBD','To'}\n",
    "    first_person_pronouns=['i','I','me','my','mine','we','us','our','ours'] #'i',\n",
    "    second_person_pronouns=['you','your','yours', 'ya']\n",
    "    third_person_pronouns=['he','she','it','him','her','it','his','hers','its','they','them','their','theirs']\n",
    "\n",
    "    for word in tokens:\n",
    "        w_lower=word.lower()\n",
    "        if w_lower in first_person_pronouns:\n",
    "            poscount['FirstPersonPronoun']+=1\n",
    "        elif w_lower in second_person_pronouns:\n",
    "            poscount['SecondPersonPronoun']+=1\n",
    "        elif w_lower in third_person_pronouns:\n",
    "            poscount['ThirdPersonPronoun']+=1\n",
    "    \n",
    "    postag = nltk.pos_tag(tokens)\n",
    "    for g1 in postag:\n",
    "        if g1[1] in Nouns:\n",
    "            poscount['Noun'] += 1\n",
    "        elif g1[1] in Verbs:\n",
    "            poscount['Verb']+= 1\n",
    "        elif g1[1]=='ADJ'or g1[1]=='JJ':\n",
    "            poscount['Adjective']+=1\n",
    "        elif g1[1]=='PRP' or g1[1]=='PRON' or g1[1]=='PRP$':\n",
    "            poscount['Pronoun']+=1\n",
    "        elif g1[1] in Adverbs or g1[1]=='ADV':\n",
    "            poscount['Adverb']+=1\n",
    "        elif g1[1]=='CD':\n",
    "            poscount['Numeral']+=1\n",
    "        elif g1[1]=='CC' or g1[1]=='IN':\n",
    "            poscount['Conjunction_inj']+=1\n",
    "        elif g1[1]=='RP':\n",
    "            poscount['Particle']+=1\n",
    "        elif g1[1]=='MD':\n",
    "            poscount['Modal']+=1\n",
    "        elif g1[1]=='DT':\n",
    "            poscount['Determiner']+=1\n",
    "        elif g1[1] in Whs:\n",
    "            poscount['Whs']+=1\n",
    "    return poscount\n",
    "\n",
    "def fetchRawText(path, events, tweetType):\n",
    "    jsons = []\n",
    "    for i, event in enumerate(events):\n",
    "        jsons.append(glob('%s/%s/**/%s/*.json' % (path, event,tweetType)))\n",
    "    for i,d in enumerate(jsons): print(\"%s's length is %d\" %(events[i], len(d)))\n",
    "\n",
    "    targets = []\n",
    "    features = []\n",
    "    for index, dataset in enumerate(jsons):\n",
    "        targetEvent = []\n",
    "        dataEvent = []\n",
    "        count = 0  # help var\n",
    "        for jsonFile in dataset:\n",
    "            count += 1\n",
    "            if jsonFile.find(\"non-rumours\") == -1:\n",
    "                targetEvent.append(1)\n",
    "            else:\n",
    "                targetEvent.append(0)\n",
    "\n",
    "            with open(jsonFile, 'r') as f:\n",
    "                for l in f.readlines():\n",
    "                    if not l.strip():  # skip empty lines\n",
    "                        continue\n",
    "                    try:\n",
    "                        json_data = json.loads(l)\n",
    "                    except:\n",
    "                        print (l,\"\\n\\n\")\n",
    "                        break\n",
    "                    dataEvent.append(json_data)\n",
    "        print(index, events[index], len(targetEvent), len(dataEvent))\n",
    "        targets.append(targetEvent)\n",
    "        features.append(dataEvent)\n",
    "\n",
    "    # print(\"\\nNumber of Events:\", len(targets))\n",
    "    # print(\"Number of tweets in the first event:\", len(targets[0]))\n",
    "\n",
    "    # targets은 targetEvent들을 리스트에 담은 것\n",
    "    target_list = []\n",
    "    for event in targets:\n",
    "        for elem in event:\n",
    "            target_list.append(elem)\n",
    "    target = pd.DataFrame(target_list, columns=[\"target\"])\n",
    "\n",
    "    extracted_features = []\n",
    "\n",
    "    extracted = []\n",
    "\n",
    "    for obj_list in features:\n",
    "        extracted_event = []\n",
    "        for obj in obj_list:\n",
    "            output_f = dict()\n",
    "            output_f['text'] = obj['text']\n",
    "            urls_dicts = obj['entities']['urls']\n",
    "            output_f['URLcount'] = extract_urls(urls_dicts)\n",
    "        \n",
    "            # print(type(obj['user']))\n",
    "            # print(obj['user'].contains_key('entities'))\n",
    "            # if ('url' in obj['user']):\n",
    "            #     output_f['hasUserURL'] = 1\n",
    "            #     output_f['user_url'] = 1 if (obj['user']['url'] != None) else 0\n",
    "            # elif ('entities' in obj['user']):\n",
    "            #     output_f['user_entity'] = obj['user']['entities']['url']['urls']\n",
    "            #     # print(obj['user']['entities']['url']['urls'])\n",
    "            #     output_f['user_url'] = obj['user']['entities']['expanded_url']\n",
    "            #     output_f['hasUserURL'] , _ , output_f['user_url'] = extract_urls(obj['user']['entities']['url']['urls'])\n",
    "            # else:\n",
    "            #     # output_f['user_entity'] = None\n",
    "            #     output_f['user_url'] = 0\n",
    "            #     output_f['hasUserURL'] = 0\n",
    "            \n",
    "\n",
    "            output_f['text_token'], output_f['URLcount'] = getTokenization(obj['text'])\n",
    "            '''POS Tagging and text cleansing for POS'''\n",
    "            temp = output_f['text']\n",
    "            temp=  emoji.demojize(temp)\n",
    "            temp = re.sub(r\"(#)(\\S+)\", r'\\1 \\2', temp)\n",
    "            temp = re.sub(r\"http\\S+\", \"\", temp)\n",
    "            temp = replaceContraction(temp.lower())\n",
    "            temp = temp.split()\n",
    "            pos_dict=getposcount(temp)\n",
    "            output_f.update(pos_dict)\n",
    "            output_f['emoji_count'] = emoji.emoji_count(obj['text'])\n",
    "\n",
    "            output_f['char_count'] = len(output_f['text'])\n",
    "            output_f['word_count'] = len(output_f['text'].split())\n",
    "\n",
    "            output_f['has_question'] = \"?\" in output_f[\"text\"]\n",
    "            output_f['has_exclaim'] = \"!\" in output_f[\"text\"]\n",
    "            output_f['has_period'] = \".\" in output_f[\"text\"]\n",
    "\n",
    "            output_f['capital_ratio']=(capitalratio(obj['text']))\n",
    "            output_f['retweet_count'] = obj['retweet_count']\n",
    "            output_f['tweet_count'] = np.log10(obj['user']['statuses_count'])\n",
    "            output_f['listed_count'] = np.log10(obj['user']['listed_count'])\n",
    "            output_f['friends_count'] = np.log10(obj['user']['friends_count'])\n",
    "            output_f['follow_ratio'] = np.log10(obj['user']['followers_count'])\n",
    "\n",
    "            acc_created = datetime.strptime(obj['user']['created_at'], '%a %b %d %H:%M:%S %z %Y')\n",
    "            tweet_created = datetime.strptime(obj['created_at'], '%a %b %d %H:%M:%S %z %Y')\n",
    "            age = (tweet_created - acc_created)\n",
    "            output_f['account_age_days'] = age.days\n",
    "            \n",
    "            output_f['capital_ratio']=(capitalratio(obj['text']))\n",
    "            output_f['verified'] = obj['user']['verified']\n",
    "\n",
    "            extracted_event.append(output_f)\n",
    "        extracted_features.append(extracted_event)\n",
    "\n",
    "    extracted_df = []\n",
    "    for i, data in enumerate(extracted_features):\n",
    "        temp = pd.DataFrame(data)\n",
    "        temp[\"Event\"] = events[i]\n",
    "        extracted_df.append(pd.DataFrame(temp))\n",
    "\n",
    "    final = pd.concat(extracted_df, ignore_index=True)\n",
    "    final = pd.concat([final, target], axis=1)\n",
    "    return final\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHEME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-217-a7e6d1abce43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtweetType\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'source-tweet'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mjsons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetchRawText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweetType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverified\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverified\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-216-d22b68828612>\u001b[0m in \u001b[0;36mfetchRawText\u001b[0;34m(path, events, tweetType)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mjsons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mjsons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s/%s/**/%s/*.json'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtweetType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjsons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s's length is %d\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/glob2/impl.py\u001b[0m in \u001b[0;36mglob\u001b[0;34m(self, pathname, with_matches, include_hidden, recursive, norm_paths, case_sensitive, sep)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0ma\u001b[0m \u001b[0mdot\u001b[0m \u001b[0mare\u001b[0m \u001b[0malso\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \"\"\"\n\u001b[0;32m---> 60\u001b[0;31m         return list(self.iglob(pathname, with_matches, include_hidden,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                norm_paths, case_sensitive, sep))\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/glob2/impl.py\u001b[0m in \u001b[0;36m_iglob\u001b[0;34m(self, pathname, rootcall, include_hidden, norm_paths, case_sensitive, sep)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# Resolve ``basename`` expr for every directory found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_groups\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             for name, groups in self.resolve_pattern(dirname, basename,\n\u001b[1;32m    128\u001b[0m                                                      \u001b[0;32mnot\u001b[0m \u001b[0mrootcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/glob2/impl.py\u001b[0m in \u001b[0;36m_iglob\u001b[0;34m(self, pathname, rootcall, include_hidden, norm_paths, case_sensitive, sep)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# Resolve ``basename`` expr for every directory found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_groups\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             for name, groups in self.resolve_pattern(dirname, basename,\n\u001b[1;32m    128\u001b[0m                                                      \u001b[0;32mnot\u001b[0m \u001b[0mrootcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/glob2/impl.py\u001b[0m in \u001b[0;36m_iglob\u001b[0;34m(self, pathname, rootcall, include_hidden, norm_paths, case_sensitive, sep)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# Resolve ``basename`` expr for every directory found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_groups\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             for name, groups in self.resolve_pattern(dirname, basename,\n\u001b[0m\u001b[1;32m    128\u001b[0m                                                      \u001b[0;32mnot\u001b[0m \u001b[0mrootcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                                                      norm_paths, case_sensitive, sep):\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/glob2/impl.py\u001b[0m in \u001b[0;36mresolve_pattern\u001b[0;34m(self, dirname, pattern, globstar_with_root, include_hidden, norm_paths, case_sensitive, sep)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;31m# having to deal with os.path.normpath() later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mglobstar_with_root\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentries\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                     \u001b[0m_mkabs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_join_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                     \u001b[0mnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_mkabs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/glob2/impl.py\u001b[0m in \u001b[0;36mwalk\u001b[0;34m(self, top, followlinks, sep)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mnew_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_join_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfollowlinks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollowlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/glob2/impl.py\u001b[0m in \u001b[0;36mwalk\u001b[0;34m(self, top, followlinks, sep)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mnew_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_join_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfollowlinks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollowlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/glob2/impl.py\u001b[0m in \u001b[0;36mwalk\u001b[0;34m(self, top, followlinks, sep)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mnew_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_join_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfollowlinks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollowlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/glob2/impl.py\u001b[0m in \u001b[0;36mwalk\u001b[0;34m(self, top, followlinks, sep)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mnew_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_join_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mfollowlinks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollowlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/posixpath.py\u001b[0m in \u001b[0;36mislink\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;34m\"\"\"Test whether a path is a symbolic link\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path = \"../pheme-rnr-dataset\"\n",
    "events = ['charliehebdo', 'ferguson',\n",
    "          'germanwings-crash', 'ottawashooting', 'sydneysiege']\n",
    "tweetType = 'source-tweet'\n",
    "jsons = []\n",
    "final = fetchRawText(path, events, tweetType)\n",
    "target = final.target\n",
    "final.verified = final.verified.replace({True: 1, False: 0}) \n",
    "final.has_question = final.has_question.replace({True: 1, False: 0}) \n",
    "final.has_exclaim = final.has_exclaim.replace({True: 1, False: 0}) \n",
    "final.has_period = final.has_period.replace({True: 1, False: 0}) \n",
    "final = final.replace(-np.inf, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>URLcount</th>\n      <th>Noun</th>\n      <th>Verb</th>\n      <th>Adjective</th>\n      <th>Pronoun</th>\n      <th>FirstPersonPronoun</th>\n      <th>SecondPersonPronoun</th>\n      <th>ThirdPersonPronoun</th>\n      <th>Adverb</th>\n      <th>Numeral</th>\n      <th>Conjunction_inj</th>\n      <th>Particle</th>\n      <th>Determiner</th>\n      <th>Modal</th>\n      <th>Whs</th>\n      <th>HashTag</th>\n      <th>char_count</th>\n      <th>word_count</th>\n      <th>has_question</th>\n      <th>has_exclaim</th>\n      <th>has_period</th>\n      <th>capital_ratio</th>\n      <th>tweet_count</th>\n      <th>listed_count</th>\n      <th>friends_count</th>\n      <th>follow_ratio</th>\n      <th>verified</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>7</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>88</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.159091</td>\n      <td>4.803286</td>\n      <td>3.855943</td>\n      <td>2.788168</td>\n      <td>5.287349</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>53</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.037736</td>\n      <td>3.031812</td>\n      <td>2.146128</td>\n      <td>2.574031</td>\n      <td>3.672929</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>136</td>\n      <td>18</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.073529</td>\n      <td>3.856245</td>\n      <td>2.879669</td>\n      <td>2.772322</td>\n      <td>4.309651</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>4</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>138</td>\n      <td>16</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.101449</td>\n      <td>4.735814</td>\n      <td>5.009820</td>\n      <td>3.016197</td>\n      <td>7.187664</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>7</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>117</td>\n      <td>13</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.145299</td>\n      <td>5.021181</td>\n      <td>4.132996</td>\n      <td>2.662758</td>\n      <td>5.925434</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   URLcount  Noun  Verb  Adjective  Pronoun  FirstPersonPronoun  \\\n0         1     7     2          0        0                   0   \n1         0     3     2          0        0                   0   \n2         0     4     4          7        0                   0   \n3         2     4     5          1        0                   0   \n4         2     7     2          0        0                   0   \n\n   SecondPersonPronoun  ThirdPersonPronoun  Adverb  Numeral  Conjunction_inj  \\\n0                    0                   0       0        0                2   \n1                    0                   0       0        0                1   \n2                    0                   0       1        0                2   \n3                    0                   0       0        0                0   \n4                    0                   0       0        0                2   \n\n   Particle  Determiner  Modal  Whs  HashTag  char_count  word_count  \\\n0         0           0      0    0        0          88          12   \n1         0           0      0    0        1          53           6   \n2         0           0      0    0        2         136          18   \n3         0           2      0    1        1         138          16   \n4         0           0      0    0        1         117          13   \n\n   has_question  has_exclaim  has_period  capital_ratio  tweet_count  \\\n0             0            0           1       0.159091     4.803286   \n1             0            0           1       0.037736     3.031812   \n2             0            0           1       0.073529     3.856245   \n3             0            0           1       0.101449     4.735814   \n4             0            0           1       0.145299     5.021181   \n\n   listed_count  friends_count  follow_ratio  verified  \n0      3.855943       2.788168      5.287349         1  \n1      2.146128       2.574031      3.672929         0  \n2      2.879669       2.772322      4.309651         0  \n3      5.009820       3.016197      7.187664         1  \n4      4.132996       2.662758      5.925434         1  "
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.drop(['text_token','text','Event','target'], axis=1, inplace=True)\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('./data/_PHEME_sparse.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHEME (Extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ebola-essien's length is 14\n",
      "prince-toronto's length is 233\n",
      "putinmissing's length is 238\n",
      "0 ebola-essien 14 14\n",
      "1 prince-toronto 233 233\n",
      "2 putinmissing 238 238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-03eea0864855>:227: RuntimeWarning: divide by zero encountered in log10\n",
      "  output_f['listed_count'] = np.log10(obj['user']['listed_count'])\n",
      "<ipython-input-4-03eea0864855>:228: RuntimeWarning: divide by zero encountered in log10\n",
      "  output_f['friends_count'] = np.log10(obj['user']['friends_count'])\n"
     ]
    }
   ],
   "source": [
    "path = \"../PHEME/all-rnr-annotated-threads\"\n",
    "events = ['ebola-essien', 'prince-toronto', 'putinmissing']\n",
    "tweetType = 'source-tweets'\n",
    "jsons = []\n",
    "final_ext = fetchRawText(path,events,tweetType)\n",
    "ext_target = final_ext.target\n",
    "final_ext.verified = final_ext.verified.replace({True: 1, False: 0}) \n",
    "final_ext.has_question = final_ext.has_question.replace({True: 1, False: 0}) \n",
    "final_ext.has_exclaim = final_ext.has_exclaim.replace({True: 1, False: 0}) \n",
    "final_ext.has_period = final_ext.has_period.replace({True: 1, False: 0}) \n",
    "final_ext = final_ext.replace(-np.inf, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>URLcount</th>\n      <th>Noun</th>\n      <th>Verb</th>\n      <th>Adjective</th>\n      <th>Pronoun</th>\n      <th>FirstPersonPronoun</th>\n      <th>SecondPersonPronoun</th>\n      <th>ThirdPersonPronoun</th>\n      <th>Adverb</th>\n      <th>Numeral</th>\n      <th>Conjunction_inj</th>\n      <th>Particle</th>\n      <th>Determiner</th>\n      <th>Modal</th>\n      <th>Whs</th>\n      <th>HashTag</th>\n      <th>char_count</th>\n      <th>word_count</th>\n      <th>has_question</th>\n      <th>has_exclaim</th>\n      <th>has_period</th>\n      <th>capital_ratio</th>\n      <th>tweet_count</th>\n      <th>listed_count</th>\n      <th>friends_count</th>\n      <th>follow_ratio</th>\n      <th>verified</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>69</td>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.101449</td>\n      <td>4.609338</td>\n      <td>2.170262</td>\n      <td>2.814248</td>\n      <td>4.339113</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>9</td>\n      <td>6</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>148</td>\n      <td>25</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.027027</td>\n      <td>2.706718</td>\n      <td>3.210319</td>\n      <td>2.245513</td>\n      <td>5.688861</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>7</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>119</td>\n      <td>20</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.025210</td>\n      <td>4.920290</td>\n      <td>3.335458</td>\n      <td>2.158362</td>\n      <td>5.366137</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>5</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>130</td>\n      <td>16</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.107692</td>\n      <td>4.188872</td>\n      <td>2.783904</td>\n      <td>2.854913</td>\n      <td>4.866571</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>4</td>\n      <td>4</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>120</td>\n      <td>15</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.066667</td>\n      <td>4.920290</td>\n      <td>3.335458</td>\n      <td>2.158362</td>\n      <td>5.366137</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   URLcount  Noun  Verb  Adjective  Pronoun  FirstPersonPronoun  \\\n0         1     2     2          1        0                   0   \n1         0     9     6          3        0                   2   \n2         0     7     4          1        1                   0   \n3         2     5     3          2        0                   0   \n4         1     4     4          2        0                   0   \n\n   SecondPersonPronoun  ThirdPersonPronoun  Adverb  Numeral  Conjunction_inj  \\\n0                    0                   0       0        0                1   \n1                    0                   0       3        0                4   \n2                    0                   1       0        0                2   \n3                    0                   0       0        0                1   \n4                    0                   0       1        0                2   \n\n   Particle  Determiner  Modal  Whs  HashTag  char_count  word_count  \\\n0         0           1      0    0        0          69           8   \n1         0           1      1    0        0         148          25   \n2         0           3      0    1        0         119          20   \n3         0           2      0    0        0         130          16   \n4         0           1      0    0        0         120          15   \n\n   has_question  has_exclaim  has_period  capital_ratio  tweet_count  \\\n0             0            0           1       0.101449     4.609338   \n1             0            0           1       0.027027     2.706718   \n2             0            0           1       0.025210     4.920290   \n3             0            0           1       0.107692     4.188872   \n4             0            0           1       0.066667     4.920290   \n\n   listed_count  friends_count  follow_ratio  verified  \n0      2.170262       2.814248      4.339113         0  \n1      3.210319       2.245513      5.688861         1  \n2      3.335458       2.158362      5.366137         0  \n3      2.783904       2.854913      4.866571         1  \n4      3.335458       2.158362      5.366137         0  "
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ext.drop(['text_token','text','Event','target'], axis=1, inplace=True)\n",
    "final_ext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ext.to_csv('./data/_PHEMEext_sparse.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHEME ALL (Reactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from glob2 import glob\n",
    "import json\n",
    "\n",
    "pd.options.display.float_format = '{:.5f}'.format\n",
    "\n",
    "def fetchRawText_all(path, events, tweetType):\n",
    "    jsons = []\n",
    "    for i, event in enumerate(events):\n",
    "        jsons.append(glob('%s/%s/**/%s/[0-9]*.json' % (path, event, tweetType)))\n",
    "    for i,d in enumerate(jsons): print(\"%s's length is %d\" %(events[i], len(d)))\n",
    "\n",
    "    targets = []\n",
    "    features = []\n",
    "    isSrcTweet = []\n",
    "    for index, dataset in enumerate(jsons):\n",
    "        targetEvent = []\n",
    "        dataEvent = []\n",
    "        count = 0  # help var\n",
    "        for jsonFile in dataset:\n",
    "            count += 1\n",
    "            if jsonFile.find(\"non-rumours\") == -1:\n",
    "                targetEvent.append(1)\n",
    "            else:\n",
    "                targetEvent.append(0)\n",
    "            if jsonFile.find(\"source-tweet\") == -1:\n",
    "                isSrcTweet.append(0)\n",
    "            else: #if jsonFile.find(\"reactions\") == 1:\n",
    "                isSrcTweet.append(1)\n",
    "                \n",
    "\n",
    "            with open(jsonFile, 'r') as f:\n",
    "                for l in f.readlines():\n",
    "                    if not l.strip():  # skip empty lines\n",
    "                        continue\n",
    "                    json_data = json.loads(l)\n",
    "                    dataEvent.append(json_data)\n",
    "        targets.append(targetEvent)\n",
    "        features.append(dataEvent)\n",
    "        # isSrcTweet.append(isSrcTweet)\n",
    "\n",
    "    # print(\"\\nNumber of Events:\", len(targets))\n",
    "    # print(\"Number of tweets in the first event:\", len(targets[0]))\n",
    "\n",
    "    # targets은 targetEvent들을 리스트에 담은 것\n",
    "    target_list = []\n",
    "    for event in targets:\n",
    "        for elem in event:\n",
    "            target_list.append(elem)\n",
    "    target = pd.DataFrame(target_list, columns=[\"target\"])\n",
    "    isSrcTweet = pd.DataFrame(isSrcTweet, columns=[\"isSrcTweet\"])\n",
    "\n",
    "    extracted_features = []\n",
    "\n",
    "    extracted = []\n",
    "\n",
    "    for obj_list in features:\n",
    "        extracted_event = []\n",
    "        for obj in obj_list:\n",
    "            output_f = dict()\n",
    "\n",
    "            if ('text' in obj):\n",
    "                output_f['text'] = obj['text']\n",
    "            else:\n",
    "                output_f['text'] = None\n",
    "            if ('id' in obj):\n",
    "                output_f['id'] = obj['id']\n",
    "            else:\n",
    "                output_f['id'] = None\n",
    "            if ('in_reply_to_status_id' in obj):\n",
    "                output_f['pid'] = obj['in_reply_to_status_id']\n",
    "            else:\n",
    "                output_f['pid'] = None\n",
    "            output_f['emoji_count'] = emoji.emoji_count(obj['text'])\n",
    "            urls_dicts = obj['entities']['urls']\n",
    "            if \"media\" in obj['entities']:\n",
    "                output_f['has_media'] = len(obj['entities']['media'])\n",
    "                # output_f['media_type'] = obj['entities']['media'][0]['type']\n",
    "            else:\n",
    "                output_f['has_media'] = 0\n",
    "                # output_f['media_type'] = 0\n",
    "            output_f['URLcount'] = len(urls_dicts)\n",
    "            # output_f['URLcount'] = extract_urls(urls_dicts)\n",
    "            # temp = obj['text'].lower()\n",
    "            temp = re.sub(r\"http\\S+\", \"HTTPURL\", obj['text'])\n",
    "            verification = 0\n",
    "            verification += len(re.findall(r'is(that|this|it) true', obj['text']))\n",
    "            verification += len(re.findall(r'wh[a]*t[?!|!?][?!|!?]*', obj['text']))\n",
    "            verification += len(re.findall(r'(rumour|rumor|debunk)', obj['text']))\n",
    "            verification += len(re.findall(r'(real?|really?|uncomfirmed)', obj['text']))\n",
    "            verification += len(re.findall(r'(that|this|it) is not true', obj['text']))\n",
    "            verification += len(re.findall(r'(that|this|it) is false', obj['text']))\n",
    "            verification += len(re.findall(r'(h[m]*)', obj['text']))\n",
    "            output_f['Skepticism'] = verification            \n",
    "            url, mention = 0, 0\n",
    "            for token in temp:\n",
    "                if token.startswith('HTTPURL'):\n",
    "                # if token.startswith (r\"http\\S+\"):\n",
    "                    url+=1\n",
    "                if token.startswith('@'):\n",
    "                    mention+=1 \n",
    "            # output_f['URLcount'] = url\n",
    "            output_f['MentionCount'] = mention\n",
    "\n",
    "            '''POS Tagging'''\n",
    "            temp = output_f['text']\n",
    "            temp = replaceContraction(temp.lower())\n",
    "            temp = re.sub(r\"(#)(\\S+)\", '', temp)\n",
    "            temp = re.sub(r\"(@)(\\S+)\", '', temp)\n",
    "            temp = re.sub(r\"http\\S+\", \"\", temp)\n",
    "            temp = re.sub(r'([^\\s\\w#\\*]|_)+', '', temp) # Erasing Special Characters\n",
    "\n",
    "            temp = temp.split()\n",
    "            pos_dict=getposcount(temp)\n",
    "            output_f['token_for_POS'] = temp\n",
    "            output_f.update(pos_dict)\n",
    "\n",
    "            output_f['char_count'] = len(output_f['text'])\n",
    "            output_f['word_count'] = len(output_f['text'].split())\n",
    "\n",
    "            # output_f['HashTag'] = len(obj['entities'][0]['hashtags'])\n",
    "            output_f['HashTag'] = len(obj['entities']['hashtags'])\n",
    "            \n",
    "            output_f['has_question'] = \"?\" in output_f[\"text\"]\n",
    "            output_f['has_exclaim'] = \"!\" in output_f[\"text\"]\n",
    "            output_f['has_period'] = \".\" in output_f[\"text\"]\n",
    "\n",
    "            output_f['retweet_count'] = obj['retweet_count']\n",
    "            output_f['isRT'] = obj['retweeted']\n",
    "\n",
    "            output_f['tweet_count'] = np.log10(obj['user']['statuses_count'])\n",
    "            output_f['listed_count'] = np.log10(obj['user']['listed_count'])\n",
    "            output_f['friends_count'] = np.log10(obj['user']['friends_count'])\n",
    "            output_f['follow_ratio'] = np.log10(obj['user']['followers_count'])\n",
    "            \n",
    "            acc_created = datetime.strptime(obj['user']['created_at'], '%a %b %d %H:%M:%S %z %Y')\n",
    "            tweet_created = datetime.strptime(obj['created_at'], '%a %b %d %H:%M:%S %z %Y')\n",
    "            age = (tweet_created - acc_created)\n",
    "            output_f['account_age_days'] = age.days\n",
    "            output_f['tweet_created'] = datetime.timestamp(tweet_created)\n",
    "            # output_f['tweet_created2'] = tweet_created\n",
    "\n",
    "\n",
    "\n",
    "            output_f['capital_ratio']=(capitalratio(obj['text']))\n",
    "            output_f['verified'] = obj['user']['verified']\n",
    "\n",
    "            extracted_event.append(output_f)\n",
    "        extracted_features.append(extracted_event)\n",
    "\n",
    "    extracted_df = []\n",
    "    # print(events)\n",
    "    # print(len(extracted_features))\n",
    "    for i, data in enumerate(extracted_features):\n",
    "        temp = pd.DataFrame(data)\n",
    "        temp[\"Event\"] = events[i]\n",
    "        extracted_df.append(pd.DataFrame(temp))\n",
    "\n",
    "    final = pd.concat(extracted_df, ignore_index=True)\n",
    "    final = pd.concat([final, isSrcTweet ,target], axis=1)\n",
    "    final.pid = final.pid\n",
    "    return final\n",
    "\n",
    "def depth(x):\n",
    "    if type(x) is dict and x:\n",
    "        return 1 + max(depth(x[a]) for a in x)\n",
    "    if type(x) is list and x:\n",
    "        return 1 + max(depth(a) for a in x)\n",
    "    return 0\n",
    "\n",
    "def getThreadData(path, events):\n",
    "    import re\n",
    "\n",
    "    sources = []\n",
    "    for i, event in enumerate(events):\n",
    "        sources.append(glob('%s/%s/*/*' % (path, event)))\n",
    "    roots = []\n",
    "    children = []\n",
    "    features = []\n",
    "    isSrcTweet = []\n",
    "    for num, event in enumerate(sources):\n",
    "        for index, dataset in enumerate(event):\n",
    "            # print(dataset)\n",
    "            # children.append(glob('%s/reactions/*/*' % (dataset)))\n",
    "            childs = [os.path.basename(x) for x in glob('%s/reactions/*.json' % (dataset))]\n",
    "            reext = re.compile(r'(.*?)\\.json')\n",
    "            childs = (reext.match(child) for child in childs)\n",
    "            children.append([match.group(1) for match in childs if match])\n",
    "            # print(dataset)\n",
    "            roots.append(os.path.basename(dataset))\n",
    "\n",
    "    df = pd.DataFrame(roots, columns=['Root'])\n",
    "    df = pd.concat([df,pd.DataFrame(children)],axis=1)\n",
    "    \n",
    "    structfile = []\n",
    "    for i, event in enumerate(events):\n",
    "        structfile.append(glob('%s/%s/**/[0-9]*/structure.json' % (path, event)))\n",
    "\n",
    "    for i,d in enumerate(structfile): print(\"%s's structure.json number is %d\" %(events[i], len(d)))\n",
    "    # print(structfile)\n",
    "\n",
    "    thread_depths = []\n",
    "    thread_roots = []\n",
    "    for index, dataset in enumerate(structfile):\n",
    "        targetEvent = []\n",
    "        dataEvent = []\n",
    "        count = 0  # help var\n",
    "        for jsonFile in dataset:\n",
    "            # print(jsonFile)\n",
    "            match = re.search(\"/([0-9]*)/\", jsonFile)\n",
    "            # p.match(\"lalalaI want this partlalala\").group(1)\n",
    "            rootname = match.group(1) if match else None\n",
    "            # print(rootname)\n",
    "            with open(jsonFile, 'r') as f:\n",
    "                for l in f.readlines():\n",
    "                    if not l.strip():  # skip empty lines\n",
    "                        continue\n",
    "                json_data = json.loads(l)\n",
    "                # print(json_data)\n",
    "                thread_depth = depth(json_data)\n",
    "                thread_depths.append([rootname,thread_depth])\n",
    "                # thread_roots.append(rootname)\n",
    "    df_depth = pd.DataFrame(thread_depths, columns=['Root', 'depth'])\n",
    "    df = pd.merge(df, df_depth, on=\"Root\")\n",
    "                \n",
    "    \n",
    "    # return pd.DataFrame(thread_depths)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def getThreadInfo(structure, df):\n",
    "    threadInfo = []\n",
    "    thread_depth = structure[['Root', 'depth']]\n",
    "    structure = structure.drop('depth', axis=1)\n",
    "    for index, data in enumerate(structure.Root):\n",
    "        tweetInfo = []\n",
    "        # print(\"data: %s\\n\" %(data))\n",
    "        # print(\"data: %s\\n%s\\n\" %(data, structure.loc[index,0:].values))\n",
    "        # print(\"root: %s\\tFirst reaction: %s\\n\" %(data, structure.loc[index,0]))\n",
    "\n",
    "        \n",
    "        pid = int(data)\n",
    "        thread = structure.loc[structure['Root']==pid].dropna(axis=1)\n",
    "        # threadRange = structure.loc[structure['Root']==data].any().sum()-1\n",
    "        # print(structure)\n",
    "        threadRange = len(structure.iloc[index,:].dropna())\n",
    "\n",
    "        # 아래로는 성공적인 Features\n",
    "        friends_count = np.sum([np.sum(df.loc[(df['id'] == int(childid))]['friends_count'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        friends_countavg = np.mean([np.sum(df.loc[(df['id'] == int(childid))]['friends_count'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        words_count = np.mean([np.sum(df.loc[(df['id'] == int(childid))]['word_count'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        char_count = np.mean([np.sum(df.loc[(df['id'] == int(childid))]['char_count'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        hashtagavg = np.mean([np.sum(df.loc[(df['id'] == int(childid))]['HashTag'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        hashtagsum = np.sum([np.sum(df.loc[(df['id'] == int(childid))]['HashTag'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        hashtagpercentage = np.sum([np.any(df.loc[(df['id'] == int(childid))]['HashTag']) for childid in structure.loc[index,:].dropna()])/threadRange\n",
    "        urlavg = np.mean([np.sum(df.loc[(df['id'] == int(childid))]['URLcount'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        urlstd = np.std([np.sum(df.loc[(df['id'] == int(childid))]['URLcount'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        urlratio = np.mean([np.any(df.loc[(df['id'] == int(childid))]['URLcount'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        mentionsum = np.sum([np.sum(df.loc[(df['id'] == int(childid))]['MentionCount'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        mentionavg = np.mean([np.sum(df.loc[(df['id'] == int(childid))]['MentionCount'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        mentionpercentage = np.sum([np.any(df.loc[(df['id'] == int(childid))]['MentionCount'].values) for childid in structure.loc[index,:].dropna()])/threadRange\n",
    "        verifiedratio = np.mean([np.any(df.loc[(df['id'] == int(childid))]['verified'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        verifiedsum = np.sum([np.sum(df.loc[(df['id'] == int(childid))]['verified'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        retweetsum = np.sum([np.sum(df.loc[(df['id'] == int(childid))]['retweet_count'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        retweetavg = np.mean([np.sum(df.loc[(df['id'] == int(childid))]['retweet_count'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        retweetstd = np.std([np.sum(df.loc[(df['id'] == int(childid))]['retweet_count'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        accageavg = np.mean([np.sum(df.loc[(df['id'] == int(childid))]['account_age_days'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        accagestd = np.std([np.sum(df.loc[(df['id'] == int(childid))]['account_age_days'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        emojistd = np.std([np.sum(df.loc[(df['id'] == int(childid))]['emoji_count'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        emojimean = np.mean([np.sum(df.loc[(df['id'] == int(childid))]['emoji_count'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        mediaratio = np.mean([np.sum(df.loc[(df['id'] == int(childid))]['has_media'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        questionratio = np.mean([np.sum(df.loc[(df['id'] == int(childid))]['has_question'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        exclamationratio = np.mean([np.sum(df.loc[(df['id'] == int(childid))]['has_exclaim'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        periodratio = np.mean([np.sum(df.loc[(df['id'] == int(childid))]['has_period'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        FPPmean = np.mean([np.sum(df.loc[(df['id'] == int(childid))]['FirstPersonPronoun'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        FPPstd = np.std([np.sum(df.loc[(df['id'] == int(childid))]['FirstPersonPronoun'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        SPPmean = np.mean([np.sum(df.loc[(df['id'] == int(childid))]['SecondPersonPronoun'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        SPPstd = np.std([np.sum(df.loc[(df['id'] == int(childid))]['SecondPersonPronoun'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        TPPmean = np.mean([np.sum(df.loc[(df['id'] == int(childid))]['ThirdPersonPronoun'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        TPPstd = np.std([np.sum(df.loc[(df['id'] == int(childid))]['ThirdPersonPronoun'].values) for childid in structure.loc[index,:].dropna()])\n",
    "        Skepticismmean = np.mean([np.sum(df.loc[(df['id'] == int(childid))]['Skepticism'].values) for childid in structure.loc[index,:].dropna()])\n",
    "\n",
    "        # Get the lifetime of thread\n",
    "        # root_created = df.loc[(df['id'] == int(pid))].tweet_created.sum()\n",
    "        # try:\n",
    "        #     thread_latest = np.max([np.sum(df.loc[(df['id'] == childid)]['tweet_created'].values) for childid in structure.loc[index,'0':].dropna()])\n",
    "        # except:\n",
    "        # #     print([df.loc[(df['id'] == int(childid))]['tweet_created'].values for childid in structure.loc[index,'0':].dropna()])\n",
    "        #     print(\"error\")\n",
    "\n",
    "        # thread_life = thread_latest - root_created\n",
    "        try:\n",
    "            thread_life = np.max([np.sum(df.loc[(df['id'] == childid)]['tweet_created']) for childid in structure.loc[index,:].dropna()] - df.loc[(df['id'] == pid)].tweet_created.sum())\n",
    "        except:\n",
    "            print(\"index:\", index)\n",
    "\n",
    "        # 해당 스레드의 트윗 개수\n",
    "        thread_node_count = len([childid for childid in structure.loc[index,:].dropna()]) \n",
    "        # print(\"thread_node_count:\",thread_node_count,\", threadRange:\",threadRange, \"lastest Thread:\", thread_latest )\n",
    "        # print(structure.loc[structure.Root == data])\n",
    "\n",
    "        tweetInfo.append(data)\n",
    "        tweetInfo.append(friends_count)\n",
    "        tweetInfo.append(friends_countavg)\n",
    "        tweetInfo.append(words_count)\n",
    "        tweetInfo.append(char_count)\n",
    "        tweetInfo.append(hashtagavg)\n",
    "        tweetInfo.append(hashtagsum)\n",
    "        tweetInfo.append(hashtagpercentage)\n",
    "        tweetInfo.append(urlavg)\n",
    "        tweetInfo.append(urlstd)\n",
    "        tweetInfo.append(urlratio)\n",
    "        tweetInfo.append(mentionsum)\n",
    "        tweetInfo.append(mentionavg)\n",
    "        tweetInfo.append(mentionpercentage)\n",
    "        tweetInfo.append(thread_node_count)\n",
    "        tweetInfo.append(verifiedratio)\n",
    "        tweetInfo.append(verifiedsum)\n",
    "        tweetInfo.append(retweetsum)\n",
    "        tweetInfo.append(retweetavg)\n",
    "        tweetInfo.append(retweetstd)\n",
    "        tweetInfo.append(accageavg)\n",
    "        tweetInfo.append(accagestd)\n",
    "        tweetInfo.append(thread_life)\n",
    "        tweetInfo.append(emojistd)\n",
    "        tweetInfo.append(emojimean)\n",
    "        tweetInfo.append(mediaratio)\n",
    "        tweetInfo.append(questionratio)\n",
    "        tweetInfo.append(exclamationratio)\n",
    "        tweetInfo.append(periodratio)\n",
    "        tweetInfo.append(FPPmean)\n",
    "        tweetInfo.append(FPPstd)\n",
    "        tweetInfo.append(SPPmean)\n",
    "        tweetInfo.append(SPPstd)\n",
    "        tweetInfo.append(TPPmean)\n",
    "        tweetInfo.append(TPPstd)\n",
    "        tweetInfo.append(Skepticismmean)\n",
    "\n",
    "        threadInfo.append(tweetInfo)\n",
    "\n",
    "        result = pd.DataFrame(threadInfo, columns=['Root', 'SUM FriendsCount','AVG FriendsCount', 'AVG WordCount', 'AVG CharCount', 'AVG HashTag', 'SUM HashTag', 'Ratio HashTag', 'AVG Url','STD Url','RATIO Url','SUM Mention', 'AVG Mention', 'Ratio Mention', 'Tweets Count', 'Ratio Verified','SUM Verified','SUM RT', 'AVG RT','STD RT', 'AVG AccAge', 'STD AccAge', 'thread_time', \"STD Emoji\",\"AVG Emoji\",\"Ratio Media\",'RATIO Question', 'RATIO Exclaim','RATIO Period', 'AVG FPP','STD FPP','AVG SPP','STD SPP','AVG TPP','STD TPP','AVG Skepticism'])\n",
    "        result = pd.merge(thread_depth, result, on=\"Root\").drop(['Root'], axis=1)\n",
    "    # print(threadInfo)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'loc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-220-adb077c51d3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'loc'"
     ]
    }
   ],
   "source": [
    "len(structure_ext.loc[0,'0':].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threadRange = len(structure_ext.loc[9,'0':].dropna())\n",
    "print(threadRange)\n",
    "print(np.sum([np.sum(all_ext.loc[(all_ext['id'] == int(childid))]['HashTag'].values) for childid in structure_ext.loc[9,'0':].dropna()]))\n",
    "np.sum([np.any(all_ext.loc[(all_ext['id'] == int(childid))]['HashTag'].values) for childid in structure_ext.loc[9,'0':].dropna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHEME ALL Create\n",
    "\n",
    "420120 Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pheme_all.shape)\n",
    "# pheme_all.loc[pheme_all['Event']=='charliehebdo'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ottawashooting's length is 12284\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-243-f7cb1524227f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtweetType\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetchRawText_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweetType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverified\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverified\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_question\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-218-af7a53243159>\u001b[0m in \u001b[0;36mfetchRawText_all\u001b[0;34m(path, events, tweetType)\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# skip empty lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                     \u001b[0mjson_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                     \u001b[0mdataEvent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetEvent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path = \"../pheme-rnr-dataset\"\n",
    "# events = ['charliehebdo', 'ferguson',\n",
    "#           'germanwings-crash', 'ottawashooting', 'sydneysiege']\n",
    "events = ['ottawashooting']\n",
    "# events = [ 'sydneysiege']\n",
    "\n",
    "tweetType = '*'\n",
    "final = fetchRawText_all(path, events, tweetType)\n",
    "final.verified = final.verified.replace({True: 1, False: 0}) \n",
    "final.has_question = final.has_question.replace({True: 1, False: 0}) \n",
    "final.has_exclaim = final.has_exclaim.replace({True: 1, False: 0}) \n",
    "final.has_period = final.has_period.replace({True: 1, False: 0}) \n",
    "final = final.replace(-np.inf, 0)\n",
    "\n",
    "pheme_all = final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheme_all.to_csv('./data/all/_PHEMEall.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheme_structure = getThreadData(path, events)\n",
    "pheme_structure.to_csv('./data/all/_PHEME_structure.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHEME ALL Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125208, 34)\n",
      "(5802, 346)\n"
     ]
    }
   ],
   "source": [
    "all_pheme = pd.read_csv(\"./data/all/_PHEMEall.csv\")\n",
    "structure_pheme = pd.read_csv(\"./data/all/_PHEME_structure.csv\")\n",
    "print(all_pheme.shape)\n",
    "print(structure_pheme.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SUM FriendsCount</th>\n      <th>AVG FriendsCount</th>\n      <th>AVG WordCount</th>\n      <th>AVG HashTag</th>\n      <th>SUM HashTag</th>\n      <th>Percent HashTag</th>\n      <th>AVG Url</th>\n      <th>AVG Mention</th>\n      <th>Percent Mention</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>54.85092</td>\n      <td>3.04727</td>\n      <td>19.05556</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.22222</td>\n      <td>1.77778</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>21.91754</td>\n      <td>3.13108</td>\n      <td>13.57143</td>\n      <td>0.14286</td>\n      <td>1.00000</td>\n      <td>0.14286</td>\n      <td>0.14286</td>\n      <td>2.00000</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>16.88907</td>\n      <td>2.81484</td>\n      <td>14.50000</td>\n      <td>0.33333</td>\n      <td>2.00000</td>\n      <td>0.16667</td>\n      <td>0.16667</td>\n      <td>1.66667</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>24.22340</td>\n      <td>2.69149</td>\n      <td>15.88889</td>\n      <td>0.55556</td>\n      <td>5.00000</td>\n      <td>0.44444</td>\n      <td>0.33333</td>\n      <td>1.33333</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10.72064</td>\n      <td>2.68016</td>\n      <td>15.50000</td>\n      <td>0.50000</td>\n      <td>2.00000</td>\n      <td>0.25000</td>\n      <td>0.50000</td>\n      <td>1.25000</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5797</th>\n      <td>220.85182</td>\n      <td>2.56804</td>\n      <td>14.61628</td>\n      <td>0.09302</td>\n      <td>8.00000</td>\n      <td>0.09302</td>\n      <td>0.26744</td>\n      <td>2.58140</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>5798</th>\n      <td>232.80737</td>\n      <td>2.19630</td>\n      <td>14.38679</td>\n      <td>0.24528</td>\n      <td>26.00000</td>\n      <td>0.11321</td>\n      <td>0.01887</td>\n      <td>1.34906</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>5799</th>\n      <td>17.67264</td>\n      <td>2.52466</td>\n      <td>17.57143</td>\n      <td>1.57143</td>\n      <td>11.00000</td>\n      <td>0.57143</td>\n      <td>0.28571</td>\n      <td>1.00000</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>5800</th>\n      <td>113.39473</td>\n      <td>2.41265</td>\n      <td>14.53191</td>\n      <td>0.08511</td>\n      <td>4.00000</td>\n      <td>0.08511</td>\n      <td>0.08511</td>\n      <td>1.76596</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>5801</th>\n      <td>27.45784</td>\n      <td>2.74578</td>\n      <td>14.40000</td>\n      <td>0.90000</td>\n      <td>9.00000</td>\n      <td>0.20000</td>\n      <td>0.00000</td>\n      <td>1.60000</td>\n      <td>1.00000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5802 rows × 9 columns</p>\n</div>",
      "text/plain": "      SUM FriendsCount  AVG FriendsCount  AVG WordCount  AVG HashTag  \\\n0             54.85092           3.04727       19.05556      0.00000   \n1             21.91754           3.13108       13.57143      0.14286   \n2             16.88907           2.81484       14.50000      0.33333   \n3             24.22340           2.69149       15.88889      0.55556   \n4             10.72064           2.68016       15.50000      0.50000   \n...                ...               ...            ...          ...   \n5797         220.85182           2.56804       14.61628      0.09302   \n5798         232.80737           2.19630       14.38679      0.24528   \n5799          17.67264           2.52466       17.57143      1.57143   \n5800         113.39473           2.41265       14.53191      0.08511   \n5801          27.45784           2.74578       14.40000      0.90000   \n\n      SUM HashTag  Percent HashTag  AVG Url  AVG Mention  Percent Mention  \n0         0.00000          0.00000  0.22222      1.77778          1.00000  \n1         1.00000          0.14286  0.14286      2.00000          1.00000  \n2         2.00000          0.16667  0.16667      1.66667          1.00000  \n3         5.00000          0.44444  0.33333      1.33333          1.00000  \n4         2.00000          0.25000  0.50000      1.25000          1.00000  \n...           ...              ...      ...          ...              ...  \n5797      8.00000          0.09302  0.26744      2.58140          1.00000  \n5798     26.00000          0.11321  0.01887      1.34906          1.00000  \n5799     11.00000          0.57143  0.28571      1.00000          1.00000  \n5800      4.00000          0.08511  0.08511      1.76596          1.00000  \n5801      9.00000          0.20000  0.00000      1.60000          1.00000  \n\n[5802 rows x 9 columns]"
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pheme_thread = getThreadInfo(structure_pheme, all_pheme)\n",
    "pheme_thread = pheme_thread.fillna(0)\n",
    "pheme_thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheme_thread.to_csv('./data/all/_PHEME_thread.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHEMEext Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ebola-essien's length is 226\n",
      "prince-toronto's length is 902\n",
      "putinmissing's length is 835\n",
      "(1963, 42)\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>id</th>\n      <th>pid</th>\n      <th>emoji_count</th>\n      <th>has_media</th>\n      <th>URLcount</th>\n      <th>Skepticism</th>\n      <th>MentionCount</th>\n      <th>token_for_POS</th>\n      <th>Noun</th>\n      <th>Verb</th>\n      <th>Adjective</th>\n      <th>Pronoun</th>\n      <th>FirstPersonPronoun</th>\n      <th>SecondPersonPronoun</th>\n      <th>ThirdPersonPronoun</th>\n      <th>Adverb</th>\n      <th>Numeral</th>\n      <th>Conjunction_inj</th>\n      <th>Particle</th>\n      <th>Determiner</th>\n      <th>Modal</th>\n      <th>Whs</th>\n      <th>char_count</th>\n      <th>word_count</th>\n      <th>HashTag</th>\n      <th>has_question</th>\n      <th>has_exclaim</th>\n      <th>has_period</th>\n      <th>retweet_count</th>\n      <th>isRT</th>\n      <th>tweet_count</th>\n      <th>listed_count</th>\n      <th>friends_count</th>\n      <th>follow_ratio</th>\n      <th>account_age_days</th>\n      <th>tweet_created</th>\n      <th>capital_ratio</th>\n      <th>verified</th>\n      <th>Event</th>\n      <th>isSrcTweet</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@Mourinholic 😕😕 http://t.co/sFoV1v8uDo</td>\n      <td>521410632953131008</td>\n      <td>521369179392581632.00000</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>[]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>38</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4.89226</td>\n      <td>1.20412</td>\n      <td>3.13799</td>\n      <td>3.56062</td>\n      <td>1569</td>\n      <td>1413148956.00000</td>\n      <td>0.10526</td>\n      <td>0</td>\n      <td>ebola-essien</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>“@Mourinholic: Micheal Essien denying the Ebola rumours like https://t.co/8Yo8iLgISS”</td>\n      <td>521373142347153409</td>\n      <td>521369179392581632.00000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1</td>\n      <td>[micheal, essien, denying, the, ebola, rumours, like]</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>85</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3.58827</td>\n      <td>0.90309</td>\n      <td>3.10072</td>\n      <td>3.10653</td>\n      <td>242</td>\n      <td>1413140018.00000</td>\n      <td>0.10588</td>\n      <td>0</td>\n      <td>ebola-essien</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@Mourinholic Hmmm.</td>\n      <td>521369380249432064</td>\n      <td>521369179392581632.00000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>[hmmm]</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>18</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4.52289</td>\n      <td>1.17609</td>\n      <td>1.89209</td>\n      <td>3.14426</td>\n      <td>653</td>\n      <td>1413139121.00000</td>\n      <td>0.11111</td>\n      <td>0</td>\n      <td>ebola-essien</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@Mourinholic Even though it was against us, it was a bloody amazing goal.</td>\n      <td>521370496928337920</td>\n      <td>521369179392581632.00000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>[even, though, it, was, against, us, it, was, a, bloody, amazing, goal]</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>73</td>\n      <td>13</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2.96755</td>\n      <td>0.00000</td>\n      <td>2.35025</td>\n      <td>1.75587</td>\n      <td>1762</td>\n      <td>1413139387.00000</td>\n      <td>0.02740</td>\n      <td>0</td>\n      <td>ebola-essien</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@CdtChoco1er thanks bro.</td>\n      <td>521370224256614400</td>\n      <td>521370061550809088.00000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>[thanks, bro]</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>24</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4.62381</td>\n      <td>2.20140</td>\n      <td>2.82607</td>\n      <td>4.35601</td>\n      <td>1570</td>\n      <td>1413139322.00000</td>\n      <td>0.08333</td>\n      <td>0</td>\n      <td>ebola-essien</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                                                                    text  \\\n0                                                 @Mourinholic 😕😕 http://t.co/sFoV1v8uDo   \n1  “@Mourinholic: Micheal Essien denying the Ebola rumours like https://t.co/8Yo8iLgISS”   \n2                                                                     @Mourinholic Hmmm.   \n3              @Mourinholic Even though it was against us, it was a bloody amazing goal.   \n4                                                               @CdtChoco1er thanks bro.   \n\n                   id                      pid  emoji_count  has_media  \\\n0  521410632953131008 521369179392581632.00000            2          1   \n1  521373142347153409 521369179392581632.00000            0          0   \n2  521369380249432064 521369179392581632.00000            0          0   \n3  521370496928337920 521369179392581632.00000            0          0   \n4  521370224256614400 521370061550809088.00000            0          0   \n\n   URLcount  Skepticism  MentionCount  \\\n0         0           2             1   \n1         1           5             1   \n2         0           1             1   \n3         0           3             1   \n4         0           2             1   \n\n                                                             token_for_POS  \\\n0                                                                       []   \n1                    [micheal, essien, denying, the, ebola, rumours, like]   \n2                                                                   [hmmm]   \n3  [even, though, it, was, against, us, it, was, a, bloody, amazing, goal]   \n4                                                            [thanks, bro]   \n\n   Noun  Verb  Adjective  Pronoun  FirstPersonPronoun  SecondPersonPronoun  \\\n0     0     0          0        0                   0                    0   \n1     2     2          1        0                   0                    0   \n2     1     0          0        0                   0                    0   \n3     1     2          2        3                   1                    0   \n4     1     1          0        0                   0                    0   \n\n   ThirdPersonPronoun  Adverb  Numeral  Conjunction_inj  Particle  Determiner  \\\n0                   0       0        0                0         0           0   \n1                   0       0        0                1         0           1   \n2                   0       0        0                0         0           0   \n3                   2       1        0                2         0           1   \n4                   0       0        0                0         0           0   \n\n   Modal  Whs  char_count  word_count  HashTag  has_question  has_exclaim  \\\n0      0    0          38           3        0             0            0   \n1      0    0          85           9        0             0            0   \n2      0    0          18           2        0             0            0   \n3      0    0          73          13        0             0            0   \n4      0    0          24           3        0             0            0   \n\n   has_period  retweet_count isRT  tweet_count  listed_count  friends_count  \\\n0           1              0    0      4.89226       1.20412        3.13799   \n1           1              0    0      3.58827       0.90309        3.10072   \n2           1              0    0      4.52289       1.17609        1.89209   \n3           1              0    0      2.96755       0.00000        2.35025   \n4           1              0    0      4.62381       2.20140        2.82607   \n\n   follow_ratio  account_age_days    tweet_created  capital_ratio  verified  \\\n0       3.56062              1569 1413148956.00000        0.10526         0   \n1       3.10653               242 1413140018.00000        0.10588         0   \n2       3.14426               653 1413139121.00000        0.11111         0   \n3       1.75587              1762 1413139387.00000        0.02740         0   \n4       4.35601              1570 1413139322.00000        0.08333         0   \n\n          Event  isSrcTweet  target  \n0  ebola-essien           0       1  \n1  ebola-essien           0       1  \n2  ebola-essien           0       1  \n3  ebola-essien           0       1  \n4  ebola-essien           0       1  "
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../PHEME/all-rnr-annotated-threads\"\n",
    "events = ['ebola-essien', 'prince-toronto', 'putinmissing']\n",
    "# events = ['ebola-essien']\n",
    "tweetType = '*'\n",
    "\n",
    "all_ext = fetchRawText_all(path, events, tweetType)\n",
    "all_ext.isRT = all_ext.isRT.replace({True: 1, False: 0}) \n",
    "all_ext.verified = all_ext.verified.replace({True: 1, False: 0}) \n",
    "all_ext.has_question = all_ext.has_question.replace({True: 1, False: 0}) \n",
    "all_ext.has_exclaim = all_ext.has_exclaim.replace({True: 1, False: 0}) \n",
    "all_ext.has_period = all_ext.has_period.replace({True: 1, False: 0}) \n",
    "all_ext = all_ext.replace(-np.inf, 0)\n",
    "\n",
    "print(all_ext.shape)\n",
    "all_ext.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ext.to_csv('./data/all/_PHEMEextall.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ebola-essien's structure.json number is 14\n",
      "prince-toronto's structure.json number is 233\n",
      "putinmissing's structure.json number is 238\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# ext_structure = getThreadData(path, events)\n",
    "structure_ext = getThreadData(path, events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_ext.to_csv('./data/all/_PHEMEext_structure.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHEME EXT PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1963, 38)\n",
      "(485, 28)\n"
     ]
    }
   ],
   "source": [
    "all_ext = pd.read_csv(\"./data/all/_PHEMEextall.csv\")\n",
    "structure_ext = pd.read_csv(\"./data/all/_PHEMEext_structure.csv\")\n",
    "ext_y = pd.read_csv('./data/_PHEMEext_target.csv')\n",
    "print(all_ext.shape)\n",
    "print(structure_ext.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_thread.to_csv('./data/all/_PHEMEext_thread.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>depth</th>\n      <th>SUM FriendsCount</th>\n      <th>AVG FriendsCount</th>\n      <th>AVG WordCount</th>\n      <th>AVG CharCount</th>\n      <th>AVG HashTag</th>\n      <th>SUM HashTag</th>\n      <th>Ratio HashTag</th>\n      <th>AVG Url</th>\n      <th>STD Url</th>\n      <th>RATIO Url</th>\n      <th>SUM Mention</th>\n      <th>AVG Mention</th>\n      <th>Ratio Mention</th>\n      <th>Tweets Count</th>\n      <th>Ratio Verified</th>\n      <th>SUM Verified</th>\n      <th>SUM RT</th>\n      <th>AVG RT</th>\n      <th>STD RT</th>\n      <th>AVG AccAge</th>\n      <th>STD AccAge</th>\n      <th>thread_time</th>\n      <th>STD Emoji</th>\n      <th>AVG Emoji</th>\n      <th>Ratio Media</th>\n      <th>RATIO Question</th>\n      <th>RATIO Exclaim</th>\n      <th>RATIO Period</th>\n      <th>AVG FPP</th>\n      <th>STD FPP</th>\n      <th>AVG SPP</th>\n      <th>STD SPP</th>\n      <th>AVG TPP</th>\n      <th>STD TPP</th>\n      <th>AVG Skepticism</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>41.55384</td>\n      <td>2.77026</td>\n      <td>8.80000</td>\n      <td>67.13333</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>0.00000</td>\n      <td>0.40000</td>\n      <td>0.48990</td>\n      <td>0.40000</td>\n      <td>16</td>\n      <td>1.06667</td>\n      <td>0.93333</td>\n      <td>15</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>118</td>\n      <td>7.86667</td>\n      <td>29.16817</td>\n      <td>939.13333</td>\n      <td>510.37501</td>\n      <td>-1413139073.00000</td>\n      <td>0.54160</td>\n      <td>0.20000</td>\n      <td>0.06667</td>\n      <td>0.06667</td>\n      <td>0.13333</td>\n      <td>0.93333</td>\n      <td>0.13333</td>\n      <td>0.33993</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.40000</td>\n      <td>0.71181</td>\n      <td>3.46667</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>49.35564</td>\n      <td>2.59767</td>\n      <td>7.47368</td>\n      <td>50.63158</td>\n      <td>0.10526</td>\n      <td>2</td>\n      <td>0.10526</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>23</td>\n      <td>1.21053</td>\n      <td>0.94737</td>\n      <td>19</td>\n      <td>0.05263</td>\n      <td>1</td>\n      <td>10416</td>\n      <td>548.21053</td>\n      <td>2322.56083</td>\n      <td>999.36842</td>\n      <td>478.35004</td>\n      <td>-1413138772.00000</td>\n      <td>0.30689</td>\n      <td>0.10526</td>\n      <td>0.00000</td>\n      <td>0.05263</td>\n      <td>0.31579</td>\n      <td>0.26316</td>\n      <td>0.26316</td>\n      <td>0.63595</td>\n      <td>0.36842</td>\n      <td>0.58133</td>\n      <td>0.10526</td>\n      <td>0.30689</td>\n      <td>2.68421</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>16.71898</td>\n      <td>2.38843</td>\n      <td>12.42857</td>\n      <td>78.42857</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>7</td>\n      <td>1.00000</td>\n      <td>0.85714</td>\n      <td>7</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>127</td>\n      <td>18.14286</td>\n      <td>44.03385</td>\n      <td>1126.85714</td>\n      <td>792.28863</td>\n      <td>-1413357828.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.14286</td>\n      <td>0.14286</td>\n      <td>0.57143</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.71429</td>\n      <td>0.45175</td>\n      <td>2.71429</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>31.75615</td>\n      <td>2.44278</td>\n      <td>11.15385</td>\n      <td>74.07692</td>\n      <td>0.07692</td>\n      <td>1</td>\n      <td>0.07692</td>\n      <td>0.23077</td>\n      <td>0.42133</td>\n      <td>0.23077</td>\n      <td>17</td>\n      <td>1.30769</td>\n      <td>0.92308</td>\n      <td>13</td>\n      <td>0.07692</td>\n      <td>1</td>\n      <td>195</td>\n      <td>15.00000</td>\n      <td>51.09719</td>\n      <td>700.38462</td>\n      <td>553.63047</td>\n      <td>-1413140606.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.15385</td>\n      <td>0.07692</td>\n      <td>0.07692</td>\n      <td>0.61538</td>\n      <td>0.30769</td>\n      <td>0.46154</td>\n      <td>0.07692</td>\n      <td>0.26647</td>\n      <td>0.38462</td>\n      <td>0.62493</td>\n      <td>3.23077</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>10.72732</td>\n      <td>2.14546</td>\n      <td>11.60000</td>\n      <td>96.00000</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>0.00000</td>\n      <td>0.40000</td>\n      <td>0.48990</td>\n      <td>0.40000</td>\n      <td>12</td>\n      <td>2.40000</td>\n      <td>1.00000</td>\n      <td>5</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>197</td>\n      <td>39.40000</td>\n      <td>78.30096</td>\n      <td>1099.60000</td>\n      <td>695.26760</td>\n      <td>-1413136436.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.20000</td>\n      <td>0.00000</td>\n      <td>0.80000</td>\n      <td>0.20000</td>\n      <td>0.40000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>4.20000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>52.33015</td>\n      <td>2.61651</td>\n      <td>12.55000</td>\n      <td>84.85000</td>\n      <td>0.05000</td>\n      <td>1</td>\n      <td>0.05000</td>\n      <td>0.25000</td>\n      <td>0.43301</td>\n      <td>0.25000</td>\n      <td>26</td>\n      <td>1.30000</td>\n      <td>0.95000</td>\n      <td>20</td>\n      <td>0.10000</td>\n      <td>2</td>\n      <td>1469</td>\n      <td>73.45000</td>\n      <td>319.01481</td>\n      <td>1295.35000</td>\n      <td>584.85667</td>\n      <td>-1413142049.00000</td>\n      <td>0.35707</td>\n      <td>0.15000</td>\n      <td>0.00000</td>\n      <td>0.05000</td>\n      <td>0.20000</td>\n      <td>0.55000</td>\n      <td>0.75000</td>\n      <td>0.88741</td>\n      <td>0.15000</td>\n      <td>0.35707</td>\n      <td>0.20000</td>\n      <td>0.40000</td>\n      <td>4.10000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>73.00639</td>\n      <td>2.70394</td>\n      <td>10.55556</td>\n      <td>85.66667</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>0.00000</td>\n      <td>0.07407</td>\n      <td>0.26189</td>\n      <td>0.07407</td>\n      <td>39</td>\n      <td>1.44444</td>\n      <td>0.96296</td>\n      <td>27</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>581</td>\n      <td>21.51852</td>\n      <td>108.54948</td>\n      <td>1154.18519</td>\n      <td>521.98929</td>\n      <td>-1413133719.00000</td>\n      <td>0.62854</td>\n      <td>0.22222</td>\n      <td>0.40741</td>\n      <td>0.22222</td>\n      <td>0.07407</td>\n      <td>0.66667</td>\n      <td>0.03704</td>\n      <td>0.18885</td>\n      <td>0.22222</td>\n      <td>0.49690</td>\n      <td>0.44444</td>\n      <td>0.95581</td>\n      <td>2.70370</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3</td>\n      <td>23.82766</td>\n      <td>2.64752</td>\n      <td>13.00000</td>\n      <td>96.66667</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>14</td>\n      <td>1.55556</td>\n      <td>0.88889</td>\n      <td>9</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>849</td>\n      <td>94.33333</td>\n      <td>243.60761</td>\n      <td>835.44444</td>\n      <td>329.65845</td>\n      <td>-1413137001.00000</td>\n      <td>0.41574</td>\n      <td>0.22222</td>\n      <td>0.44444</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.66667</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.22222</td>\n      <td>0.41574</td>\n      <td>3.88889</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>5</td>\n      <td>23.89819</td>\n      <td>2.38982</td>\n      <td>10.70000</td>\n      <td>78.80000</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>0.00000</td>\n      <td>0.10000</td>\n      <td>0.30000</td>\n      <td>0.10000</td>\n      <td>14</td>\n      <td>1.40000</td>\n      <td>0.90000</td>\n      <td>10</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>153</td>\n      <td>15.30000</td>\n      <td>36.77785</td>\n      <td>855.00000</td>\n      <td>724.36745</td>\n      <td>-1413136852.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.10000</td>\n      <td>0.00000</td>\n      <td>0.80000</td>\n      <td>0.10000</td>\n      <td>0.30000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.50000</td>\n      <td>0.92195</td>\n      <td>3.20000</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>3</td>\n      <td>39.22661</td>\n      <td>2.45166</td>\n      <td>12.62500</td>\n      <td>87.12500</td>\n      <td>0.31250</td>\n      <td>5</td>\n      <td>0.31250</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>20</td>\n      <td>1.25000</td>\n      <td>0.93750</td>\n      <td>16</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>144</td>\n      <td>9.00000</td>\n      <td>34.59949</td>\n      <td>1087.06250</td>\n      <td>550.85235</td>\n      <td>-1413192966.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.18750</td>\n      <td>0.18750</td>\n      <td>0.31250</td>\n      <td>0.06250</td>\n      <td>0.24206</td>\n      <td>0.25000</td>\n      <td>0.75000</td>\n      <td>0.50000</td>\n      <td>0.70711</td>\n      <td>3.50000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>4</td>\n      <td>78.86282</td>\n      <td>2.92085</td>\n      <td>16.59259</td>\n      <td>111.55556</td>\n      <td>0.07407</td>\n      <td>2</td>\n      <td>0.07407</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>27</td>\n      <td>1.00000</td>\n      <td>0.88889</td>\n      <td>27</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>1336</td>\n      <td>49.48148</td>\n      <td>169.76091</td>\n      <td>1384.00000</td>\n      <td>531.60435</td>\n      <td>-1413125063.00000</td>\n      <td>0.41574</td>\n      <td>0.11111</td>\n      <td>0.00000</td>\n      <td>0.14815</td>\n      <td>0.07407</td>\n      <td>0.74074</td>\n      <td>0.07407</td>\n      <td>0.26189</td>\n      <td>0.18519</td>\n      <td>0.38845</td>\n      <td>0.85185</td>\n      <td>0.75541</td>\n      <td>5.33333</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>8</td>\n      <td>44.76577</td>\n      <td>2.35609</td>\n      <td>8.00000</td>\n      <td>65.26316</td>\n      <td>0.05263</td>\n      <td>1</td>\n      <td>0.05263</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>32</td>\n      <td>1.68421</td>\n      <td>0.94737</td>\n      <td>19</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>250</td>\n      <td>13.15789</td>\n      <td>55.35366</td>\n      <td>836.57895</td>\n      <td>594.40924</td>\n      <td>-1413134719.00000</td>\n      <td>1.98610</td>\n      <td>1.05263</td>\n      <td>0.10526</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.10526</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.10526</td>\n      <td>0.30689</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>3.84211</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>5</td>\n      <td>66.17276</td>\n      <td>2.64691</td>\n      <td>10.24000</td>\n      <td>77.08000</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>0.00000</td>\n      <td>0.12000</td>\n      <td>0.32496</td>\n      <td>0.12000</td>\n      <td>38</td>\n      <td>1.52000</td>\n      <td>0.92000</td>\n      <td>25</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>3878</td>\n      <td>155.12000</td>\n      <td>734.27000</td>\n      <td>1029.84000</td>\n      <td>656.33599</td>\n      <td>-1413136755.00000</td>\n      <td>0.43081</td>\n      <td>0.12000</td>\n      <td>0.12000</td>\n      <td>0.12000</td>\n      <td>0.04000</td>\n      <td>0.44000</td>\n      <td>0.04000</td>\n      <td>0.19596</td>\n      <td>0.20000</td>\n      <td>0.48990</td>\n      <td>0.24000</td>\n      <td>0.51225</td>\n      <td>2.40000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>3</td>\n      <td>34.58284</td>\n      <td>2.47020</td>\n      <td>13.64286</td>\n      <td>99.78571</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>0.00000</td>\n      <td>0.07143</td>\n      <td>0.25754</td>\n      <td>0.07143</td>\n      <td>17</td>\n      <td>1.21429</td>\n      <td>0.92857</td>\n      <td>14</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>268</td>\n      <td>19.14286</td>\n      <td>69.02055</td>\n      <td>801.57143</td>\n      <td>389.16958</td>\n      <td>-1413137327.00000</td>\n      <td>1.47254</td>\n      <td>0.78571</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.85714</td>\n      <td>0.07143</td>\n      <td>0.25754</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>4.14286</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1</td>\n      <td>2.90037</td>\n      <td>2.90037</td>\n      <td>18.00000</td>\n      <td>100.00000</td>\n      <td>1.00000</td>\n      <td>1</td>\n      <td>1.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>1</td>\n      <td>1.00000</td>\n      <td>1</td>\n      <td>3</td>\n      <td>3.00000</td>\n      <td>0.00000</td>\n      <td>2261.00000</td>\n      <td>0.00000</td>\n      <td>-1415130785.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>3.00000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "    depth  SUM FriendsCount  AVG FriendsCount  AVG WordCount  AVG CharCount  \\\n0       4          41.55384           2.77026        8.80000       67.13333   \n1       4          49.35564           2.59767        7.47368       50.63158   \n2       2          16.71898           2.38843       12.42857       78.42857   \n3       3          31.75615           2.44278       11.15385       74.07692   \n4       2          10.72732           2.14546       11.60000       96.00000   \n5       5          52.33015           2.61651       12.55000       84.85000   \n6       6          73.00639           2.70394       10.55556       85.66667   \n7       3          23.82766           2.64752       13.00000       96.66667   \n8       5          23.89819           2.38982       10.70000       78.80000   \n9       3          39.22661           2.45166       12.62500       87.12500   \n10      4          78.86282           2.92085       16.59259      111.55556   \n11      8          44.76577           2.35609        8.00000       65.26316   \n12      5          66.17276           2.64691       10.24000       77.08000   \n13      3          34.58284           2.47020       13.64286       99.78571   \n14      1           2.90037           2.90037       18.00000      100.00000   \n\n    AVG HashTag  SUM HashTag  Ratio HashTag  AVG Url  STD Url  RATIO Url  \\\n0       0.00000            0        0.00000  0.40000  0.48990    0.40000   \n1       0.10526            2        0.10526  0.00000  0.00000    0.00000   \n2       0.00000            0        0.00000  0.00000  0.00000    0.00000   \n3       0.07692            1        0.07692  0.23077  0.42133    0.23077   \n4       0.00000            0        0.00000  0.40000  0.48990    0.40000   \n5       0.05000            1        0.05000  0.25000  0.43301    0.25000   \n6       0.00000            0        0.00000  0.07407  0.26189    0.07407   \n7       0.00000            0        0.00000  0.00000  0.00000    0.00000   \n8       0.00000            0        0.00000  0.10000  0.30000    0.10000   \n9       0.31250            5        0.31250  0.00000  0.00000    0.00000   \n10      0.07407            2        0.07407  0.00000  0.00000    0.00000   \n11      0.05263            1        0.05263  0.00000  0.00000    0.00000   \n12      0.00000            0        0.00000  0.12000  0.32496    0.12000   \n13      0.00000            0        0.00000  0.07143  0.25754    0.07143   \n14      1.00000            1        1.00000  0.00000  0.00000    0.00000   \n\n    SUM Mention  AVG Mention  Ratio Mention  Tweets Count  Ratio Verified  \\\n0            16      1.06667        0.93333            15         0.00000   \n1            23      1.21053        0.94737            19         0.05263   \n2             7      1.00000        0.85714             7         0.00000   \n3            17      1.30769        0.92308            13         0.07692   \n4            12      2.40000        1.00000             5         0.00000   \n5            26      1.30000        0.95000            20         0.10000   \n6            39      1.44444        0.96296            27         0.00000   \n7            14      1.55556        0.88889             9         0.00000   \n8            14      1.40000        0.90000            10         0.00000   \n9            20      1.25000        0.93750            16         0.00000   \n10           27      1.00000        0.88889            27         0.00000   \n11           32      1.68421        0.94737            19         0.00000   \n12           38      1.52000        0.92000            25         0.00000   \n13           17      1.21429        0.92857            14         0.00000   \n14            0      0.00000        0.00000             1         1.00000   \n\n    SUM Verified  SUM RT    AVG RT     STD RT  AVG AccAge  STD AccAge  \\\n0              0     118   7.86667   29.16817   939.13333   510.37501   \n1              1   10416 548.21053 2322.56083   999.36842   478.35004   \n2              0     127  18.14286   44.03385  1126.85714   792.28863   \n3              1     195  15.00000   51.09719   700.38462   553.63047   \n4              0     197  39.40000   78.30096  1099.60000   695.26760   \n5              2    1469  73.45000  319.01481  1295.35000   584.85667   \n6              0     581  21.51852  108.54948  1154.18519   521.98929   \n7              0     849  94.33333  243.60761   835.44444   329.65845   \n8              0     153  15.30000   36.77785   855.00000   724.36745   \n9              0     144   9.00000   34.59949  1087.06250   550.85235   \n10             0    1336  49.48148  169.76091  1384.00000   531.60435   \n11             0     250  13.15789   55.35366   836.57895   594.40924   \n12             0    3878 155.12000  734.27000  1029.84000   656.33599   \n13             0     268  19.14286   69.02055   801.57143   389.16958   \n14             1       3   3.00000    0.00000  2261.00000     0.00000   \n\n         thread_time  STD Emoji  AVG Emoji  Ratio Media  RATIO Question  \\\n0  -1413139073.00000    0.54160    0.20000      0.06667         0.06667   \n1  -1413138772.00000    0.30689    0.10526      0.00000         0.05263   \n2  -1413357828.00000    0.00000    0.00000      0.00000         0.14286   \n3  -1413140606.00000    0.00000    0.00000      0.15385         0.07692   \n4  -1413136436.00000    0.00000    0.00000      0.00000         0.20000   \n5  -1413142049.00000    0.35707    0.15000      0.00000         0.05000   \n6  -1413133719.00000    0.62854    0.22222      0.40741         0.22222   \n7  -1413137001.00000    0.41574    0.22222      0.44444         0.00000   \n8  -1413136852.00000    0.00000    0.00000      0.00000         0.10000   \n9  -1413192966.00000    0.00000    0.00000      0.00000         0.18750   \n10 -1413125063.00000    0.41574    0.11111      0.00000         0.14815   \n11 -1413134719.00000    1.98610    1.05263      0.10526         0.00000   \n12 -1413136755.00000    0.43081    0.12000      0.12000         0.12000   \n13 -1413137327.00000    1.47254    0.78571      0.00000         0.00000   \n14 -1415130785.00000    0.00000    0.00000      0.00000         0.00000   \n\n    RATIO Exclaim  RATIO Period  AVG FPP  STD FPP  AVG SPP  STD SPP  AVG TPP  \\\n0         0.13333       0.93333  0.13333  0.33993  0.00000  0.00000  0.40000   \n1         0.31579       0.26316  0.26316  0.63595  0.36842  0.58133  0.10526   \n2         0.14286       0.57143  0.00000  0.00000  0.00000  0.00000  0.71429   \n3         0.07692       0.61538  0.30769  0.46154  0.07692  0.26647  0.38462   \n4         0.00000       0.80000  0.20000  0.40000  0.00000  0.00000  0.00000   \n5         0.20000       0.55000  0.75000  0.88741  0.15000  0.35707  0.20000   \n6         0.07407       0.66667  0.03704  0.18885  0.22222  0.49690  0.44444   \n7         0.00000       0.66667  0.00000  0.00000  0.00000  0.00000  0.22222   \n8         0.00000       0.80000  0.10000  0.30000  0.00000  0.00000  0.50000   \n9         0.18750       0.31250  0.06250  0.24206  0.25000  0.75000  0.50000   \n10        0.07407       0.74074  0.07407  0.26189  0.18519  0.38845  0.85185   \n11        0.00000       0.10526  0.00000  0.00000  0.10526  0.30689  0.00000   \n12        0.04000       0.44000  0.04000  0.19596  0.20000  0.48990  0.24000   \n13        0.00000       0.85714  0.07143  0.25754  0.00000  0.00000  0.00000   \n14        0.00000       0.00000  0.00000  0.00000  0.00000  0.00000  0.00000   \n\n    STD TPP  AVG Skepticism  \n0   0.71181         3.46667  \n1   0.30689         2.68421  \n2   0.45175         2.71429  \n3   0.62493         3.23077  \n4   0.00000         4.20000  \n5   0.40000         4.10000  \n6   0.95581         2.70370  \n7   0.41574         3.88889  \n8   0.92195         3.20000  \n9   0.70711         3.50000  \n10  0.75541         5.33333  \n11  0.00000         3.84211  \n12  0.51225         2.40000  \n13  0.00000         4.14286  \n14  0.00000         3.00000  "
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SUM FriendsCount\tAVG FriendsCount 들이 -inf 값 포함\n",
    "ext_thread = getThreadInfo(structure_ext, all_ext)\n",
    "# ext_thread = ext_thread.replace(-np.inf, 0)\n",
    "ext_thread = ext_thread.fillna(0)\n",
    "ext_thread = ext_thread.replace(-np.inf, 0)\n",
    "ext_thread.head(15)\n",
    "# ext_thread.to_csv('./data/_PHEMEext_thread.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 👇 Thread 정보만을 추출한 결과\n",
    "아래의 Features들은 모두 한 Root 트윗에 달린 Thread의 정보를 포함한다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>depth</th>\n      <th>SUM FriendsCount</th>\n      <th>AVG FriendsCount</th>\n      <th>AVG WordCount</th>\n      <th>AVG CharCount</th>\n      <th>AVG HashTag</th>\n      <th>SUM HashTag</th>\n      <th>Ratio HashTag</th>\n      <th>AVG Url</th>\n      <th>STD Url</th>\n      <th>RATIO Url</th>\n      <th>SUM Mention</th>\n      <th>AVG Mention</th>\n      <th>Ratio Mention</th>\n      <th>Tweets Count</th>\n      <th>Ratio Verified</th>\n      <th>SUM Verified</th>\n      <th>SUM RT</th>\n      <th>AVG RT</th>\n      <th>STD RT</th>\n      <th>AVG AccAge</th>\n      <th>STD AccAge</th>\n      <th>thread_time</th>\n      <th>STD Emoji</th>\n      <th>AVG Emoji</th>\n      <th>Ratio Media</th>\n      <th>RATIO Question</th>\n      <th>RATIO Exclaim</th>\n      <th>RATIO Period</th>\n      <th>AVG FPP</th>\n      <th>STD FPP</th>\n      <th>AVG SPP</th>\n      <th>STD SPP</th>\n      <th>AVG TPP</th>\n      <th>STD TPP</th>\n      <th>AVG Skepticism</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n      <td>485.00000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>2.19794</td>\n      <td>10.84186</td>\n      <td>2.72841</td>\n      <td>14.26121</td>\n      <td>99.20002</td>\n      <td>0.84626</td>\n      <td>1.79381</td>\n      <td>0.41010</td>\n      <td>0.34205</td>\n      <td>0.13854</td>\n      <td>0.33213</td>\n      <td>5.11959</td>\n      <td>0.88153</td>\n      <td>0.58595</td>\n      <td>4.04742</td>\n      <td>0.10093</td>\n      <td>0.26186</td>\n      <td>54.36907</td>\n      <td>6.15717</td>\n      <td>12.34183</td>\n      <td>1407.53838</td>\n      <td>306.40342</td>\n      <td>-1420562049.76082</td>\n      <td>0.08762</td>\n      <td>0.05846</td>\n      <td>0.17974</td>\n      <td>0.21324</td>\n      <td>0.14564</td>\n      <td>0.75296</td>\n      <td>0.27241</td>\n      <td>0.24974</td>\n      <td>0.09706</td>\n      <td>0.08811</td>\n      <td>0.35316</td>\n      <td>0.25523</td>\n      <td>3.22114</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.70257</td>\n      <td>12.99575</td>\n      <td>0.51150</td>\n      <td>4.16138</td>\n      <td>25.41987</td>\n      <td>1.11570</td>\n      <td>2.08169</td>\n      <td>0.41270</td>\n      <td>0.42563</td>\n      <td>0.21214</td>\n      <td>0.40211</td>\n      <td>8.14323</td>\n      <td>0.80019</td>\n      <td>0.40283</td>\n      <td>4.83785</td>\n      <td>0.25315</td>\n      <td>0.56361</td>\n      <td>514.15357</td>\n      <td>27.35858</td>\n      <td>112.42284</td>\n      <td>599.53507</td>\n      <td>330.59886</td>\n      <td>5668373.99026</td>\n      <td>0.34064</td>\n      <td>0.28811</td>\n      <td>0.32602</td>\n      <td>0.32331</td>\n      <td>0.26356</td>\n      <td>0.30629</td>\n      <td>0.43975</td>\n      <td>0.39512</td>\n      <td>0.28692</td>\n      <td>0.19393</td>\n      <td>0.46498</td>\n      <td>0.35308</td>\n      <td>1.49462</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>4.00000</td>\n      <td>39.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>1.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>2.00000</td>\n      <td>0.12500</td>\n      <td>0.00000</td>\n      <td>5.00000</td>\n      <td>0.00000</td>\n      <td>-1426484176.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.00000</td>\n      <td>2.98811</td>\n      <td>2.46835</td>\n      <td>11.00000</td>\n      <td>81.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>1.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>2.00000</td>\n      <td>1.75000</td>\n      <td>0.00000</td>\n      <td>1028.33333</td>\n      <td>0.00000</td>\n      <td>-1426316918.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.50000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>2.00000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2.00000</td>\n      <td>5.47855</td>\n      <td>2.73241</td>\n      <td>14.00000</td>\n      <td>96.85714</td>\n      <td>0.50000</td>\n      <td>1.00000</td>\n      <td>0.25000</td>\n      <td>0.15789</td>\n      <td>0.00000</td>\n      <td>0.14286</td>\n      <td>2.00000</td>\n      <td>0.93333</td>\n      <td>0.75000</td>\n      <td>2.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>5.00000</td>\n      <td>2.75000</td>\n      <td>1.00000</td>\n      <td>1390.61538</td>\n      <td>186.20201</td>\n      <td>-1415143918.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>1.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.16667</td>\n      <td>0.00000</td>\n      <td>3.00000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>3.00000</td>\n      <td>13.37434</td>\n      <td>3.00346</td>\n      <td>17.00000</td>\n      <td>121.00000</td>\n      <td>1.00000</td>\n      <td>3.00000</td>\n      <td>1.00000</td>\n      <td>0.53846</td>\n      <td>0.39031</td>\n      <td>0.50000</td>\n      <td>6.00000</td>\n      <td>1.25000</td>\n      <td>0.96296</td>\n      <td>5.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>12.00000</td>\n      <td>5.00000</td>\n      <td>4.00000</td>\n      <td>1924.00000</td>\n      <td>575.88878</td>\n      <td>-1415124588.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.20000</td>\n      <td>0.33333</td>\n      <td>0.20000</td>\n      <td>1.00000</td>\n      <td>0.50000</td>\n      <td>0.47140</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.60000</td>\n      <td>0.48412</td>\n      <td>4.00000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>16.00000</td>\n      <td>78.86282</td>\n      <td>4.38684</td>\n      <td>29.00000</td>\n      <td>143.00000</td>\n      <td>7.00000</td>\n      <td>15.00000</td>\n      <td>1.00000</td>\n      <td>2.00000</td>\n      <td>0.94281</td>\n      <td>1.00000</td>\n      <td>44.00000</td>\n      <td>6.50000</td>\n      <td>1.00000</td>\n      <td>27.00000</td>\n      <td>1.00000</td>\n      <td>5.00000</td>\n      <td>10416.00000</td>\n      <td>548.21053</td>\n      <td>2322.56083</td>\n      <td>3021.00000</td>\n      <td>1371.00000</td>\n      <td>-1413125063.00000</td>\n      <td>4.00000</td>\n      <td>4.00000</td>\n      <td>1.00000</td>\n      <td>1.00000</td>\n      <td>1.00000</td>\n      <td>1.00000</td>\n      <td>4.00000</td>\n      <td>2.00000</td>\n      <td>3.00000</td>\n      <td>1.00000</td>\n      <td>2.00000</td>\n      <td>1.93907</td>\n      <td>9.00000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "          depth  SUM FriendsCount  AVG FriendsCount  AVG WordCount  \\\ncount 485.00000         485.00000         485.00000      485.00000   \nmean    2.19794          10.84186           2.72841       14.26121   \nstd     1.70257          12.99575           0.51150        4.16138   \nmin     1.00000           0.00000           0.00000        4.00000   \n25%     1.00000           2.98811           2.46835       11.00000   \n50%     2.00000           5.47855           2.73241       14.00000   \n75%     3.00000          13.37434           3.00346       17.00000   \nmax    16.00000          78.86282           4.38684       29.00000   \n\n       AVG CharCount  AVG HashTag  SUM HashTag  Ratio HashTag   AVG Url  \\\ncount      485.00000    485.00000    485.00000      485.00000 485.00000   \nmean        99.20002      0.84626      1.79381        0.41010   0.34205   \nstd         25.41987      1.11570      2.08169        0.41270   0.42563   \nmin         39.00000      0.00000      0.00000        0.00000   0.00000   \n25%         81.00000      0.00000      0.00000        0.00000   0.00000   \n50%         96.85714      0.50000      1.00000        0.25000   0.15789   \n75%        121.00000      1.00000      3.00000        1.00000   0.53846   \nmax        143.00000      7.00000     15.00000        1.00000   2.00000   \n\n        STD Url  RATIO Url  SUM Mention  AVG Mention  Ratio Mention  \\\ncount 485.00000  485.00000    485.00000    485.00000      485.00000   \nmean    0.13854    0.33213      5.11959      0.88153        0.58595   \nstd     0.21214    0.40211      8.14323      0.80019        0.40283   \nmin     0.00000    0.00000      0.00000      0.00000        0.00000   \n25%     0.00000    0.00000      0.00000      0.00000        0.00000   \n50%     0.00000    0.14286      2.00000      0.93333        0.75000   \n75%     0.39031    0.50000      6.00000      1.25000        0.96296   \nmax     0.94281    1.00000     44.00000      6.50000        1.00000   \n\n       Tweets Count  Ratio Verified  SUM Verified      SUM RT    AVG RT  \\\ncount     485.00000       485.00000     485.00000   485.00000 485.00000   \nmean        4.04742         0.10093       0.26186    54.36907   6.15717   \nstd         4.83785         0.25315       0.56361   514.15357  27.35858   \nmin         1.00000         0.00000       0.00000     2.00000   0.12500   \n25%         1.00000         0.00000       0.00000     2.00000   1.75000   \n50%         2.00000         0.00000       0.00000     5.00000   2.75000   \n75%         5.00000         0.00000       0.00000    12.00000   5.00000   \nmax        27.00000         1.00000       5.00000 10416.00000 548.21053   \n\n          STD RT  AVG AccAge  STD AccAge       thread_time  STD Emoji  \\\ncount  485.00000   485.00000   485.00000         485.00000  485.00000   \nmean    12.34183  1407.53838   306.40342 -1420562049.76082    0.08762   \nstd    112.42284   599.53507   330.59886     5668373.99026    0.34064   \nmin      0.00000     5.00000     0.00000 -1426484176.00000    0.00000   \n25%      0.00000  1028.33333     0.00000 -1426316918.00000    0.00000   \n50%      1.00000  1390.61538   186.20201 -1415143918.00000    0.00000   \n75%      4.00000  1924.00000   575.88878 -1415124588.00000    0.00000   \nmax   2322.56083  3021.00000  1371.00000 -1413125063.00000    4.00000   \n\n       AVG Emoji  Ratio Media  RATIO Question  RATIO Exclaim  RATIO Period  \\\ncount  485.00000    485.00000       485.00000      485.00000     485.00000   \nmean     0.05846      0.17974         0.21324        0.14564       0.75296   \nstd      0.28811      0.32602         0.32331        0.26356       0.30629   \nmin      0.00000      0.00000         0.00000        0.00000       0.00000   \n25%      0.00000      0.00000         0.00000        0.00000       0.50000   \n50%      0.00000      0.00000         0.00000        0.00000       1.00000   \n75%      0.00000      0.20000         0.33333        0.20000       1.00000   \nmax      4.00000      1.00000         1.00000        1.00000       1.00000   \n\n        AVG FPP   STD FPP   AVG SPP   STD SPP   AVG TPP   STD TPP  \\\ncount 485.00000 485.00000 485.00000 485.00000 485.00000 485.00000   \nmean    0.27241   0.24974   0.09706   0.08811   0.35316   0.25523   \nstd     0.43975   0.39512   0.28692   0.19393   0.46498   0.35308   \nmin     0.00000   0.00000   0.00000   0.00000   0.00000   0.00000   \n25%     0.00000   0.00000   0.00000   0.00000   0.00000   0.00000   \n50%     0.00000   0.00000   0.00000   0.00000   0.16667   0.00000   \n75%     0.50000   0.47140   0.00000   0.00000   0.60000   0.48412   \nmax     4.00000   2.00000   3.00000   1.00000   2.00000   1.93907   \n\n       AVG Skepticism  \ncount       485.00000  \nmean          3.22114  \nstd           1.49462  \nmin           0.00000  \n25%           2.00000  \n50%           3.00000  \n75%           4.00000  \nmax           9.00000  "
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ext_thread.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['depth', 'SUM FriendsCount', 'AVG FriendsCount', 'AVG WordCount',\n       'AVG CharCount', 'AVG HashTag', 'SUM HashTag', 'Ratio HashTag',\n       'AVG Url', 'STD Url', 'RATIO Url', 'SUM Mention', 'AVG Mention',\n       'Ratio Mention', 'Tweets Count', 'Ratio Verified', 'SUM Verified',\n       'SUM RT', 'AVG RT', 'STD RT', 'AVG AccAge', 'STD AccAge', 'thread_time',\n       'STD Emoji', 'AVG Emoji', 'Ratio Media', 'RATIO Question',\n       'RATIO Exclaim', 'RATIO Period', 'AVG FPP', 'STD FPP', 'AVG SPP',\n       'STD SPP', 'AVG TPP', 'STD TPP', 'AVG Skepticism'],\n      dtype='object')"
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ext_thread.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>id</th>\n      <th>pid</th>\n      <th>emoji_count</th>\n      <th>has_media</th>\n      <th>URLcount</th>\n      <th>Skepticism</th>\n      <th>MentionCount</th>\n      <th>token_for_POS</th>\n      <th>Noun</th>\n      <th>Verb</th>\n      <th>Adjective</th>\n      <th>Pronoun</th>\n      <th>FirstPersonPronoun</th>\n      <th>SecondPersonPronoun</th>\n      <th>ThirdPersonPronoun</th>\n      <th>Adverb</th>\n      <th>Numeral</th>\n      <th>Conjunction_inj</th>\n      <th>Particle</th>\n      <th>Determiner</th>\n      <th>Modal</th>\n      <th>Whs</th>\n      <th>char_count</th>\n      <th>word_count</th>\n      <th>HashTag</th>\n      <th>has_question</th>\n      <th>has_exclaim</th>\n      <th>has_period</th>\n      <th>retweet_count</th>\n      <th>isRT</th>\n      <th>tweet_count</th>\n      <th>listed_count</th>\n      <th>friends_count</th>\n      <th>follow_ratio</th>\n      <th>account_age_days</th>\n      <th>tweet_created</th>\n      <th>capital_ratio</th>\n      <th>verified</th>\n      <th>Event</th>\n      <th>isSrcTweet</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>15</th>\n      <td>@MichaelEssien Glad you are healthy and well! #ForzaMilan</td>\n      <td>521368486053150721</td>\n      <td>521367917322338304.00000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>[glad, you, are, healthy, and, well]</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>57</td>\n      <td>8</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4.59222</td>\n      <td>2.09342</td>\n      <td>2.99782</td>\n      <td>3.52840</td>\n      <td>2118</td>\n      <td>1413138908.00000</td>\n      <td>0.08772</td>\n      <td>0</td>\n      <td>ebola-essien</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>@MichaelEssien that's a shame wanted to Invest in you😔</td>\n      <td>521368752135610368</td>\n      <td>521367917322338304.00000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>[that, is, a, shame, wanted, to, invest, in, you]</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>54</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>4.86344</td>\n      <td>0.69897</td>\n      <td>2.75282</td>\n      <td>4.02057</td>\n      <td>342</td>\n      <td>1413138971.00000</td>\n      <td>0.05556</td>\n      <td>0</td>\n      <td>ebola-essien</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>@MichaelEssien u got kik?</td>\n      <td>521368597734912000</td>\n      <td>521367917322338304.00000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>[u, got, kik]</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>25</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4.66114</td>\n      <td>0.47712</td>\n      <td>1.99123</td>\n      <td>3.22968</td>\n      <td>512</td>\n      <td>1413138934.00000</td>\n      <td>0.08000</td>\n      <td>0</td>\n      <td>ebola-essien</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                                         text  \\\n15  @MichaelEssien Glad you are healthy and well! #ForzaMilan   \n16     @MichaelEssien that's a shame wanted to Invest in you😔   \n18                                  @MichaelEssien u got kik?   \n\n                    id                      pid  emoji_count  has_media  \\\n15  521368486053150721 521367917322338304.00000            0          0   \n16  521368752135610368 521367917322338304.00000            1          0   \n18  521368597734912000 521367917322338304.00000            0          0   \n\n    URLcount  Skepticism  MentionCount  \\\n15         0           3             1   \n16         0           3             1   \n18         0           1             1   \n\n                                        token_for_POS  Noun  Verb  Adjective  \\\n15               [glad, you, are, healthy, and, well]     1     1          1   \n16  [that, is, a, shame, wanted, to, invest, in, you]     1     3          0   \n18                                      [u, got, kik]     1     1          1   \n\n    Pronoun  FirstPersonPronoun  SecondPersonPronoun  ThirdPersonPronoun  \\\n15        1                   0                    1                   0   \n16        1                   0                    1                   0   \n18        0                   0                    0                   0   \n\n    Adverb  Numeral  Conjunction_inj  Particle  Determiner  Modal  Whs  \\\n15       1        0                1         0           0      0    0   \n16       0        0                1         0           2      0    0   \n18       0        0                0         0           0      0    0   \n\n    char_count  word_count  HashTag  has_question  has_exclaim  has_period  \\\n15          57           8        1             0            1           0   \n16          54           9        0             0            0           0   \n18          25           4        0             1            0           0   \n\n    retweet_count isRT  tweet_count  listed_count  friends_count  \\\n15              0    0      4.59222       2.09342        2.99782   \n16              2    0      4.86344       0.69897        2.75282   \n18              1    0      4.66114       0.47712        1.99123   \n\n    follow_ratio  account_age_days    tweet_created  capital_ratio  verified  \\\n15       3.52840              2118 1413138908.00000        0.08772         0   \n16       4.02057               342 1413138971.00000        0.05556         0   \n18       3.22968               512 1413138934.00000        0.08000         0   \n\n           Event  isSrcTweet  target  \n15  ebola-essien           0       1  \n16  ebola-essien           0       1  \n18  ebola-essien           0       1  "
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ext.loc[(all_ext['pid'] == int(521367917322338304))].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT & EMOJI 이모지 다루는 차이 😂😂😂😂😂\n",
    "\n",
    "> Before applying fastBPE to the pre-training corpus of 850M English Tweets, we tokenized these Tweets using TweetTokenizer from the NLTK toolkit and used the emoji package to translate emotion icons into text strings (here, each icon is referred to as a word token). We also normalized the Tweets by converting user mentions and web/url links into special tokens @USER and HTTPURL, respectively. Thus it is recommended to also apply the same pre-processing step for BERTweet-based downstream applications w.r.t. the raw input Tweets. BERTweet provides this pre-processing step by enabling the normalization argument.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer \n",
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\", )\n",
    "\n",
    "# For transformers v4.x+: \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[array(['2014-10-12 21:22:36+00:00'], dtype=object),\n array([], dtype=object),\n array(['2014-10-12 18:38:41+00:00'], dtype=object),\n array(['2014-10-12 18:43:07+00:00'], dtype=object),\n array(['2014-10-12 18:42:02+00:00'], dtype=object),\n array(['2014-10-12 18:44:13+00:00'], dtype=object),\n array(['2014-10-12 19:15:21+00:00'], dtype=object),\n array([], dtype=object),\n array(['2014-10-12 18:54:47+00:00'], dtype=object),\n array(['2014-10-12 18:39:06+00:00'], dtype=object),\n array(['2014-10-12 18:41:23+00:00'], dtype=object),\n array(['2014-10-12 18:39:51+00:00'], dtype=object),\n array([], dtype=object),\n array(['2014-10-12 18:56:53+00:00'], dtype=object)]"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ext_thread.thread_time[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT TWEET IS ALREADY NORMALIZED!\n",
    "line = \"@MichaelEssien that's a shame wanted to Invest in you😔\"\n",
    "print(line)\n",
    "print(tokenizer.encode(line),\"\\n\")\n",
    "input_ids = torch.tensor([tokenizer.encode(line)])\n",
    "# print(input_ids)\n",
    "\n",
    "line = \"HTTPURL @MichaelEssien that's a shame wanted to Invest INVEST in you😔😂😂 http://www.google.com\"\n",
    "input_ids = torch.tensor([tokenizer.encode(line)])\n",
    "print(line)\n",
    "print(tokenizer.encode(line),\"\\n\")\n",
    "\n",
    "line = \"HTTPURL @USER that's a shame wanted to Invest INVEST in you:pensive_face::face_with_tears_of_joy::face_with_tears_of_joy: HTTPURL\"\n",
    "input_ids = torch.tensor([tokenizer.encode(line)])\n",
    "print(line)\n",
    "print(tokenizer.encode(line),\"\\n\")\n",
    "\n",
    "line = \"HTTPURL @USER that's a shame wanted to Invest INVEST in you pensive_face face_with_tears_of_joy face_with_tears_of_joy :grinning_face_with_big_eyes: HTTPURL  \"\n",
    "input_ids = torch.tensor([tokenizer.encode(line)])\n",
    "print(line)\n",
    "print(tokenizer.encode(line))\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     features = bertweet(input_ids)  # Models outputs are now tuples\n",
    "# print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"HTTPURL @MichaelEssien that's a shame wanted to Invest INVEST in you😔😂😂😃 http://www.google.com\"\n",
    "print(emoji.emoji_count(text),\"\\n\")\n",
    "print(text)\n",
    "text = emoji.demojize(text)\n",
    "# print(emoji.get_emoji_regexp(),\"\\n\")\n",
    "# text=text.strip(':')\n",
    "# text = re.sub(r'(@.*?)[\\s]', '@USER ', text)\n",
    "text = re.sub(r\"@\\S+\", \"@USER\", text)   # mention -> '@'\n",
    "text = re.sub(r\"http\\S+\", \"HTTPURL\", text)  # http link -> '*'\n",
    "emojis = re.findall(r'(::)', text)\n",
    "print(emojis)\n",
    "# print(text,\"\\n\")\n",
    "text = re.sub(r':[^:]*:', r' \\g<0>', text)  # http link -> '*'\n",
    "print(text,\"\\n\")\n",
    "\n",
    "text=text.split()\n",
    "# text= re.sub(r'(:[!_\\-\\w]+:)', '', text)\n",
    "print(text,\"\\n\")\n",
    "# emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 추가 데이터들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ext.loc[(all_ext['id'] == int(529657433866915840))].HashTag.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ext.loc[(all_ext['pid'] == int(521367917322338304))][['text','token_for_POS','HashTag','Pronoun','FirstPersonPronoun','SecondPersonPronoun','ThirdPersonPronoun','Numeral','Modal','Whs', 'Noun', 'Verb','Adjective','has_question',\t'has_exclaim',\t'has_period']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ext.loc[((all_ext['Skepticism'] >-1))].groupby('target').mean()  #[['text','URLcount','token_for_POS','id','pid','HashTag','Pronoun','FirstPersonPronoun','SecondPersonPronoun','ThirdPersonPronoun']]\n",
    "all_ext.loc[((all_ext['Skepticism'] >-1))].groupby('target').mean()  #[['text','URLcount','token_for_POS','id','pid','HashTag','Pronoun','FirstPersonPronoun','SecondPersonPronoun','ThirdPersonPronoun']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([ext_thread, ext_y], axis=1).groupby('target').mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ext[['id','text','token_for_POS','HashTag','URLcount','target','Skepticism']].loc[all_ext.HashTag > 4]\n",
    "# all_ext['HashTag'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.max([np.sum(all_ext.loc[(all_ext['id'] == int(childid))]['tweet_created'].values) for childid in structure_ext.loc[0,'0':].dropna()])\n",
    "# np.max([np.sum(all_ext.loc[(all_ext['id'] == childid)]['tweet_created']) for childid in structure_ext.loc[0,'0':'13'].dropna()] - all_ext.loc[(all_ext['id'] == int(521369179392581632))].tweet_created.sum())\n",
    "[np.sum(all_ext.loc[(all_ext['id'] == childid)]['urls_dicts_len']) for childid in structure_ext.loc[11,:].dropna().drop(['depth'],axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thread()를 만드는데 필요한 정보들\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>pid</th>\n      <th>emoji_count</th>\n      <th>has_media</th>\n      <th>URLcount</th>\n      <th>Skepticism</th>\n      <th>MentionCount</th>\n      <th>Noun</th>\n      <th>Verb</th>\n      <th>Adjective</th>\n      <th>Pronoun</th>\n      <th>FirstPersonPronoun</th>\n      <th>SecondPersonPronoun</th>\n      <th>ThirdPersonPronoun</th>\n      <th>Adverb</th>\n      <th>Numeral</th>\n      <th>Conjunction_inj</th>\n      <th>Particle</th>\n      <th>Determiner</th>\n      <th>Modal</th>\n      <th>Whs</th>\n      <th>char_count</th>\n      <th>word_count</th>\n      <th>HashTag</th>\n      <th>has_question</th>\n      <th>has_exclaim</th>\n      <th>has_period</th>\n      <th>retweet_count</th>\n      <th>tweet_count</th>\n      <th>listed_count</th>\n      <th>friends_count</th>\n      <th>follow_ratio</th>\n      <th>account_age_days</th>\n      <th>tweet_created</th>\n      <th>capital_ratio</th>\n      <th>verified</th>\n      <th>isSrcTweet</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1963.00000</td>\n      <td>1478.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n      <td>1963.00000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>548748486071387968.00000</td>\n      <td>547483663616841984.00000</td>\n      <td>0.08304</td>\n      <td>0.11768</td>\n      <td>0.19511</td>\n      <td>3.03311</td>\n      <td>1.26490</td>\n      <td>3.49465</td>\n      <td>2.37341</td>\n      <td>0.89812</td>\n      <td>0.65869</td>\n      <td>0.33724</td>\n      <td>0.11258</td>\n      <td>0.39226</td>\n      <td>0.72033</td>\n      <td>0.12124</td>\n      <td>1.29954</td>\n      <td>0.06928</td>\n      <td>0.78859</td>\n      <td>0.20122</td>\n      <td>0.18136</td>\n      <td>88.74325</td>\n      <td>13.01528</td>\n      <td>0.44320</td>\n      <td>0.18747</td>\n      <td>0.17728</td>\n      <td>0.63169</td>\n      <td>13.43301</td>\n      <td>3.85033</td>\n      <td>1.18417</td>\n      <td>2.67871</td>\n      <td>2.86539</td>\n      <td>1296.51605</td>\n      <td>1419666808.49159</td>\n      <td>0.08952</td>\n      <td>0.06470</td>\n      <td>0.24707</td>\n      <td>0.81915</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>24197288222753516.00000</td>\n      <td>24194908479467996.00000</td>\n      <td>0.52533</td>\n      <td>0.32231</td>\n      <td>0.40779</td>\n      <td>2.08666</td>\n      <td>0.92597</td>\n      <td>2.55108</td>\n      <td>1.83844</td>\n      <td>0.99326</td>\n      <td>0.92956</td>\n      <td>0.72692</td>\n      <td>0.36411</td>\n      <td>0.68698</td>\n      <td>0.98238</td>\n      <td>0.37989</td>\n      <td>1.26827</td>\n      <td>0.26190</td>\n      <td>0.91239</td>\n      <td>0.46241</td>\n      <td>0.44552</td>\n      <td>39.00655</td>\n      <td>6.42202</td>\n      <td>0.98470</td>\n      <td>0.39039</td>\n      <td>0.38200</td>\n      <td>0.48247</td>\n      <td>254.08308</td>\n      <td>0.81881</td>\n      <td>0.92393</td>\n      <td>0.59617</td>\n      <td>0.93745</td>\n      <td>737.28048</td>\n      <td>5769083.07608</td>\n      <td>0.07571</td>\n      <td>0.24605</td>\n      <td>0.43142</td>\n      <td>0.38499</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>521310417696858112.00000</td>\n      <td>521310417696858112.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>8.00000</td>\n      <td>1.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.30103</td>\n      <td>3.00000</td>\n      <td>1413125063.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>529676680548595712.00000</td>\n      <td>529654768249354240.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>1.00000</td>\n      <td>1.00000</td>\n      <td>1.00000</td>\n      <td>1.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>55.00000</td>\n      <td>8.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>3.39872</td>\n      <td>0.47712</td>\n      <td>2.32118</td>\n      <td>2.24674</td>\n      <td>652.00000</td>\n      <td>1415119735.50000</td>\n      <td>0.04167</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>529753935545118720.00000</td>\n      <td>529735657531656192.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>3.00000</td>\n      <td>1.00000</td>\n      <td>3.00000</td>\n      <td>2.00000</td>\n      <td>1.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>1.00000</td>\n      <td>0.00000</td>\n      <td>1.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>90.00000</td>\n      <td>13.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>1.00000</td>\n      <td>0.00000</td>\n      <td>3.94161</td>\n      <td>1.11394</td>\n      <td>2.73957</td>\n      <td>2.82478</td>\n      <td>1313.00000</td>\n      <td>1415138155.00000</td>\n      <td>0.07246</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>576560832456267776.00000</td>\n      <td>576504635738951680.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>4.00000</td>\n      <td>2.00000</td>\n      <td>5.00000</td>\n      <td>4.00000</td>\n      <td>1.00000</td>\n      <td>1.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>1.00000</td>\n      <td>1.00000</td>\n      <td>0.00000</td>\n      <td>2.00000</td>\n      <td>0.00000</td>\n      <td>1.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>128.00000</td>\n      <td>18.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>1.00000</td>\n      <td>2.00000</td>\n      <td>4.43603</td>\n      <td>1.78174</td>\n      <td>3.04513</td>\n      <td>3.39436</td>\n      <td>2031.00000</td>\n      <td>1426297788.50000</td>\n      <td>0.11611</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>579479145828257792.00000</td>\n      <td>577453947599777792.00000</td>\n      <td>10.00000</td>\n      <td>1.00000</td>\n      <td>2.00000</td>\n      <td>12.00000</td>\n      <td>7.00000</td>\n      <td>21.00000</td>\n      <td>10.00000</td>\n      <td>6.00000</td>\n      <td>5.00000</td>\n      <td>7.00000</td>\n      <td>3.00000</td>\n      <td>5.00000</td>\n      <td>6.00000</td>\n      <td>3.00000</td>\n      <td>7.00000</td>\n      <td>2.00000</td>\n      <td>5.00000</td>\n      <td>3.00000</td>\n      <td>3.00000</td>\n      <td>148.00000</td>\n      <td>29.00000</td>\n      <td>7.00000</td>\n      <td>1.00000</td>\n      <td>1.00000</td>\n      <td>1.00000</td>\n      <td>10402.00000</td>\n      <td>5.73594</td>\n      <td>4.15927</td>\n      <td>4.83829</td>\n      <td>6.31487</td>\n      <td>3021.00000</td>\n      <td>1426993569.00000</td>\n      <td>0.76471</td>\n      <td>1.00000</td>\n      <td>1.00000</td>\n      <td>1.00000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                            id                      pid  emoji_count  \\\ncount               1963.00000               1478.00000   1963.00000   \nmean  548748486071387968.00000 547483663616841984.00000      0.08304   \nstd    24197288222753516.00000  24194908479467996.00000      0.52533   \nmin   521310417696858112.00000 521310417696858112.00000      0.00000   \n25%   529676680548595712.00000 529654768249354240.00000      0.00000   \n50%   529753935545118720.00000 529735657531656192.00000      0.00000   \n75%   576560832456267776.00000 576504635738951680.00000      0.00000   \nmax   579479145828257792.00000 577453947599777792.00000     10.00000   \n\n       has_media   URLcount  Skepticism  MentionCount       Noun       Verb  \\\ncount 1963.00000 1963.00000  1963.00000    1963.00000 1963.00000 1963.00000   \nmean     0.11768    0.19511     3.03311       1.26490    3.49465    2.37341   \nstd      0.32231    0.40779     2.08666       0.92597    2.55108    1.83844   \nmin      0.00000    0.00000     0.00000       0.00000    0.00000    0.00000   \n25%      0.00000    0.00000     1.00000       1.00000    1.00000    1.00000   \n50%      0.00000    0.00000     3.00000       1.00000    3.00000    2.00000   \n75%      0.00000    0.00000     4.00000       2.00000    5.00000    4.00000   \nmax      1.00000    2.00000    12.00000       7.00000   21.00000   10.00000   \n\n       Adjective    Pronoun  FirstPersonPronoun  SecondPersonPronoun  \\\ncount 1963.00000 1963.00000          1963.00000           1963.00000   \nmean     0.89812    0.65869             0.33724              0.11258   \nstd      0.99326    0.92956             0.72692              0.36411   \nmin      0.00000    0.00000             0.00000              0.00000   \n25%      0.00000    0.00000             0.00000              0.00000   \n50%      1.00000    0.00000             0.00000              0.00000   \n75%      1.00000    1.00000             0.00000              0.00000   \nmax      6.00000    5.00000             7.00000              3.00000   \n\n       ThirdPersonPronoun     Adverb    Numeral  Conjunction_inj   Particle  \\\ncount          1963.00000 1963.00000 1963.00000       1963.00000 1963.00000   \nmean              0.39226    0.72033    0.12124          1.29954    0.06928   \nstd               0.68698    0.98238    0.37989          1.26827    0.26190   \nmin               0.00000    0.00000    0.00000          0.00000    0.00000   \n25%               0.00000    0.00000    0.00000          0.00000    0.00000   \n50%               0.00000    0.00000    0.00000          1.00000    0.00000   \n75%               1.00000    1.00000    0.00000          2.00000    0.00000   \nmax               5.00000    6.00000    3.00000          7.00000    2.00000   \n\n       Determiner      Modal        Whs  char_count  word_count    HashTag  \\\ncount  1963.00000 1963.00000 1963.00000  1963.00000  1963.00000 1963.00000   \nmean      0.78859    0.20122    0.18136    88.74325    13.01528    0.44320   \nstd       0.91239    0.46241    0.44552    39.00655     6.42202    0.98470   \nmin       0.00000    0.00000    0.00000     8.00000     1.00000    0.00000   \n25%       0.00000    0.00000    0.00000    55.00000     8.00000    0.00000   \n50%       1.00000    0.00000    0.00000    90.00000    13.00000    0.00000   \n75%       1.00000    0.00000    0.00000   128.00000    18.00000    0.00000   \nmax       5.00000    3.00000    3.00000   148.00000    29.00000    7.00000   \n\n       has_question  has_exclaim  has_period  retweet_count  tweet_count  \\\ncount    1963.00000   1963.00000  1963.00000     1963.00000   1963.00000   \nmean        0.18747      0.17728     0.63169       13.43301      3.85033   \nstd         0.39039      0.38200     0.48247      254.08308      0.81881   \nmin         0.00000      0.00000     0.00000        0.00000      0.00000   \n25%         0.00000      0.00000     0.00000        0.00000      3.39872   \n50%         0.00000      0.00000     1.00000        0.00000      3.94161   \n75%         0.00000      0.00000     1.00000        2.00000      4.43603   \nmax         1.00000      1.00000     1.00000    10402.00000      5.73594   \n\n       listed_count  friends_count  follow_ratio  account_age_days  \\\ncount    1963.00000     1963.00000    1963.00000        1963.00000   \nmean        1.18417        2.67871       2.86539        1296.51605   \nstd         0.92393        0.59617       0.93745         737.28048   \nmin         0.00000        0.00000       0.30103           3.00000   \n25%         0.47712        2.32118       2.24674         652.00000   \n50%         1.11394        2.73957       2.82478        1313.00000   \n75%         1.78174        3.04513       3.39436        2031.00000   \nmax         4.15927        4.83829       6.31487        3021.00000   \n\n         tweet_created  capital_ratio   verified  isSrcTweet     target  \ncount       1963.00000     1963.00000 1963.00000  1963.00000 1963.00000  \nmean  1419666808.49159        0.08952    0.06470     0.24707    0.81915  \nstd      5769083.07608        0.07571    0.24605     0.43142    0.38499  \nmin   1413125063.00000        0.00000    0.00000     0.00000    0.00000  \n25%   1415119735.50000        0.04167    0.00000     0.00000    1.00000  \n50%   1415138155.00000        0.07246    0.00000     0.00000    1.00000  \n75%   1426297788.50000        0.11611    0.00000     0.00000    1.00000  \nmax   1426993569.00000        0.76471    1.00000     1.00000    1.00000  "
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ext.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "target\n1    71\n0    26\ndtype: int64"
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ext.loc[all_ext['account_age_days']<100][['account_age_days','verified', 'HashTag', 'URLcount',\n",
    "                                           'MentionCount', 'retweet_count', 'isSrcTweet', 'target']].value_counts('target')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-240-4b19871eb0f9>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-240-4b19871eb0f9>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    all_ext.loc[all_ext['depth']<5][['account_age_days','verified', 'HashTag', 'URLcount',\u001b[0m\n\u001b[0m                                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "all_ext.loc[all_ext['depth']<10][['account_age_days','verified', 'HashTag', 'URLcount',\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>verified</th>\n      <th>HashTag</th>\n      <th>URLcount</th>\n      <th>MentionCount</th>\n      <th>retweet_count</th>\n      <th>isSrcTweet</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>320</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>272</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>442</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1860</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1657</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1177</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>915</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1873</th>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1712</th>\n      <td>0</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>15</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>603</th>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1779</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1528</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>11</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>574</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>280</th>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>659</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>22</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>666</th>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>562</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "      verified  HashTag  URLcount  MentionCount  retweet_count  isSrcTweet  \\\n320          0        1         0             2              1           0   \n272          0        0         0             0              2           1   \n442          1        1         1             0              2           1   \n1860         0        0         0             1              1           0   \n1657         0        0         0             1              8           0   \n1177         0        1         1             1              5           1   \n915          0        0         0             1              1           0   \n1873         0        2         0             1              1           0   \n1712         0        5         1             0             15           1   \n603          0        0         2             0              3           1   \n1779         0        1         2             0              4           1   \n1528         0        0         1             0             11           1   \n574          0        1         1             2              2           1   \n280          0        2         0             0             10           1   \n198          0        0         0             1             13           0   \n659          1        0         0             0             22           1   \n666          0        3         1             2              1           0   \n562          0        0         0             0              2           1   \n\n      target  \n320        1  \n272        1  \n442        1  \n1860       0  \n1657       0  \n1177       1  \n915        1  \n1873       0  \n1712       0  \n603        1  \n1779       0  \n1528       1  \n574        1  \n280        1  \n198        1  \n659        1  \n666        1  \n562        1  "
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ext.loc[(all_ext['retweet_count'] > 0) and (all_ext.pid==521369179392581632)][['verified', 'HashTag', 'URLcount',\n",
    "                                           'MentionCount', 'retweet_count', 'isSrcTweet', 'target']].sample(18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>verified</th>\n      <th>HashTag</th>\n      <th>URLcount</th>\n      <th>MentionCount</th>\n      <th>retweet_count</th>\n    </tr>\n    <tr>\n      <th>target</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.03356</td>\n      <td>1.77852</td>\n      <td>0.65772</td>\n      <td>0.44966</td>\n      <td>12.28859</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.19880</td>\n      <td>0.73695</td>\n      <td>0.73695</td>\n      <td>0.68273</td>\n      <td>49.27309</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "        verified  HashTag  URLcount  MentionCount  retweet_count\ntarget                                                          \n0        0.03356  1.77852   0.65772       0.44966       12.28859\n1        0.19880  0.73695   0.73695       0.68273       49.27309"
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ext.loc[all_ext['retweet_count']>0][['text', 'verified', 'HashTag', 'URLcount', 'MentionCount','retweet_count','target']].groupby('target').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0    521410632953131008.00000\n1    521373142347153408.00000\n2    521369380249432064.00000\n3    521370496928337920.00000\n4    521370224256614400.00000\n5    521370771793670144.00000\n6    521378607231279104.00000\n7    521370530134626304.00000\n8    521373433654157312.00000\n9    521369485144387584.00000\n10   521370061550809088.00000\n11   521369671975858176.00000\n12   521372372927266816.00000\n13   521373960509079552.00000\nName: 0, dtype: float64"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structure_ext.loc[0,'0':].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "15"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# friends_count = [np.sum(all_ext.loc[(all_ext['id'] == int(childid))]['friends_count'].values) for childid in structure_ext.loc[0,'0':].dropna()]\n",
    "len([childid for childid in structure_ext.loc[0,:].dropna()]) \n",
    "\n",
    "# # '''print where 'Sum Friends Count' is 0'''\n",
    "# ext_thread.loc[ext_thread['Sum Friends Count'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Root   521369179392581632.00000\n0      521410632953131008.00000\n1      521373142347153408.00000\n2      521369380249432064.00000\n3      521370496928337920.00000\n4      521370224256614400.00000\n5      521370771793670144.00000\n6      521378607231279104.00000\n7      521370530134626304.00000\n8      521373433654157312.00000\n9      521369485144387584.00000\n10     521370061550809088.00000\n11     521369671975858176.00000\n12     521372372927266816.00000\n13     521373960509079552.00000\nName: 0, dtype: float64"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structure_ext.loc[0].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth : 0.09651135294025893\n",
      "SUM FriendsCount : 0.11708794533602392\n",
      "AVG FriendsCount : 0.06858376543611712\n",
      "AVG WordCount : 0.08943391962695811\n",
      "AVG CharCount : 0.05225660220179892\n",
      "AVG HashTag : -0.2712498153933583\n",
      "SUM HashTag : -0.2020213812062223\n",
      "Ratio HashTag : -0.240456334984137\n",
      "AVG Url : 0.14876200291574\n",
      "STD Url : 0.12458058557305425\n",
      "RATIO Url : 0.1453347220107918\n",
      "SUM Mention : 0.12588775058523002\n",
      "AVG Mention : 0.17327324771229943\n",
      "Ratio Mention : 0.18082808972064832\n",
      "Tweets Count : 0.11451521885513202\n",
      "Ratio Verified : 0.17773330379532\n",
      "SUM Verified : 0.20925620944171405\n",
      "SUM RT : 0.04211967680606234\n",
      "AVG RT : 0.008996997540094446\n",
      "STD RT : 0.04153692828246094\n",
      "AVG AccAge : 0.07005471617303197\n",
      "STD AccAge : 0.04071581547793717\n",
      "thread_time : 0.5331074867901635\n",
      "STD Emoji : 0.10345835221768021\n",
      "AVG Emoji : 0.034777051500645445\n",
      "Ratio Media : -0.23048349004434254\n",
      "RATIO Question : 0.07184087232996553\n",
      "RATIO Exclaim : -0.013189960608181589\n",
      "RATIO Period : -0.028188725777964507\n",
      "AVG FPP : -0.0063552516058029895\n",
      "STD FPP : 0.09478814765368376\n",
      "AVG SPP : -0.028301014299289187\n",
      "STD SPP : 0.008505390026803394\n",
      "AVG TPP : -0.0633077622426266\n",
      "STD TPP : 0.039503662633227604\n",
      "AVG Skepticism : -0.008395034821742257\n"
     ]
    }
   ],
   "source": [
    "for data in ext_thread.columns:\n",
    "    print(data,\":\",ext_thread[data].corr(ext_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (all_ext.loc[(all_ext['pid'] == 521369179392581632)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheme_all = pd.read_csv(\"./data/all/_PHEMEall.csv\")\n",
    "pheme_thread = pd.read_csv(\"./data/all/_PHEME_thread.csv\")\n",
    "ext_all = pd.read_csv(\"./data/all/_PHEMEextall.csv\")\n",
    "ext_thread = pd.read_csv(\"./data/all/_PHEMEext_thread.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 12)\n"
     ]
    }
   ],
   "source": [
    "# print(pheme_all.shape)\n",
    "# print(pheme_thread.shape)\n",
    "# print(ext_all.shape)\n",
    "print(ext_thread.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(X_train, X_test, y_train, y_test, clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    result = clf.predict(X_test)\n",
    "    print(\"Accuracy:\\t\\t\",accuracy_score(y_test,result))\n",
    "    print('Precision Score:\\t', str(precision_score(y_test,result)))\n",
    "    print('Recall Score:\\t\\t' + str(recall_score(y_test,result)))\n",
    "    print(\"F1 Score:\\t\\t\",f1_score(y_test, result, average='macro', zero_division=True))\n",
    "    print(classification_report(y_test, result))\n",
    "    \n",
    "ext_y = pd.read_csv('./data/_PHEMEext_text.csv').target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "ext_thread = ext_thread.replace(-np.inf, 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(ext_thread.iloc[:,:], ext_y, test_size=0.12, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t\t 0.6907216494845361\n",
      "Precision Score:\t 0.6907216494845361\n",
      "Recall Score:\t\t1.0\n",
      "F1 Score:\t\t 0.40853658536585363\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        30\n",
      "           1       0.69      1.00      0.82        67\n",
      "\n",
      "    accuracy                           0.69        97\n",
      "   macro avg       0.35      0.50      0.41        97\n",
      "weighted avg       0.48      0.69      0.56        97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8\n",
    "clf = LogisticRegression()\n",
    "train_test(pd.concat([X_train.iloc[:,0:3], X_train.iloc[:,4:9]], axis=1), pd.concat([X_test.iloc[:,0:3], X_test.iloc[:,4:9]], axis=1), y_train, y_test, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t\t 0.6907216494845361\n",
      "Precision Score:\t 0.6907216494845361\n",
      "Recall Score:\t\t1.0\n",
      "F1 Score:\t\t 0.40853658536585363\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        30\n",
      "           1       0.69      1.00      0.82        67\n",
      "\n",
      "    accuracy                           0.69        97\n",
      "   macro avg       0.35      0.50      0.41        97\n",
      "weighted avg       0.48      0.69      0.56        97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11\n",
    "clf = SVC()\n",
    "train_test(X_train.iloc[:,0:11], X_test.iloc[:,0:11], y_train, y_test, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t\t 0.6907216494845361\n",
      "Precision Score:\t 0.6907216494845361\n",
      "Recall Score:\t\t1.0\n",
      "F1 Score:\t\t 0.40853658536585363\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        30\n",
      "           1       0.69      1.00      0.82        67\n",
      "\n",
      "    accuracy                           0.69        97\n",
      "   macro avg       0.35      0.50      0.41        97\n",
      "weighted avg       0.48      0.69      0.56        97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 12\n",
    "clf = SVC()\n",
    "train_test(X_train.iloc[:,0:12], X_test.iloc[:,0:12], y_train, y_test, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t\t 0.5567010309278351\n",
      "Precision Score:\t 0.7307692307692307\n",
      "Recall Score:\t\t0.5671641791044776\n",
      "F1 Score:\t\t 0.5326610644257703\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.53      0.43        30\n",
      "           1       0.73      0.57      0.64        67\n",
      "\n",
      "    accuracy                           0.56        97\n",
      "   macro avg       0.54      0.55      0.53        97\n",
      "weighted avg       0.61      0.56      0.57        97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 12\n",
    "clf = GaussianNB()\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "train_test(X_train.iloc[:,0:14].drop(['Ratio Verified'],axis=1), X_test.iloc[:,0:14].drop(['Ratio Verified'],axis=1), y_train, y_test, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-248-6db6c01a24c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# X_train = scaler.fit_transform(X_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# X_test = scaler.transform(X_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Ratio Verified'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Ratio Verified'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "# 12\n",
    "clf = GaussianNB()\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "train_test(X_train.iloc[:,0:17].drop(['Ratio Verified'],axis=1), X_test.iloc[:,0:17].drop(['Ratio Verified'],axis=1), y_train, y_test, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t\t 0.7627118644067796\n",
      "Precision Score:\t 0.9032258064516129\n",
      "Recall Score:\t\t0.717948717948718\n",
      "F1 Score:\t\t 0.7541666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.85      0.71        20\n",
      "           1       0.90      0.72      0.80        39\n",
      "\n",
      "    accuracy                           0.76        59\n",
      "   macro avg       0.76      0.78      0.75        59\n",
      "weighted avg       0.80      0.76      0.77        59\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 12\n",
    "clf = GaussianNB()\n",
    "train_test(X_train, X_test, y_train, y_test, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t\t 0.7796610169491526\n",
      "Precision Score:\t 0.76\n",
      "Recall Score:\t\t0.9743589743589743\n",
      "F1 Score:\t\t 0.7028283611003487\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.40      0.55        20\n",
      "           1       0.76      0.97      0.85        39\n",
      "\n",
      "    accuracy                           0.78        59\n",
      "   macro avg       0.82      0.69      0.70        59\n",
      "weighted avg       0.80      0.78      0.75        59\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 12\n",
    "clf = SVC()\n",
    "train_test(X_train, X_test, y_train, y_test, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fetchData import fetchdata \n",
    "import __MLP\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "((426, 36), (59, 36))"
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([426, 1, 36])\n",
      "torch.Size([426, 1])\n",
      "Train Size 426 Test Size 59\n"
     ]
    }
   ],
   "source": [
    "# tensor_x1 = torch.Tensor(pheme_sparse.values).unsqueeze(1)\n",
    "# tensor_x1 = torch.Tensor(X_train.values).unsqueeze(1)\n",
    "tensor_y1 = torch.Tensor(y_train.values).unsqueeze(1)\n",
    "tensor_x1 = torch.Tensor(X_train).unsqueeze(1)\n",
    "train_dataset = TensorDataset(tensor_x1,tensor_y1)\n",
    "\n",
    "# tensor_x2 = torch.Tensor(ext_sparse.values).unsqueeze(1)\n",
    "# tensor_x2 = torch.Tensor(X_test.values).unsqueeze(1)\n",
    "tensor_y2 = torch.Tensor(y_test.values).unsqueeze(1)\n",
    "tensor_x2 = torch.Tensor(X_test).unsqueeze(1)\n",
    "test_dataset = TensorDataset(tensor_x2,tensor_y2)\n",
    "\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 16\n",
    "\n",
    "# Initialize WeightedRandomSampler to deal with the unbalanced dataset\n",
    "counts = np.bincount(y_train.values)\n",
    "labels_weights = 1. / counts\n",
    "weights = labels_weights[y_train.values]\n",
    "train_sampler = WeightedRandomSampler(weights, len(weights))\n",
    "test_sampler = SequentialSampler(tensor_x2)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=2)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "train_size = int(tensor_y1.size(0))\n",
    "test_size = int(tensor_y2.size(0))\n",
    "print(tensor_x1.shape)\n",
    "print(tensor_y1.shape)\n",
    "print(\"Train Size\",train_size,\"Test Size\",test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sparse_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(sparse_model, self).__init__() # 1*20\n",
    "        self.fc1 = nn.Linear(36, 6, bias=True) # 420\n",
    "        self.fc3 = nn.Linear(6, 1)\n",
    "\n",
    "        self.drop_3 = nn.Dropout(0.3)\n",
    "        self.drop_4 = nn.Dropout(0.4)\n",
    "        self.drop_2 = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop_3(F.elu(self.fc1(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sparse = sparse_model()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = optim.SGD(model_sparse.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = optim.Adam(model_sparse.parameters(), lr=5e-5, eps=1e-8, weight_decay=1e-6)\n",
    "# scheduler = lr_scheduler.ExponentialLR(optimizer, gamma= 0.99)  \n",
    "\n",
    "epochs = 100\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,  # Default value\n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "PATH = \"./Model/state_dict_sparse_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_report(train_loss, train_acc, val_loss, val_acc):\n",
    "    fig, ax = plt.subplots(4, 1, figsize=(12,8))\n",
    "    ax[0].plot(train_loss[:])\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_title('Training Loss')\n",
    "\n",
    "    ax[1].plot(train_acc[:])\n",
    "    ax[1].set_ylabel('Classification Accuracy')\n",
    "    ax[1].set_title('Training Accuracy')\n",
    "\n",
    "    ax[2].plot(val_loss[:])\n",
    "    ax[2].set_ylabel('Classification Accuracy')\n",
    "    ax[2].set_title('Testing Loss')\n",
    "\n",
    "    ax[3].plot(val_acc[:])\n",
    "    ax[3].set_ylabel('Classification Accuracy')\n",
    "    ax[3].set_title('Testing Accuracy')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Min of Training Loss: %4f\"%(np.min(train_loss)))\n",
    "    print(\"Max of Training Accuracy: %4f\"%(np.max(train_acc)))\n",
    "    print(\"Mean of Training Loss: %4f\"%(np.mean(train_loss)))\n",
    "    print(\"Mean of Training Accuracy: %4f\"%(np.mean(train_acc)))\n",
    "    print(\"----\")\n",
    "    print(\"Max of Testing Accuracy: %4f\"%(np.max(val_acc)))\n",
    "    print(\"Mean of Testing Loss: %4f\"%(np.mean(val_loss_list)))\n",
    "    print(\"Mean of Testing Accuracy: %4f\"%(np.mean(val_acc)))\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train1(model, num_epochs, criterion, optimizer, scheduler, train_loader, train_size, test_loader=None, test_size=None, patience=5, PATH='./state_dict_model.pt'):\n",
    "    set_seed(42)\n",
    "    train_loss = []\n",
    "    patience_count = 0\n",
    "    train_accuracy = []\n",
    "    prev_loss = 10\n",
    "    best_loss = 10.0\n",
    "    val_corrects_list = []\n",
    "    val_loss_list = []\n",
    "    val_acc_list = []\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        # print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        # print('-' * 10)\n",
    "        running_corrects = 0.0\n",
    "        running_loss = 0.0\n",
    "        model.train()  # Set model to training mode\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.float(), labels.float()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            #  _, predictions = torch.max(outputs.data, 1) won’t work if your output only contains a single output unit.\n",
    "            # _, preds = torch.max(outputs, 1)\n",
    "            # print(outputs.flatten().size())\n",
    "            preds = outputs.squeeze(1) > 0.0\n",
    "\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # step function\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / train_size\n",
    "        # print(running_loss)\n",
    "        # print(train_size)\n",
    "        epoch_acc = running_corrects.double() / train_size\n",
    "        train_loss.append(epoch_loss)\n",
    "        train_accuracy.append(epoch_acc)\n",
    "\n",
    "        if (epoch % 2 == 0):\n",
    "            print('Epoch {}/{}\\tTrain) Acc: {:.4f}, Loss: {:.4f}'.format(epoch,\n",
    "                                                                         num_epochs - 1, epoch_acc, epoch_loss))\n",
    "\n",
    "        if (test_loader != None):\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0.0\n",
    "                val_corrects = 0\n",
    "                val_preds_list = []\n",
    "                val_label_list = []\n",
    "                for j, val in enumerate(test_loader, 0):\n",
    "                    val_x, val_label = val\n",
    "                    val_x, val_label = val_x.float(), val_label.float()\n",
    "                    val_outputs = model(val_x)\n",
    "                    # _, val_preds = torch.max(val_outputs, 1)\n",
    "                    val_preds = val_outputs.squeeze(1) > 0.0\n",
    "\n",
    "                    val_preds_list.append(val_preds)\n",
    "                    val_label_list.append(val_label)\n",
    "                    v_loss = criterion(val_outputs, val_label.unsqueeze(1))\n",
    "                    val_loss += (v_loss.item() * val_x.size(0))\n",
    "                    val_corrects += torch.sum(val_preds == val_label)\n",
    "                    # accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "\n",
    "                if (epoch % 2 == 0):\n",
    "                    val_preds_list = torch.cat(val_preds_list, 0)\n",
    "                    val_label_list = torch.cat(val_label_list, 0)\n",
    "                    # print(\"\\t\\tValidation) Acc: {:.4f} Loss:{:.4f} F1 score: {:4f}\".format(val_corrects/test_size, val_loss/test_size, f1_score(val_label_list,val_preds_list,average='macro')))\n",
    "                    print(\"\\t\\tValidation) Acc: {:.4f} Loss:{:.4f}\".format(\n",
    "                        val_corrects/test_size, val_loss/test_size))\n",
    "            val_corrects_list.append(val_corrects/test_size)\n",
    "            val_loss_list.append(val_loss/test_size)\n",
    "            val_acc = val_corrects.double() / test_size\n",
    "            val_acc_list.append(val_acc)\n",
    "\n",
    "        if epoch_loss < best_loss:\n",
    "            # print(\"prev_loss: {:.5f}\".format(prev_loss))\n",
    "            # print(\"loss: {:.5f}\".format(loss))\n",
    "            print(\n",
    "                \"\\t\\tSaving the best model w/ loss {:.4f}\".format(epoch_loss))\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            best_loss = epoch_loss\n",
    "            patience_count = 0\n",
    "        elif best_loss < epoch_loss:\n",
    "            patience_count += 1\n",
    "        if patience_count >= patience:\n",
    "            print(\"Finishing the Model: Loss is not decreasing...\")\n",
    "            print(train_loss[-6:-1])\n",
    "            return train_accuracy, train_loss, val_acc_list, val_loss_list\n",
    "    return train_accuracy, train_loss, val_acc_list, val_loss_list\n",
    "\n",
    "def train2(model, num_epochs, criterion, optimizer, train_loader, train_size, test_loader=None, test_size=None, patience=5, PATH='./state_dict_model.pt'):\n",
    "    set_seed(42)\n",
    "    train_loss = []\n",
    "    patience_count = 0\n",
    "    train_accuracy = []\n",
    "    prev_loss = 10\n",
    "    best_loss = 10.0\n",
    "    val_corrects_list = []\n",
    "    val_loss_list = []\n",
    "    val_acc_list = []\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        # print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        # print('-' * 10)\n",
    "        running_corrects = 0.0\n",
    "        running_loss = 0.0\n",
    "        model.train()  # Set model to training mode\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.float(), labels.float()\n",
    "            print(inputs.size())\n",
    "            print(labels.size())\n",
    "            print(inputs.flatten())\n",
    "            print(labels.flatten())\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            print(\"outputs:\",outputs.size())\n",
    "            print(\"outputs:\",outputs)\n",
    "            print(\"labels:\",labels.size())\n",
    "            print(\"labels:\",labels.unsqueeze(1).size())\n",
    "\n",
    "            #  _, predictions = torch.max(outputs.data, 1) won’t work if your output only contains a single output unit.\n",
    "            # _, preds = torch.max(outputs, 1)\n",
    "            preds = torch.argmax(outputs, dim=1).flatten()\n",
    "            # print(outputs.flatten().size())\n",
    "            # preds = outputs > 0.0\n",
    "            # labels = labels.view(-1)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # step function\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            # print('running correct')\n",
    "            # print(running_corrects)\n",
    "\n",
    "        epoch_loss = running_loss / train_size\n",
    "        # print(running_loss)\n",
    "        # print(train_size)\n",
    "        epoch_acc = running_corrects.double() / train_size\n",
    "        train_loss.append(epoch_loss)\n",
    "        train_accuracy.append(epoch_acc)\n",
    "\n",
    "        if (epoch % 2 == 0):\n",
    "            print('Epoch {}/{}\\tTrain) Acc: {:.4f}, Loss: {:.4f}'.format(epoch,\n",
    "                                                                         num_epochs - 1, epoch_acc, epoch_loss))\n",
    "\n",
    "        if (test_loader != None):\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0.0\n",
    "                val_corrects = 0\n",
    "                val_preds_list = []\n",
    "                val_label_list = []\n",
    "                for j, val in enumerate(test_loader, 0):\n",
    "                    val_x, val_label = val\n",
    "                    val_x, val_label = val_x.float(), val_label.float()\n",
    "                    val_outputs = model(val_x)\n",
    "                    val_preds = torch.argmax(val_outputs, dim=1).flatten()\n",
    "                    # _, val_preds = torch.max(val_outputs, 1)\n",
    "                    # print(\"val_outputs:\",val_outputs.flatten())\n",
    "                    # val_preds = val_outputs > 0.0\n",
    "                    # print(\"val_preds:\",val_preds)\n",
    "                    val_preds_list.append(val_preds)\n",
    "                    val_label_list.append(val_label)\n",
    "                    v_loss = criterion(val_outputs, val_label.unsqueeze(1))\n",
    "                    val_loss += (v_loss.item() * val_x.size(0))\n",
    "                    val_corrects += torch.sum(val_preds ==\n",
    "                                              val_label.data).double()\n",
    "                if (epoch % 2 == 0):\n",
    "                    val_preds_list = torch.cat(val_preds_list, 0)\n",
    "                    val_label_list = torch.cat(val_label_list, 0)\n",
    "                    # print(\"\\t\\tValidation) Acc: {:.4f} Loss:{:.4f} F1 score: {:4f}\".format(val_corrects/test_size, val_loss/test_size, f1_score(val_label_list,val_preds_list,average='macro')))\n",
    "                    print(\"\\t\\tValidation) Acc: {:.4f} Loss:{:.4f}\".format(\n",
    "                        val_corrects/test_size, val_loss/test_size))\n",
    "            val_corrects_list.append(val_corrects/test_size)\n",
    "            val_loss_list.append(val_loss/test_size)\n",
    "            val_acc = val_corrects.double() / test_size\n",
    "            val_acc_list.append(val_acc)\n",
    "\n",
    "        if epoch_loss < best_loss:\n",
    "            # print(\"prev_loss: {:.5f}\".format(prev_loss))\n",
    "            # print(\"loss: {:.5f}\".format(loss))\n",
    "            print(\n",
    "                \"\\t\\tSaving the best model w/ loss {:.4f}\".format(epoch_loss))\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            best_loss = epoch_loss\n",
    "            patience_count = 0\n",
    "        elif best_loss < epoch_loss:\n",
    "            patience_count += 1\n",
    "        if patience_count >= patience:\n",
    "            print(\"Finishing the Model: Loss is not decreasing...\")\n",
    "            print(train_loss[-6:-1])\n",
    "            return train_accuracy, train_loss, val_acc_list, val_loss_list\n",
    "    return train_accuracy, train_loss, val_acc_list, val_loss_list\n",
    "\n",
    "def predict(model, criterion, val_dataloader, val_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        for j, val in enumerate(val_dataloader, 0):\n",
    "            val_x, val_label = val\n",
    "            val_x, val_label = val_x.float(), val_label.float()\n",
    "            val_outputs = model(val_x)\n",
    "            val_preds = val_outputs.squeeze(1) > 0.0\n",
    "\n",
    "            val_preds_list.append(val_preds)\n",
    "            val_label_list.append(val_label)\n",
    "            v_loss = criterion(val_outputs, val_label.unsqueeze(1))\n",
    "            val_loss += (v_loss.item() * val_x.size(0))\n",
    "            val_corrects += torch.sum(val_preds == val_label)\n",
    "\n",
    "    val_preds_list = torch.cat(val_preds_list, 0)\n",
    "    val_label_list = torch.cat(val_label_list, 0)\n",
    "    val_corrects = val_corrects/val_size\n",
    "    val_loss/test_size\n",
    "    val_acc = val_corrects.double() / val_size\n",
    "    print(\"\\t\\tValidation) Acc: {:.4f} Loss:{:.4f}\".format(\n",
    "        val_corrects/val_size, val_loss/test_size))\n",
    "    # print(\"\\t\\tValidation) Acc: {:.4f} Loss:{:.4f} F1 score: {:4f}\".format(val_corrects/val_size, val_loss/test_size, f1_score(val_label_list,val_preds_list,average='macro')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\tTrain) Acc: 0.5469, Loss: 0.6808\n",
      "\t\tValidation) Acc: 0.6780 Loss:0.6227\n",
      "\t\tSaving the best model w/ loss 0.6808\n",
      "\t\tSaving the best model w/ loss 0.6759\n",
      "Epoch 2/99\tTrain) Acc: 0.5798, Loss: 0.6710\n",
      "\t\tValidation) Acc: 0.6610 Loss:0.6198\n",
      "\t\tSaving the best model w/ loss 0.6710\n",
      "Epoch 4/99\tTrain) Acc: 0.5892, Loss: 0.6615\n",
      "\t\tValidation) Acc: 0.6780 Loss:0.6171\n",
      "\t\tSaving the best model w/ loss 0.6615\n",
      "Epoch 6/99\tTrain) Acc: 0.5845, Loss: 0.6716\n",
      "\t\tValidation) Acc: 0.6780 Loss:0.6147\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "train_acc, train_loss, val_acc, val_loss_list = train1(model=model_sparse, num_epochs=epochs,patience=8, criterion=criterion, optimizer=optimizer, scheduler=scheduler, train_loader=train_dataloader, train_size=train_size, test_loader=test_dataloader, test_size=test_size, PATH=PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 864x576 with 4 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min of Training Loss: 5.763210\n",
      "Max of Training Accuracy: 0.533981\n",
      "Mean of Training Loss: 7.527830\n",
      "Mean of Training Accuracy: 0.490985\n",
      "----\n",
      "Max of Testing Accuracy: 0.465753\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_loss_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-633-4f5d2641781e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m__MLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Code/FYP/code_data/__MLP.py\u001b[0m in \u001b[0;36mclf_report\u001b[0;34m(train_loss, train_acc, val_loss, val_acc)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \"\"\"Set seed for reproducibility.\n\u001b[0;32m---> 54\u001b[0;31m     \"\"\"\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_loss_list' is not defined"
     ]
    }
   ],
   "source": [
    "__MLP.clf_report(train_loss, train_acc, val_loss_list, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rosetta",
   "language": "python",
   "name": "rosetta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}