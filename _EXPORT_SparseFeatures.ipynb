{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행시 등장하는 URL을 클릭하여 허용해주면 인증KEY가 나타난다. 복사하여 URL아래 빈칸에 붙여넣으면 마운트에 성공하게된다.\n",
    "from google.colab import drive\n",
    "drive.mount('./MyDrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd MyDrive/MyDrive/Capstone/code_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/june/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/june/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob2 import glob\n",
    "import json\n",
    "\n",
    "import matplotlib as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import gensim\n",
    "import gensim.models.word2vec as w2v\n",
    "from gensim.test.utils import common_texts\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "\n",
    "from nltk import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', 400)\n",
    "# pd.set_option('display.max_rowwidth', 100)\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmt = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "freqdist = nltk.FreqDist()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "\"\"\" Replaces contractions from a string to their equivalents \"\"\"\n",
    "contraction_patterns = [ (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "                         (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not'), \n",
    "                         (r'i\\'d', 'i would'), (r'I\\'d', 'I would'), (r'he\\'d', 'he would'), (r'she\\'d', 'she would'), (r'they\\'d', 'they would'), (r'we\\'d', 'we would')]\n",
    "def replaceContraction(text):\n",
    "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
    "    for (pattern, repl) in patterns:\n",
    "        (text, count) = re.subn(pattern, repl, text)\n",
    "    return text\n",
    "\n",
    "def capitalratio(tweet_text):\n",
    "    uppers = [l for l in tweet_text if l.isupper()]\n",
    "    capitalratio = len(uppers) / len(tweet_text)\n",
    "    return capitalratio \n",
    "\n",
    "def getTokenization(sent):\n",
    "    tweet_tokens = []\n",
    "    sent = sent.lower()\n",
    "    sent = replaceContraction(sent)\n",
    "\n",
    "    sent = re.sub(r\"http\\S+\", \"*\", sent) # http link -> '*'\n",
    "    # sent = re.sub(r\"@\\S+\", \"@\", sent)   # mention -> '@'\n",
    "    sent = re.sub(r\"@[^\\s]+\", \"@\", sent)   # mention -> '@'\n",
    "    sent = re.sub(r\"(#)(\\S+)\", r'\\1 \\2', sent) \n",
    "\n",
    "    sent = re.sub(r'([^\\s\\w@#\\*]|_)+', '', sent) # Erasing Special Characters\n",
    "    # sent = re.sub('@[^\\s]+','atUser',sent)\n",
    "    # sent = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','url',sent)\n",
    "    # sent = re.sub(r'#([^\\s]+)', r'\\1', sent)\n",
    "\n",
    "\n",
    "    # sent = re.sub('', '', sent.lower())\n",
    "    # sent = [tweet_tokenizer.tokenize(sent)]\n",
    "    sent = tweet_tokenizer.tokenize(sent)\n",
    "    sent = [stemmer.stem(token) for token in sent]\n",
    "    sent = [lmt.lemmatize(token) for token in sent]\n",
    "\n",
    "    temp = [token for token in sent if not token in stop_words]\n",
    "    url=0\n",
    "    for token in temp:\n",
    "        if token.startswith('*'):\n",
    "            url+=1\n",
    "    # tweet_tokens.append([temp])\n",
    "    # tweet_tokens.append(tweet_tokenizer.tokenize(sent))\n",
    "    # df_tokens = pd.DataFrame(tweet_tokens, columns=['token'])\n",
    "    return temp, url\n",
    "\n",
    "def extract_urls(entities_dicts):\n",
    "    if len(entities_dicts) < 1:\n",
    "        return 0\n",
    "    if len(entities_dicts) == 1:\n",
    "        return 1\n",
    "    if len(entities_dicts) == 2:\n",
    "        return 2\n",
    "\n",
    "    # urls = []\n",
    "    # urls_expanded = []\n",
    "\n",
    "    # key = 'url'\n",
    "    # key2 = 'expanded_url'\n",
    "    # # print(len(entities_dict))\n",
    "    # for i in entities_dicts:\n",
    "    #     urls.append(i[key])\n",
    "    #     urls_expanded.append(i[key2])\n",
    "    # return 1, urls, urls_expanded\n",
    "\n",
    "def getposcount(tokens):\n",
    "    postag = []\n",
    "    poscount = {}\n",
    "    poscount['Noun']=0\n",
    "    poscount['Verb']=0\n",
    "    poscount['Adjective'] = 0\n",
    "    poscount['Pronoun']=0\n",
    "    poscount['FirstPersonPronoun']=0\n",
    "    poscount['SecondPersonPronoun']=0\n",
    "    poscount['ThirdPersonPronoun']=0\n",
    "    poscount['Adverb']=0\n",
    "    poscount['Numeral']=0\n",
    "    poscount['Conjunction_inj']=0\n",
    "    poscount['Particle']=0\n",
    "    poscount['Determiner']=0\n",
    "    poscount['Modal']=0\n",
    "    poscount['Whs']=0\n",
    "\n",
    "    Nouns = {'NN','NNS','NNP','NNPS'}\n",
    "    Adverbs = {'RB','RBR','RBS'}\n",
    "    Whs = {'WDT','WP','WRB'} # Composition of wh-determiner(that,what), wh-pronoun(who), wh-adverb(how)\n",
    "    Verbs={'VB','VBP','VBZ','VBN','VBG','VBD','To'}\n",
    "    first_person_pronouns=['i','I','me','my','mine','we','us','our','ours'] #'i',\n",
    "    second_person_pronouns=['you','your','yours', 'ya']\n",
    "    third_person_pronouns=['he','she','it','him','her','it','his','hers','its','they','them','their','theirs']\n",
    "    test_auxiliary=['be','will','have','am','is','was','were','can','could','dare','did','may','might','must','ought','shall','should','would']\n",
    "    test_tentat=['maybe','perhaps','possibly','probably','guess']\n",
    "    test_certain=['always','never', \"can't\", 'cannot']\n",
    "\n",
    "    for word in tokens:\n",
    "        w_lower=word.lower()\n",
    "        if w_lower in first_person_pronouns:\n",
    "            poscount['FirstPersonPronoun']+=1\n",
    "        elif w_lower in second_person_pronouns:\n",
    "            poscount['SecondPersonPronoun']+=1\n",
    "        elif w_lower in third_person_pronouns:\n",
    "            poscount['ThirdPersonPronoun']+=1\n",
    "        \n",
    "    for word in tokens:\n",
    "        w_lower=word.lower()\n",
    "        if w_lower in test_auxiliary:\n",
    "            poscount['test_auxiliary']+=1\n",
    "        elif w_lower in test_tentat:\n",
    "            poscount['test_tentat']+=1\n",
    "        elif w_lower in test_certain:\n",
    "            poscount['test_certain']+=1\n",
    "\n",
    "    postag = nltk.pos_tag(tokens)\n",
    "    for g1 in postag:\n",
    "        if g1[1] in Nouns:\n",
    "            poscount['Noun'] += 1\n",
    "        elif g1[1] in Verbs:\n",
    "            poscount['Verb']+= 1\n",
    "        elif g1[1]=='ADJ'or g1[1]=='JJ':\n",
    "            poscount['Adjective']+=1\n",
    "        elif g1[1]=='PRP' or g1[1]=='PRON' or g1[1]=='PRP$':\n",
    "            poscount['Pronoun']+=1\n",
    "        elif g1[1] in Adverbs or g1[1]=='ADV':\n",
    "            poscount['Adverb']+=1\n",
    "        elif g1[1]=='CD':\n",
    "            poscount['Numeral']+=1\n",
    "        elif g1[1]=='CC' or g1[1]=='IN':\n",
    "            poscount['Conjunction_inj']+=1\n",
    "        elif g1[1]=='RP':\n",
    "            poscount['Particle']+=1\n",
    "        elif g1[1]=='MD':\n",
    "            poscount['Modal']+=1\n",
    "        elif g1[1]=='DT':\n",
    "            poscount['Determiner']+=1\n",
    "        elif g1[1] in Whs:\n",
    "            poscount['Whs']+=1\n",
    "    return poscount\n",
    "\n",
    "def fetchRawText(path, events, tweetType):\n",
    "    jsons = []\n",
    "    for i, event in enumerate(events):\n",
    "        jsons.append(glob('%s/%s/**/%s/*.json' % (path, event,tweetType)))\n",
    "    for i,d in enumerate(jsons): print(\"%s's length is %d\" %(events[i], len(d)))\n",
    "\n",
    "    targets = []\n",
    "    features = []\n",
    "    for index, dataset in enumerate(jsons):\n",
    "        targetEvent = []\n",
    "        dataEvent = []\n",
    "        count = 0  # help var\n",
    "        for jsonFile in dataset:\n",
    "            count += 1\n",
    "            if jsonFile.find(\"non-rumours\") == -1:\n",
    "                targetEvent.append(1)\n",
    "            else:\n",
    "                targetEvent.append(0)\n",
    "\n",
    "            with open(jsonFile, 'r') as f:\n",
    "                for l in f.readlines():\n",
    "                    if not l.strip():  # skip empty lines\n",
    "                        continue\n",
    "                    try:\n",
    "                        json_data = json.loads(l)\n",
    "                    except:\n",
    "                        print (l,\"\\n\\n\")\n",
    "                        break\n",
    "                    dataEvent.append(json_data)\n",
    "        print(index, events[index], len(targetEvent), len(dataEvent))\n",
    "        targets.append(targetEvent)\n",
    "        features.append(dataEvent)\n",
    "\n",
    "    # print(\"\\nNumber of Events:\", len(targets))\n",
    "    # print(\"Number of tweets in the first event:\", len(targets[0]))\n",
    "\n",
    "    # targets은 targetEvent들을 리스트에 담은 것\n",
    "    target_list = []\n",
    "    for event in targets:\n",
    "        for elem in event:\n",
    "            target_list.append(elem)\n",
    "    target = pd.DataFrame(target_list, columns=[\"target\"])\n",
    "\n",
    "    extracted_features = []\n",
    "\n",
    "    extracted = []\n",
    "\n",
    "    for obj_list in features:\n",
    "        extracted_event = []\n",
    "        for obj in obj_list:\n",
    "            output_f = dict()\n",
    "            output_f['text'] = obj['text']\n",
    "            urls_dicts = obj['entities']['urls']\n",
    "            # output_f['URLcount'] = extract_urls(urls_dicts)\n",
    "\n",
    "            # NEW Features from fetchRawText_all\n",
    "            output_f['emoji_count'] = emoji.emoji_count(obj['text'])\n",
    "            urls_dicts = obj['entities']['urls']\n",
    "            output_f['URLcount'] = len(urls_dicts)\n",
    "            if \"media\" in obj['entities']:\n",
    "                output_f['has_media'] = len(obj['entities']['media'])\n",
    "                # output_f['media_type'] = obj['entities']['media'][0]['type']\n",
    "            else:\n",
    "                output_f['has_media'] = 0\n",
    "            temp = re.sub(r\"http\\S+\", \"HTTPURL\", obj['text'])\n",
    "            verification = 0\n",
    "            verification += len(re.findall(r'is(that|this|it) true', obj['text']))\n",
    "            verification += len(re.findall(r'wh[a]*t[?!|!?][?!|!?]*', obj['text']))\n",
    "            verification += len(re.findall(r'(rumour|rumor|debunk)', obj['text']))\n",
    "            verification += len(re.findall(r'(real?|really?|uncomfirmed)', obj['text']))\n",
    "            verification += len(re.findall(r'(that|this|it) is not true', obj['text']))\n",
    "            verification += len(re.findall(r'(that|this|it) is false', obj['text']))\n",
    "            verification += len(re.findall(r'(h[m]*)', obj['text']))\n",
    "            output_f['Skepticism'] = verification    \n",
    "            mention = 0\n",
    "            for token in temp:\n",
    "                if token.startswith('@'):\n",
    "                    mention+=1 \n",
    "            output_f['MentionCount'] = mention      \n",
    "\n",
    "            output_f['text_token'], output_f['URLcount'] = getTokenization(obj['text'])\n",
    "            '''POS Tagging and text cleansing for POS'''\n",
    "            # temp = output_f['text']\n",
    "            # temp=  emoji.demojize(temp)\n",
    "            # temp = re.sub(r\"(#)(\\S+)\", r'\\1 \\2', temp)\n",
    "            # temp = re.sub(r\"http\\S+\", \"\", temp)\n",
    "            # temp = replaceContraction(temp.lower())\n",
    "            # temp = temp.split()\n",
    "            # pos_dict=getposcount(temp)\n",
    "\n",
    "            '''POS Tagging'''\n",
    "            temp = output_f['text']\n",
    "            temp = replaceContraction(temp.lower())\n",
    "            temp = re.sub(r\"(#)(\\S+)\", '', temp)\n",
    "            temp = re.sub(r\"(@)(\\S+)\", '', temp)\n",
    "            temp = re.sub(r\"http\\S+\", \"\", temp)\n",
    "            temp = re.sub(r'([^\\s\\w#\\*]|_)+', '', temp) # Erasing Special Characters\n",
    "            temp = temp.split()\n",
    "            pos_dict=getposcount_all(temp)\n",
    "            output_f.update(pos_dict)\n",
    "\n",
    "            output_f['char_count'] = len(output_f['text'])\n",
    "            output_f['word_count'] = len(output_f['text'].split())\n",
    "\n",
    "            output_f['HashTag'] = len(obj['entities']['hashtags'])\n",
    "\n",
    "            output_f['has_question'] = \"?\" in output_f[\"text\"]\n",
    "            output_f['has_exclaim'] = \"!\" in output_f[\"text\"]\n",
    "            output_f['has_period'] = \".\" in output_f[\"text\"]\n",
    "\n",
    "            output_f['capital_ratio']=(capitalratio(obj['text']))\n",
    "            output_f['retweet_count'] = obj['retweet_count']\n",
    "            # output_f['isRT'] = obj['retweeted']\n",
    "\n",
    "            output_f['tweet_count'] = obj['user']['statuses_count']\n",
    "            output_f['listed_count'] = obj['user']['listed_count']\n",
    "            output_f['friends_count'] = obj['user']['friends_count']\n",
    "            output_f['follower_count'] = obj['user']['followers_count']\n",
    "\n",
    "            if (output_f['friends_count'] <= 0):\n",
    "                output_f['followers/friend'] = obj['user']['followers_count']\n",
    "            else:\n",
    "                output_f['followers/friend'] = obj['user']['followers_count']/obj['user']['friends_count']\n",
    "\n",
    "            output_f['favourites_count'] = obj['user']['favourites_count']\n",
    "\n",
    "            acc_created = datetime.strptime(obj['user']['created_at'], '%a %b %d %H:%M:%S %z %Y')\n",
    "            tweet_created = datetime.strptime(obj['created_at'], '%a %b %d %H:%M:%S %z %Y')\n",
    "            age = (tweet_created - acc_created)\n",
    "            output_f['account_age_days'] = age.days\n",
    "            \n",
    "            output_f['capital_ratio']=(capitalratio(obj['text']))\n",
    "            output_f['verified'] = obj['user']['verified']\n",
    "\n",
    "            extracted_event.append(output_f)\n",
    "        extracted_features.append(extracted_event)\n",
    "\n",
    "    extracted_df = []\n",
    "    for i, data in enumerate(extracted_features):\n",
    "        temp = pd.DataFrame(data)\n",
    "        temp[\"Event\"] = events[i]\n",
    "        extracted_df.append(pd.DataFrame(temp))\n",
    "\n",
    "    final = pd.concat(extracted_df, ignore_index=True)\n",
    "    final = pd.concat([final, target], axis=1)\n",
    "    return final\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getposcount_all(tokens):\n",
    "    postag = []\n",
    "    poscount = {}\n",
    "\n",
    "    poscount['FirstPersonPronoun']=0\n",
    "    poscount['SecondPersonPronoun']=0\n",
    "    poscount['ThirdPersonPronoun']=0\n",
    "    poscount['test_auxiliary']=0\n",
    "    poscount['test_tentat']=0\n",
    "    poscount['test_certain']=0\n",
    "    poscount['Numeral']=0\n",
    "\n",
    "\n",
    "    first_person_pronouns=['i','me','my','mine','we','us','our','ours'] #'i',\n",
    "    second_person_pronouns=['you','your','yours', 'ya']\n",
    "    third_person_pronouns=['he','she','it','him','her','it','his','hers','its','they','them','their','theirs']\n",
    "    test_auxiliary=['be','will','have','am','is','was','were','can','could','dare','did','may','might','must','ought','shall','should','would']\n",
    "    test_tentat=['maybe','perhaps','possibly','probably','guess']\n",
    "    test_certain=['always','never']\n",
    "\n",
    "    for word in tokens:\n",
    "        w_lower=word.lower()\n",
    "        if w_lower in first_person_pronouns:\n",
    "            poscount['FirstPersonPronoun']+=1\n",
    "        elif w_lower in second_person_pronouns:\n",
    "            poscount['SecondPersonPronoun']+=1\n",
    "        elif w_lower in third_person_pronouns:\n",
    "            poscount['ThirdPersonPronoun']+=1\n",
    "        \n",
    "    for word in tokens:\n",
    "        w_lower=word.lower()\n",
    "        if w_lower in test_auxiliary:\n",
    "            poscount['test_auxiliary']+=1\n",
    "        elif w_lower in test_tentat:\n",
    "            poscount['test_tentat']+=1\n",
    "        elif w_lower in test_certain:\n",
    "            poscount['test_certain']+=1\n",
    "\n",
    "    postag = nltk.pos_tag(tokens)\n",
    "    for g1 in postag:\n",
    "        if g1[1]=='CD':\n",
    "            poscount['Numeral']+=1\n",
    "    return poscount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHEME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "charliehebdo's length is 2079\n",
      "ferguson's length is 1143\n",
      "germanwings-crash's length is 469\n",
      "ottawashooting's length is 890\n",
      "sydneysiege's length is 1221\n",
      "0 charliehebdo 2079 2079\n",
      "1 ferguson 1143 1143\n",
      "2 germanwings-crash 469 469\n",
      "3 ottawashooting 890 890\n",
      "4 sydneysiege 1221 1221\n"
     ]
    }
   ],
   "source": [
    "path = \"../pheme-rnr-dataset\"\n",
    "events = ['charliehebdo', 'ferguson',\n",
    "          'germanwings-crash', 'ottawashooting', 'sydneysiege']\n",
    "tweetType = 'source-tweet'\n",
    "jsons = []\n",
    "final = fetchRawText(path, events, tweetType)\n",
    "target = final.target\n",
    "final.verified = final.verified.replace({True: 1, False: 0}) \n",
    "final.has_question = final.has_question.replace({True: 1, False: 0}) \n",
    "final.has_exclaim = final.has_exclaim.replace({True: 1, False: 0}) \n",
    "final.has_period = final.has_period.replace({True: 1, False: 0}) \n",
    "# final.isRT = final.isRT.replace({True: 1, False: 0}) \n",
    "\n",
    "final = final.replace(-np.inf, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>emoji_count</th>\n      <th>URLcount</th>\n      <th>has_media</th>\n      <th>Skepticism</th>\n      <th>MentionCount</th>\n      <th>FirstPersonPronoun</th>\n      <th>SecondPersonPronoun</th>\n      <th>ThirdPersonPronoun</th>\n      <th>test_auxiliary</th>\n      <th>test_tentat</th>\n      <th>test_certain</th>\n      <th>Numeral</th>\n      <th>char_count</th>\n      <th>word_count</th>\n      <th>HashTag</th>\n      <th>has_question</th>\n      <th>has_exclaim</th>\n      <th>has_period</th>\n      <th>capital_ratio</th>\n      <th>retweet_count</th>\n      <th>tweet_count</th>\n      <th>listed_count</th>\n      <th>friends_count</th>\n      <th>follower_count</th>\n      <th>followers/friend</th>\n      <th>favourites_count</th>\n      <th>account_age_days</th>\n      <th>verified</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>88</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.159091</td>\n      <td>177</td>\n      <td>63575</td>\n      <td>7177</td>\n      <td>614</td>\n      <td>193798</td>\n      <td>315.631922</td>\n      <td>1.886491</td>\n      <td>2126</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>53</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.037736</td>\n      <td>134</td>\n      <td>1076</td>\n      <td>140</td>\n      <td>375</td>\n      <td>4709</td>\n      <td>12.557333</td>\n      <td>-10.000000</td>\n      <td>1050</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>136</td>\n      <td>18</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.073529</td>\n      <td>148</td>\n      <td>7182</td>\n      <td>758</td>\n      <td>592</td>\n      <td>20401</td>\n      <td>34.461149</td>\n      <td>2.173186</td>\n      <td>2030</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>138</td>\n      <td>16</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.101449</td>\n      <td>684</td>\n      <td>54427</td>\n      <td>102287</td>\n      <td>1038</td>\n      <td>15405096</td>\n      <td>14841.132948</td>\n      <td>2.952792</td>\n      <td>2891</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>117</td>\n      <td>13</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.145299</td>\n      <td>113</td>\n      <td>104998</td>\n      <td>13583</td>\n      <td>460</td>\n      <td>842236</td>\n      <td>1830.947826</td>\n      <td>1.414973</td>\n      <td>1975</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   emoji_count  URLcount  has_media  Skepticism  MentionCount  \\\n0            0         1          0           4             0   \n1            0         0          0           1             0   \n2            0         0          0           3             0   \n3            0         2          0           8             0   \n4            0         2          1           3             0   \n\n   FirstPersonPronoun  SecondPersonPronoun  ThirdPersonPronoun  \\\n0                   0                    0                   0   \n1                   0                    0                   0   \n2                   0                    0                   0   \n3                   0                    0                   0   \n4                   0                    0                   0   \n\n   test_auxiliary  test_tentat  test_certain  Numeral  char_count  word_count  \\\n0               0            0             0        0          88          12   \n1               0            0             0        0          53           6   \n2               1            0             0        0         136          18   \n3               2            0             0        0         138          16   \n4               0            0             0        0         117          13   \n\n   HashTag  has_question  has_exclaim  has_period  capital_ratio  \\\n0        0             0            0           1       0.159091   \n1        1             0            0           1       0.037736   \n2        2             0            0           1       0.073529   \n3        1             0            0           1       0.101449   \n4        1             0            0           1       0.145299   \n\n   retweet_count  tweet_count  listed_count  friends_count  follower_count  \\\n0            177        63575          7177            614          193798   \n1            134         1076           140            375            4709   \n2            148         7182           758            592           20401   \n3            684        54427        102287           1038        15405096   \n4            113       104998         13583            460          842236   \n\n   followers/friend  favourites_count  account_age_days  verified  \n0        315.631922          1.886491              2126         1  \n1         12.557333        -10.000000              1050         0  \n2         34.461149          2.173186              2030         0  \n3      14841.132948          2.952792              2891         1  \n4       1830.947826          1.414973              1975         1  "
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.drop(['text_token','text','Event','target'], axis=1, inplace=True)\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('./data/_PHEME_sparse.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHEME (Extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ebola-essien's length is 14\n",
      "prince-toronto's length is 233\n",
      "putinmissing's length is 238\n",
      "0 ebola-essien 14 14\n",
      "1 prince-toronto 233 233\n",
      "2 putinmissing 238 238\n"
     ]
    }
   ],
   "source": [
    "path = \"../PHEME/all-rnr-annotated-threads\"\n",
    "events = ['ebola-essien', 'prince-toronto', 'putinmissing']\n",
    "tweetType = 'source-tweets'\n",
    "jsons = []\n",
    "final_ext = fetchRawText(path,events,tweetType)\n",
    "ext_target = final_ext.target\n",
    "final_ext.verified = final_ext.verified.replace({True: 1, False: 0}) \n",
    "final_ext.has_question = final_ext.has_question.replace({True: 1, False: 0}) \n",
    "final_ext.has_exclaim = final_ext.has_exclaim.replace({True: 1, False: 0}) \n",
    "final_ext.has_period = final_ext.has_period.replace({True: 1, False: 0}) \n",
    "# final_ext.isRT = final_ext.isRT.replace({True: 1, False: 0}) \n",
    "final_ext = final_ext.replace(-np.inf, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'emoji_count', 'URLcount', 'has_media', 'Skepticism',\n",
      "       'MentionCount', 'text_token', 'FirstPersonPronoun',\n",
      "       'SecondPersonPronoun', 'ThirdPersonPronoun', 'test_auxiliary',\n",
      "       'test_tentat', 'test_certain', 'Numeral', 'char_count', 'word_count',\n",
      "       'HashTag', 'has_question', 'has_exclaim', 'has_period', 'capital_ratio',\n",
      "       'retweet_count', 'tweet_count', 'listed_count', 'friends_count',\n",
      "       'follower_count', 'followers/friend', 'favourites_count',\n",
      "       'account_age_days', 'verified', 'Event', 'target'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>emoji_count</th>\n      <th>URLcount</th>\n      <th>has_media</th>\n      <th>Skepticism</th>\n      <th>MentionCount</th>\n      <th>FirstPersonPronoun</th>\n      <th>SecondPersonPronoun</th>\n      <th>ThirdPersonPronoun</th>\n      <th>test_auxiliary</th>\n      <th>test_tentat</th>\n      <th>test_certain</th>\n      <th>Numeral</th>\n      <th>char_count</th>\n      <th>word_count</th>\n      <th>HashTag</th>\n      <th>has_question</th>\n      <th>has_exclaim</th>\n      <th>has_period</th>\n      <th>capital_ratio</th>\n      <th>retweet_count</th>\n      <th>tweet_count</th>\n      <th>listed_count</th>\n      <th>friends_count</th>\n      <th>follower_count</th>\n      <th>followers/friend</th>\n      <th>favourites_count</th>\n      <th>account_age_days</th>\n      <th>verified</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>69</td>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.101449</td>\n      <td>117</td>\n      <td>40676</td>\n      <td>148</td>\n      <td>652</td>\n      <td>21833</td>\n      <td>33.486196</td>\n      <td>3.984527</td>\n      <td>1570</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>148</td>\n      <td>25</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.027027</td>\n      <td>10402</td>\n      <td>509</td>\n      <td>1623</td>\n      <td>176</td>\n      <td>488496</td>\n      <td>2775.545455</td>\n      <td>0.845098</td>\n      <td>579</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>119</td>\n      <td>20</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.025210</td>\n      <td>126</td>\n      <td>83232</td>\n      <td>2165</td>\n      <td>144</td>\n      <td>232347</td>\n      <td>1613.520833</td>\n      <td>2.276462</td>\n      <td>2042</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>130</td>\n      <td>16</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.107692</td>\n      <td>192</td>\n      <td>15448</td>\n      <td>608</td>\n      <td>716</td>\n      <td>73548</td>\n      <td>102.720670</td>\n      <td>2.004321</td>\n      <td>468</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>120</td>\n      <td>15</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.066667</td>\n      <td>196</td>\n      <td>83232</td>\n      <td>2165</td>\n      <td>144</td>\n      <td>232347</td>\n      <td>1613.520833</td>\n      <td>2.276462</td>\n      <td>2039</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   emoji_count  URLcount  has_media  Skepticism  MentionCount  \\\n0            0         1          0           5             0   \n1            0         0          0           4             0   \n2            0         0          0           4             0   \n3            0         2          1           9             0   \n4            0         1          0           6             1   \n\n   FirstPersonPronoun  SecondPersonPronoun  ThirdPersonPronoun  \\\n0                   0                    0                   0   \n1                   2                    0                   0   \n2                   0                    0                   1   \n3                   0                    0                   0   \n4                   0                    0                   0   \n\n   test_auxiliary  test_tentat  test_certain  Numeral  char_count  word_count  \\\n0               0            0             0        0          69           8   \n1               4            0             0        0         148          25   \n2               0            0             0        0         119          20   \n3               0            0             0        0         130          16   \n4               1            0             0        0         120          15   \n\n   HashTag  has_question  has_exclaim  has_period  capital_ratio  \\\n0        0             0            0           1       0.101449   \n1        1             0            0           1       0.027027   \n2        0             0            0           1       0.025210   \n3        0             0            0           1       0.107692   \n4        0             0            0           1       0.066667   \n\n   retweet_count  tweet_count  listed_count  friends_count  follower_count  \\\n0            117        40676           148            652           21833   \n1          10402          509          1623            176          488496   \n2            126        83232          2165            144          232347   \n3            192        15448           608            716           73548   \n4            196        83232          2165            144          232347   \n\n   followers/friend  favourites_count  account_age_days  verified  \n0         33.486196          3.984527              1570         0  \n1       2775.545455          0.845098               579         1  \n2       1613.520833          2.276462              2042         0  \n3        102.720670          2.004321               468         1  \n4       1613.520833          2.276462              2039         0  "
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(final_ext.columns)\n",
    "final_ext.drop(['text_token','text','Event','target'], axis=1, inplace=True)\n",
    "final_ext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ext.to_csv('./data/_PHEMEext_sparse.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_imp(X, y):\n",
    "    forest = ExtraTreesClassifier(n_estimators=250,\n",
    "                                random_state=3)\n",
    "\n",
    "    forest.fit(X, y)\n",
    "    importances = forest.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "                axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(X.shape[1]):\n",
    "        print(\"%d. feature %d: %s (%f)\" % (f + 1, indices[f], X.columns[indices[f]], importances[indices[f]]))\n",
    "\n",
    "    # Plot the impurity-based feature importances of the forest\n",
    "    # plt.figure()\n",
    "    # plt.title(\"Feature importances\")\n",
    "    # plt.bar(range(X.shape[1]), importances[indices],\n",
    "    #         color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    # plt.xticks(range(X.shape[1]), indices)\n",
    "    # plt.xlim([-1, X.shape[1]])\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pheme_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-db1c180b7552>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf_imp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_ext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpheme_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pheme_y' is not defined"
     ]
    }
   ],
   "source": [
    "f_imp(final_ext, pheme_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rosetta",
   "language": "python",
   "name": "rosetta"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}