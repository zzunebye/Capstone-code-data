{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행시 등장하는 URL을 클릭하여 허용해주면 인증KEY가 나타난다. 복사하여 URL아래 빈칸에 붙여넣으면 마운트에 성공하게된다.\n",
    "from google.colab import drive\n",
    "drive.mount('./MyDrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd MyDrive/MyDrive/Capstone/code_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/june/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/june/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob2 import glob\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "from nltk.corpus import stopwords as stpdfa\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import gensim\n",
    "import gensim.models.word2vec as w2v\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "\n",
    "from nltk import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 400)\n",
    "# pd.set_option('display.max_rowwidth', 100)\n",
    "pd.set_option('display.max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmt = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "freqdist = nltk.FreqDist()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "\"\"\" Replaces contractions from a string to their equivalents \"\"\"\n",
    "contraction_patterns = [ (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "                         (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not'), \n",
    "                         (r'i\\'d', 'i would'), (r'I\\'d', 'I would'), (r'he\\'d', 'he would'), (r'she\\'d', 'she would'), (r'they\\'d', 'I would'), (r'we\\'d', 'we would')]\n",
    "def replaceContraction(text):\n",
    "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
    "    for (pattern, repl) in patterns:\n",
    "        (text, count) = re.subn(pattern, repl, text)\n",
    "    return text\n",
    "\n",
    "def capitalratio(tweet_text):\n",
    "    uppers = [l for l in tweet_text if l.isupper()]\n",
    "    capitalratio = len(uppers) / len(tweet_text)\n",
    "    return capitalratio \n",
    "\n",
    "def getTokenization(sent):\n",
    "    tweet_tokens = []\n",
    "    sent = sent.lower()\n",
    "    sent = replaceContraction(sent)\n",
    "\n",
    "    sent = re.sub(r\"http\\S+\", \"*\", sent) # http link -> '*'\n",
    "    # sent = re.sub(r\"@\\S+\", \"@\", sent)   # mention -> '@'\n",
    "    sent = re.sub(r\"@[^\\s]+\", \"@\", sent)   # mention -> '@'\n",
    "    sent = re.sub(r\"(#)(\\S+)\", r'\\1 \\2', sent) \n",
    "\n",
    "    sent = re.sub(r'([^\\s\\w@#\\*]|_)+', '', sent) # Erasing Special Characters\n",
    "    # sent = re.sub('@[^\\s]+','atUser',sent)\n",
    "    # sent = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','url',sent)\n",
    "    # sent = re.sub(r'#([^\\s]+)', r'\\1', sent)\n",
    "\n",
    "\n",
    "    # sent = re.sub('', '', sent.lower())\n",
    "    # sent = [tweet_tokenizer.tokenize(sent)]\n",
    "    sent = tweet_tokenizer.tokenize(sent)\n",
    "    sent = [stemmer.stem(token) for token in sent]\n",
    "    sent = [lmt.lemmatize(token) for token in sent]\n",
    "\n",
    "    temp = [token for token in sent if not token in stop_words]\n",
    "    url=0\n",
    "    for token in temp:\n",
    "        if token.startswith('*'):\n",
    "            url+=1\n",
    "    # tweet_tokens.append([temp])\n",
    "    # tweet_tokens.append(tweet_tokenizer.tokenize(sent))\n",
    "    # df_tokens = pd.DataFrame(tweet_tokens, columns=['token'])\n",
    "    return temp, url\n",
    "\n",
    "def extract_urls(entities_dicts):\n",
    "    if len(entities_dicts) < 1:\n",
    "        return 0\n",
    "    if len(entities_dicts) == 1:\n",
    "        return 1\n",
    "    if len(entities_dicts) == 2:\n",
    "        return 2\n",
    "\n",
    "    # urls = []\n",
    "    # urls_expanded = []\n",
    "\n",
    "    # key = 'url'\n",
    "    # key2 = 'expanded_url'\n",
    "    # # print(len(entities_dict))\n",
    "    # for i in entities_dicts:\n",
    "    #     urls.append(i[key])\n",
    "    #     urls_expanded.append(i[key2])\n",
    "    # return 1, urls, urls_expanded\n",
    "\n",
    "def getposcount(tokens):\n",
    "    postag = []\n",
    "    poscount = {}\n",
    "    poscount['Noun']=0\n",
    "    poscount['Verb']=0\n",
    "    poscount['Adjective'] = 0\n",
    "    poscount['Pronoun']=0\n",
    "    poscount['FirstPersonPronoun']=0\n",
    "    poscount['SecondPersonPronoun']=0\n",
    "    poscount['ThirdPersonPronoun']=0\n",
    "    poscount['Adverb']=0\n",
    "    poscount['Numeral']=0\n",
    "    poscount['Conjunction_inj']=0\n",
    "    poscount['Particle']=0\n",
    "    poscount['Determiner']=0\n",
    "    poscount['Modal']=0\n",
    "    poscount['Whs']=0\n",
    "    poscount['HashTag'] = 0\n",
    "    Nouns = {'NN','NNS','NNP','NNPS'}\n",
    "    Adverbs = {'RB','RBR','RBS'}\n",
    "    Whs = {'WDT','WP','WRB'} # Composition of wh-determiner(that,what), wh-pronoun(who), wh-adverb(how)\n",
    "    Verbs={'VB','VBP','VBZ','VBN','VBG','VBD','To'}\n",
    "    first_person_pronouns=['i','I','me','my','mine','we','us','our','ours'] #'i',\n",
    "    second_person_pronouns=['you','your','yours']\n",
    "    third_person_pronouns=['he','she','it','him','her','it','his','hers','its','they','them','their','theirs']\n",
    "\n",
    "    for word in tokens:\n",
    "        w_lower=word.lower()\n",
    "        if w_lower in first_person_pronouns:\n",
    "            poscount['FirstPersonPronoun']+=1\n",
    "        elif w_lower in second_person_pronouns:\n",
    "            poscount['SecondPersonPronoun']+=1\n",
    "        elif w_lower in third_person_pronouns:\n",
    "            poscount['ThirdPersonPronoun']+=1\n",
    "    \n",
    "    postag = nltk.pos_tag(tokens)\n",
    "    for g1 in postag:\n",
    "        if g1[1] == '#':\n",
    "            poscount['HashTag'] += 1\n",
    "        elif g1[1] in Nouns:\n",
    "            poscount['Noun'] += 1\n",
    "        elif g1[1] in Verbs:\n",
    "            poscount['Verb']+= 1\n",
    "        elif g1[1]=='ADJ'or g1[1]=='JJ':\n",
    "            poscount['Adjective']+=1\n",
    "        elif g1[1]=='PRP' or g1[1]=='PRON' or g1[1]=='PRP$':\n",
    "            poscount['Pronoun']+=1\n",
    "        elif g1[1] in Adverbs or g1[1]=='ADV':\n",
    "            poscount['Adverb']+=1\n",
    "        elif g1[1]=='CD':\n",
    "            poscount['Numeral']+=1\n",
    "        elif g1[1]=='CC' or g1[1]=='IN':\n",
    "            poscount['Conjunction_inj']+=1\n",
    "        elif g1[1]=='RP':\n",
    "            poscount['Particle']+=1\n",
    "        elif g1[1]=='MD':\n",
    "            poscount['Modal']+=1\n",
    "        elif g1[1]=='DT':\n",
    "            poscount['Determiner']+=1\n",
    "        elif g1[1] in Whs:\n",
    "            poscount['Whs']+=1\n",
    "    return poscount\n",
    "\n",
    "def fetchRawText(path, events, tweetType):\n",
    "    jsons = []\n",
    "    for i, event in enumerate(events):\n",
    "        jsons.append(glob('%s/%s/**/%s/*.json' % (path, event,tweetType)))\n",
    "    for i,d in enumerate(jsons): print(\"%s's length is %d\" %(events[i], len(d)))\n",
    "\n",
    "    targets = []\n",
    "    features = []\n",
    "    for index, dataset in enumerate(jsons):\n",
    "        targetEvent = []\n",
    "        dataEvent = []\n",
    "        count = 0  # help var\n",
    "        for jsonFile in dataset:\n",
    "            count += 1\n",
    "            if jsonFile.find(\"non-rumours\") == -1:\n",
    "                targetEvent.append(1)\n",
    "            else:\n",
    "                targetEvent.append(0)\n",
    "\n",
    "            with open(jsonFile, 'r') as f:\n",
    "                for l in f.readlines():\n",
    "                    if not l.strip():  # skip empty lines\n",
    "                        continue\n",
    "                    json_data = json.loads(l)\n",
    "                    # print (json_data,\"\\n\\n\")\n",
    "                    dataEvent.append(json_data)\n",
    "        print(index, events[index], len(targetEvent), len(dataEvent))\n",
    "        targets.append(targetEvent)\n",
    "        features.append(dataEvent)\n",
    "\n",
    "    # print(\"\\nNumber of Events:\", len(targets))\n",
    "    # print(\"Number of tweets in the first event:\", len(targets[0]))\n",
    "\n",
    "    # targets은 targetEvent들을 리스트에 담은 것\n",
    "    target_list = []\n",
    "    for event in targets:\n",
    "        for elem in event:\n",
    "            target_list.append(elem)\n",
    "    target = pd.DataFrame(target_list, columns=[\"target\"])\n",
    "\n",
    "    extracted_features = []\n",
    "\n",
    "    extracted = []\n",
    "\n",
    "    for obj_list in features:\n",
    "        extracted_event = []\n",
    "        for obj in obj_list:\n",
    "            output_f = dict()\n",
    "            output_f['text'] = obj['text']\n",
    "            urls_dicts = obj['entities']['urls']\n",
    "            output_f['URLcount'] = extract_urls(urls_dicts)\n",
    "        \n",
    "            # print(type(obj['user']))\n",
    "            # print(obj['user'].contains_key('entities'))\n",
    "            # if ('url' in obj['user']):\n",
    "            #     output_f['hasUserURL'] = 1\n",
    "            #     output_f['user_url'] = 1 if (obj['user']['url'] != None) else 0\n",
    "            # elif ('entities' in obj['user']):\n",
    "            #     output_f['user_entity'] = obj['user']['entities']['url']['urls']\n",
    "            #     # print(obj['user']['entities']['url']['urls'])\n",
    "            #     output_f['user_url'] = obj['user']['entities']['expanded_url']\n",
    "            #     output_f['hasUserURL'] , _ , output_f['user_url'] = extract_urls(obj['user']['entities']['url']['urls'])\n",
    "            # else:\n",
    "            #     # output_f['user_entity'] = None\n",
    "            #     output_f['user_url'] = 0\n",
    "            #     output_f['hasUserURL'] = 0\n",
    "            \n",
    "\n",
    "            output_f['text_token'], output_f['URLcount'] = getTokenization(obj['text'])\n",
    "\n",
    "            '''POS Tagging'''\n",
    "            temp = output_f['text']\n",
    "            temp = re.sub(r\"(#)(\\S+)\", r'\\1 \\2', temp)\n",
    "            temp = re.sub(r\"http\\S+\", \"\", temp)\n",
    "            temp = replaceContraction(temp.lower())\n",
    "            temp = temp.split()\n",
    "            pos_dict=getposcount(temp)\n",
    "            output_f.update(pos_dict)\n",
    "\n",
    "            output_f['char_count'] = len(output_f['text'])\n",
    "            output_f['word_count'] = len(output_f['text'].split())\n",
    "\n",
    "            output_f['has_question'] = \"?\" in output_f[\"text\"]\n",
    "            output_f['has_exclaim'] = \"!\" in output_f[\"text\"]\n",
    "            output_f['has_period'] = \".\" in output_f[\"text\"]\n",
    "\n",
    "            output_f['capital_ratio']=(capitalratio(obj['text']))\n",
    "            output_f['tweet_count'] = np.log10(obj['user']['statuses_count'])\n",
    "            output_f['listed_count'] = obj['user']['listed_count']\n",
    "            output_f['friends_count'] = obj['user']['friends_count']\n",
    "            output_f['follow_ratio'] = np.log10(obj['user']['followers_count'])\n",
    "\n",
    "            acc_created = datetime.strptime(obj['user']['created_at'], '%a %b %d %H:%M:%S %z %Y')\n",
    "            tweet_created = datetime.strptime(obj['created_at'], '%a %b %d %H:%M:%S %z %Y')\n",
    "            age = (tweet_created - acc_created)\n",
    "            output_f['capital_ratio']=(capitalratio(obj['text']))\n",
    "            output_f['verified'] = obj['user']['verified']\n",
    "\n",
    "            extracted_event.append(output_f)\n",
    "        extracted_features.append(extracted_event)\n",
    "\n",
    "    extracted_df = []\n",
    "    for i, data in enumerate(extracted_features):\n",
    "        temp = pd.DataFrame(data)\n",
    "        temp[\"Event\"] = events[i]\n",
    "        extracted_df.append(pd.DataFrame(temp))\n",
    "\n",
    "    final = pd.concat(extracted_df, ignore_index=True)\n",
    "    final = pd.concat([final, target], axis=1)\n",
    "    return final\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHEME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "charliehebdo's length is 2079\n",
      "ferguson's length is 1143\n",
      "germanwings-crash's length is 469\n",
      "ottawashooting's length is 890\n",
      "sydneysiege's length is 1221\n",
      "0 charliehebdo 2079 2079\n",
      "1 ferguson 1143 1143\n",
      "2 germanwings-crash 469 469\n",
      "3 ottawashooting 890 890\n",
      "4 sydneysiege 1221 1221\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>URLcount</th>\n      <th>Noun</th>\n      <th>Verb</th>\n      <th>Adjective</th>\n      <th>Pronoun</th>\n      <th>FirstPersonPronoun</th>\n      <th>SecondPersonPronoun</th>\n      <th>ThirdPersonPronoun</th>\n      <th>Adverb</th>\n      <th>Numeral</th>\n      <th>Conjunction_inj</th>\n      <th>Particle</th>\n      <th>Determiner</th>\n      <th>Modal</th>\n      <th>Whs</th>\n      <th>HashTag</th>\n      <th>char_count</th>\n      <th>word_count</th>\n      <th>has_question</th>\n      <th>has_exclaim</th>\n      <th>has_period</th>\n      <th>capital_ratio</th>\n      <th>tweet_count</th>\n      <th>listed_count</th>\n      <th>friends_count</th>\n      <th>follow_ratio</th>\n      <th>verified</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>7</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>88</td>\n      <td>12</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>0.159091</td>\n      <td>15.956172</td>\n      <td>7177</td>\n      <td>614</td>\n      <td>17.564194</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>53</td>\n      <td>6</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>0.037736</td>\n      <td>10.071462</td>\n      <td>140</td>\n      <td>375</td>\n      <td>12.201205</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>136</td>\n      <td>18</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>0.073529</td>\n      <td>12.810170</td>\n      <td>758</td>\n      <td>592</td>\n      <td>14.316352</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>4</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>138</td>\n      <td>16</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>0.101449</td>\n      <td>15.732035</td>\n      <td>102287</td>\n      <td>1038</td>\n      <td>23.876904</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>7</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>117</td>\n      <td>13</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>0.145299</td>\n      <td>16.680002</td>\n      <td>13583</td>\n      <td>460</td>\n      <td>19.683865</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   URLcount  Noun  Verb  Adjective  Pronoun  FirstPersonPronoun  \\\n0         1     7     2          0        0                   0   \n1         0     3     2          0        0                   0   \n2         0     4     4          7        0                   0   \n3         2     4     5          1        0                   0   \n4         2     7     2          0        0                   0   \n\n   SecondPersonPronoun  ThirdPersonPronoun  Adverb  Numeral  Conjunction_inj  \\\n0                    0                   0       0        0                2   \n1                    0                   0       0        0                1   \n2                    0                   0       1        0                2   \n3                    0                   0       0        0                0   \n4                    0                   0       0        0                2   \n\n   Particle  Determiner  Modal  Whs  HashTag  char_count  word_count  \\\n0         0           0      0    0        0          88          12   \n1         0           0      0    0        1          53           6   \n2         0           0      0    0        2         136          18   \n3         0           2      0    1        1         138          16   \n4         0           0      0    0        1         117          13   \n\n   has_question  has_exclaim  has_period  capital_ratio  tweet_count  \\\n0         False        False        True       0.159091    15.956172   \n1         False        False        True       0.037736    10.071462   \n2         False        False        True       0.073529    12.810170   \n3         False        False        True       0.101449    15.732035   \n4         False        False        True       0.145299    16.680002   \n\n   listed_count  friends_count  follow_ratio  verified  \n0          7177            614     17.564194      True  \n1           140            375     12.201205     False  \n2           758            592     14.316352     False  \n3        102287           1038     23.876904      True  \n4         13583            460     19.683865      True  "
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../pheme-rnr-dataset\"\n",
    "events = ['charliehebdo', 'ferguson',\n",
    "          'germanwings-crash', 'ottawashooting', 'sydneysiege']\n",
    "# events = ['ferguson']\n",
    "tweetType = 'source-tweet'\n",
    "jsons = []\n",
    "final = fetchRawText(path, events, tweetType)\n",
    "target = final.target\n",
    "final.drop(['text_token','text','Event','target'], axis=1, inplace=True)\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHEME (ADDITIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ebola-essien's length is 14\n",
      "prince-toronto's length is 233\n",
      "putinmissing's length is 238\n",
      "0 ebola-essien 14 14\n",
      "1 prince-toronto 233 233\n",
      "2 putinmissing 238 238\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>URLcount</th>\n      <th>Noun</th>\n      <th>Verb</th>\n      <th>Adjective</th>\n      <th>Pronoun</th>\n      <th>FirstPersonPronoun</th>\n      <th>SecondPersonPronoun</th>\n      <th>ThirdPersonPronoun</th>\n      <th>Adverb</th>\n      <th>Numeral</th>\n      <th>Conjunction_inj</th>\n      <th>Particle</th>\n      <th>Determiner</th>\n      <th>Modal</th>\n      <th>Whs</th>\n      <th>HashTag</th>\n      <th>char_count</th>\n      <th>word_count</th>\n      <th>has_question</th>\n      <th>has_exclaim</th>\n      <th>has_period</th>\n      <th>capital_ratio</th>\n      <th>tweet_count</th>\n      <th>listed_count</th>\n      <th>friends_count</th>\n      <th>follow_ratio</th>\n      <th>verified</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>69</td>\n      <td>8</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>0.101449</td>\n      <td>15.311890</td>\n      <td>148</td>\n      <td>652</td>\n      <td>14.414223</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>9</td>\n      <td>6</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>148</td>\n      <td>25</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>0.027027</td>\n      <td>8.991522</td>\n      <td>1623</td>\n      <td>176</td>\n      <td>18.897987</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>7</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>119</td>\n      <td>20</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>0.025210</td>\n      <td>16.344851</td>\n      <td>2165</td>\n      <td>144</td>\n      <td>17.825921</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>5</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>130</td>\n      <td>16</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>0.107692</td>\n      <td>13.915132</td>\n      <td>608</td>\n      <td>716</td>\n      <td>16.166398</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>4</td>\n      <td>4</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>120</td>\n      <td>15</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>0.066667</td>\n      <td>16.344851</td>\n      <td>2165</td>\n      <td>144</td>\n      <td>17.825921</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   URLcount  Noun  Verb  Adjective  Pronoun  FirstPersonPronoun  \\\n0         1     2     2          1        0                   0   \n1         0     9     6          3        0                   2   \n2         0     7     4          1        1                   0   \n3         2     5     3          2        0                   0   \n4         1     4     4          2        0                   0   \n\n   SecondPersonPronoun  ThirdPersonPronoun  Adverb  Numeral  Conjunction_inj  \\\n0                    0                   0       0        0                1   \n1                    0                   0       3        0                4   \n2                    0                   1       0        0                2   \n3                    0                   0       0        0                1   \n4                    0                   0       1        0                2   \n\n   Particle  Determiner  Modal  Whs  HashTag  char_count  word_count  \\\n0         0           1      0    0        0          69           8   \n1         0           1      1    0        0         148          25   \n2         0           3      0    1        0         119          20   \n3         0           2      0    0        0         130          16   \n4         0           1      0    0        0         120          15   \n\n   has_question  has_exclaim  has_period  capital_ratio  tweet_count  \\\n0         False        False        True       0.101449    15.311890   \n1         False        False        True       0.027027     8.991522   \n2         False        False        True       0.025210    16.344851   \n3         False        False        True       0.107692    13.915132   \n4         False        False        True       0.066667    16.344851   \n\n   listed_count  friends_count  follow_ratio  verified  \n0           148            652     14.414223     False  \n1          1623            176     18.897987      True  \n2          2165            144     17.825921     False  \n3           608            716     16.166398      True  \n4          2165            144     17.825921     False  "
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../PHEME/all-rnr-annotated-threads\"\n",
    "events = ['ebola-essien', 'prince-toronto', 'putinmissing']\n",
    "tweetType = 'source-tweets'\n",
    "jsons = []\n",
    "final_ext = fetchRawText(path,events,tweetType)\n",
    "ext_target = final_ext.target\n",
    "final_ext.drop(['text_token','text','Event','target'], axis=1, inplace=True)\n",
    "final_ext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('./data/_PHEME_sparse.csv', index = False)\n",
    "final_ext.to_csv('./data/_PHEMEext_sparse.csv', index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rosetta",
   "language": "python",
   "name": "rosetta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}