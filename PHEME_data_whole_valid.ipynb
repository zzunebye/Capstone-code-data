{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Tweet 1: \", df['text'][1])\n",
    "# print(\"Indice of '{}': {}\".format(df['text_token'][1][0], w2v_indices[df['text_token'][1][0]]))\n",
    "# print(\"Indice of '{}': {}\".format(df['text_token'][1][0], w2v_vectors[w2v_indices[df['text_token'][1][0]]]))\n",
    "# print(\"Indice of '{}': {}\".format(df['text_token'][1][1], w2v_indices[df['text_token'][1][1]]))\n",
    "# print(\"Indice of '{}': {}\".format(df['text_token'][1][1], w2v_vectors[w2v_indices[df['text_token'][1][1]]]))\n",
    "# print(\"\\nVector of the first headline:\\n\", vectorize(df['text_token'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_X[['hasURL','urls_expanded']][(df_train_X[\"urls_expanded\"] == 0) & (df_train_X[\"hasURL\"] != 0)]"
   ]
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/june/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/june/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob2 import glob\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "from nltk.corpus import stopwords as stp\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import gensim\n",
    "import gensim.models.word2vec as w2v\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 400)\n",
    "# pd.set_option('display.max_rowwidth', 100)\n",
    "pd.set_option('display.max_colwidth', 400)"
   ]
  },
  {
   "source": [
    "# TOC\n",
    "1. Data Import\n",
    "    1. Feature Extraction\n",
    "2. Text Vectorization\n",
    "    2. Word2Vec\n",
    "    3. Doc2Vec\n",
    "4. Data Export as CSV files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Data Import"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def cross_val_jsons(jsonFiles, isTest):\n",
    "    # output = [json for json in jsonFiles]\n",
    "    # print(random.choice(jsonFiles))\n",
    "    if (isTest == True):\n",
    "        test = random.choice(jsonFiles)\n",
    "        jsonFiles.remove(test)\n",
    "        train = [json for event in jsonFiles for json in event]\n",
    "        return train, test\n",
    "    else:\n",
    "        data = [json for event in jsonFiles for json in event]\n",
    "        return [data]\n",
    "\n",
    "def extract_data(datas):\n",
    "    data_lists = []\n",
    "    isRumorLists = []\n",
    "    for index, dataset in enumerate(datas):\n",
    "        data_list = []\n",
    "        isRumorList = []\n",
    "        count = 0 # help var\n",
    "\n",
    "        for jsonFile in dataset:\n",
    "            count+=1\n",
    "            if jsonFile.find(\"non-rumours\") == -1:\n",
    "                isRumorList.append(1)\n",
    "            else:\n",
    "                isRumorList.append(0)\n",
    "\n",
    "            with open (jsonFile, 'r') as f:\n",
    "                for l in f.readlines():\n",
    "                    if not l.strip (): # skip empty lines\n",
    "                        continue\n",
    "                    json_data = json.loads(l)\n",
    "                    # print (json_data,\"\\n\\n\")\n",
    "                    data_list.append(json_data)\n",
    "\n",
    "        isRumorLists.append(pd.DataFrame(isRumorList, columns=['isRumor']))\n",
    "        data_lists.append(data_list)\n",
    "    return data_lists, isRumorLists\n",
    "\n",
    "def printRumor(route):\n",
    "    if route.find(\"rumours\") == -1:\n",
    "        print('non-rumors')\n",
    "    else:\n",
    "        print('rumor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalratio(tweet_text):\n",
    "    uppers = [l for l in tweet_text if l.isupper()]\n",
    "    capitalratio = len(uppers) / len(tweet_text)\n",
    "    return capitalratio \n",
    "\n",
    "def tweets2tokens(tweet_text):\n",
    "    # Tokenizing\n",
    "    urls = []\n",
    "    tokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+','', tweet_text.lower()))\n",
    "    tweet_text = re.sub(r\"http\\S+\", \"\", tweet_text)\n",
    "    # tokens = nltk.TweetTokenizer().tokenize(re.sub(r'([\\d,.]+)','', tweet_text.lower()))\n",
    "\n",
    "    # Setting url value (whether the tweet contains http link) and filter http links\n",
    "    url=0\n",
    "    for token in tokens:\n",
    "        if token.startswith('http'):\n",
    "            url=1\n",
    "    tokens = [token for token in tokens if not token.startswith('http')]\n",
    "\n",
    "    ## Stemming\n",
    "    # porter = PorterStemmer()\n",
    "    # tokens = [porter.stem(token) for token in tokens]\n",
    "\n",
    "    # Filtering Stop words\n",
    "    # from nltk.corpus import stopwords\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # tokens = [token for token in tokens if not token in stop_words]\n",
    "\n",
    "    return tokens, url\n",
    "\n",
    "def getposcount(tokens):\n",
    "    postag = []\n",
    "    poscount = {}\n",
    "    poscount['Noun']=0\n",
    "    poscount['Verb']=0\n",
    "    poscount['Adjective'] = 0\n",
    "    poscount['Pronoun']=0\n",
    "    poscount['FirstPersonPronoun']=0\n",
    "    poscount['SecondPersonPronoun']=0\n",
    "    poscount['ThirdPersonPronoun']=0\n",
    "    poscount['Adverb']=0\n",
    "    poscount['Numeral']=0\n",
    "    poscount['Conjunction_inj']=0\n",
    "    poscount['Particle']=0\n",
    "    poscount['Determiner']=0\n",
    "    poscount['Modal']=0\n",
    "    poscount['Whs']=0\n",
    "    Nouns = {'NN','NNS','NNP','NNPS'}\n",
    "    Adverbs = {'RB','RBR','RBS'}\n",
    "    Whs = {'WDT','WP','WRB'} # Composition of wh-determiner(that,what), wh-pronoun(who), wh-adverb(how)\n",
    "    Verbs={'VB','VBP','VBZ','VBN','VBG','VBD','To'}\n",
    "    first_person_pronouns=['i','I','me','my','mine','we','us','our','ours'] #'i',\n",
    "    second_person_pronouns=['you','your','yours']\n",
    "    third_person_pronouns=['he','she','it','him','her','it','his','hers','its','they','them','their','theirs']\n",
    "\n",
    "    for word in tokens:\n",
    "        w_lower=word.lower()\n",
    "        if w_lower in first_person_pronouns:\n",
    "            poscount['FirstPersonPronoun']+=1\n",
    "        elif w_lower in second_person_pronouns:\n",
    "            poscount['SecondPersonPronoun']+=1\n",
    "        elif w_lower in third_person_pronouns:\n",
    "            poscount['ThirdPersonPronoun']+=1\n",
    "    \n",
    "    postag = nltk.pos_tag(tokens)\n",
    "    for g1 in postag:\n",
    "        if g1[1] in Nouns:\n",
    "            poscount['Noun'] += 1\n",
    "        elif g1[1] in Verbs:\n",
    "            poscount['Verb']+= 1\n",
    "        elif g1[1]=='ADJ'or g1[1]=='JJ':\n",
    "            poscount['Adjective']+=1\n",
    "        elif g1[1]=='PRP' or g1[1]=='PRON' or g1[1]=='PRP$':\n",
    "            poscount['Pronoun']+=1\n",
    "        elif g1[1] in Adverbs or g1[1]=='ADV':\n",
    "            poscount['Adverb']+=1\n",
    "        elif g1[1]=='CD':\n",
    "            poscount['Numeral']+=1\n",
    "        elif g1[1]=='CC' or g1[1]=='IN':\n",
    "            poscount['Conjunction_inj']+=1\n",
    "        elif g1[1]=='RP':\n",
    "            poscount['Particle']+=1\n",
    "        elif g1[1]=='MD':\n",
    "            poscount['Modal']+=1\n",
    "        elif g1[1]=='DT':\n",
    "            poscount['Determiner']+=1\n",
    "        elif g1[1] in Whs:\n",
    "            poscount['Whs']+=1\n",
    "    return poscount\n",
    "\n",
    "def contentlength(words):\n",
    "    wordcount = len(words)\n",
    "    return wordcount\n",
    "\n",
    "def extract_urls(entities_dicts):\n",
    "    if len(entities_dicts) < 1:\n",
    "        return 0,[],[]\n",
    "\n",
    "    urls = []\n",
    "    urls_expanded = []\n",
    "\n",
    "    key = 'url'\n",
    "    key2 = 'expanded_url'\n",
    "    # print(len(entities_dict))\n",
    "    for i in entities_dicts:\n",
    "        urls.append(i[key])\n",
    "        urls_expanded.append(i[key2])\n",
    "    return 1, urls, urls_expanded\n",
    "\n",
    "def flatten_tweets(tweets):\n",
    "    \"\"\" Flattens out tweet dictionaries so relevant JSON is in a top-level dictionary. \"\"\"\n",
    "    tweets_list = []\n",
    "    total_tokens_l = []\n",
    "\n",
    "    # Iterate through each tweet\n",
    "    for tweet_obj in tweets:\n",
    "        output_f = dict()\n",
    "\n",
    "        output_f['text']= tweet_obj['text']\n",
    "        \n",
    "        urls_dicts = tweet_obj['entities']['urls']\n",
    "        # print(urls_dicts)\n",
    "\n",
    "        output_f['hasURL'], output_f['urls'], output_f['urls_expanded'] = extract_urls(urls_dicts)\n",
    "        \n",
    "        # print(type(tweet_obj['user']))\n",
    "        # print(tweet_obj['user'].contains_key('entities'))\n",
    "        if ('url' in tweet_obj['user']):\n",
    "            output_f['hasUserURL'] = 1\n",
    "            output_f['user_url'] = tweet_obj['user']['url']\n",
    "        elif ('entities' in tweet_obj['user']):\n",
    "            # output_f['user_entity'] = tweet_obj['user']['entities']['url']['urls']\n",
    "            # print(tweet_obj['user']['entities']['url']['urls'])\n",
    "            # output_f['user_url'] = tweet_obj['user']['entities']['expanded_url']\n",
    "            output_f['hasUserURL'] , _ , output_f['user_url'] = extract_urls(tweet_obj['user']['entities']['url']['urls'])\n",
    "        else:\n",
    "            # output_f['user_entity'] = None\n",
    "            output_f['user_url'] = 0\n",
    "            output_f['hasUserURL'] = 0\n",
    "\n",
    "        output_f['text_token'], output_f['isNotOnlyText'] = tweets2tokens(tweet_obj['text'])\n",
    "        total_tokens_l.extend(output_f['text_token']) # append the tokens to list of total tokens\n",
    "\n",
    "        '''POS Tagging'''\n",
    "        pos_dict=getposcount(output_f['text_token'])\n",
    "        output_f.update(pos_dict)\n",
    "\n",
    "        output_f['char_count'] = len(output_f['text'])\n",
    "        output_f['word_count'] = len(output_f['text_token'])\n",
    "\n",
    "        output_f['has_question'] = \"?\" in output_f[\"text\"]\n",
    "        output_f['has_exclaim'] = \"!\" in output_f[\"text\"]\n",
    "        output_f['has_period'] = \".\" in output_f[\"text\"]\n",
    "    \n",
    "        ''' User info'''\n",
    "        # Store the user screen name in 'user-screen_name'\n",
    "        # output_f['user-screen_name'] = tweet_obj['user']['screen_name']\n",
    "        \n",
    "        # Store the user location\n",
    "        # output_f['user-location'] = tweet_obj['user']['location']\n",
    "\n",
    "        acc_created = datetime.strptime(tweet_obj['user']['created_at'], '%a %b %d %H:%M:%S %z %Y')\n",
    "        tweet_created = datetime.strptime(tweet_obj['created_at'], '%a %b %d %H:%M:%S %z %Y')\n",
    "        age = (tweet_created - acc_created)\n",
    "        # print(type(timedelta.total_seconds(age)))\n",
    "\n",
    "        output_f['capital_ratio']=(capitalratio(tweet_obj['text']))\n",
    "\n",
    "        # features=(capitalratio(data_list[0]['user']))\n",
    "        output_f['tweet_count'] = np.log10(tweet_obj['user']['statuses_count'])\n",
    "        output_f['listed_count'] = np.log10(tweet_obj['user']['listed_count'])\n",
    "        output_f['follow_ratio'] = np.log10(tweet_obj['user']['followers_count'])\n",
    "        output_f['age'] = int(timedelta.total_seconds(age)/86400)\n",
    "        output_f['verified'] = tweet_obj['user']['verified']\n",
    "\n",
    "        tweets_list.append(output_f)\n",
    "\n",
    "    unk_tokens_l = list(set(total_tokens_l))\n",
    "    print(\"Number of total tokens appeared: {}\\nNumber of unique tokens appeared: {}\\n\".format(len(total_tokens_l), len(unk_tokens_l))) # number of tokens and unique tokens\n",
    "\n",
    "    return tweets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class data_loader():\n",
    "\n",
    "    def getdata(self, root=True, reaction=False, split=True):\n",
    "        lists = []\n",
    "        if root == True:\n",
    "            charliehebdo_jsons = glob('../pheme-rnr-dataset/charliehebdo/**/source-tweet/*.json') \n",
    "            ferguson_jsons = glob('../pheme-rnr-dataset/ferguson/**/source-tweet/*.json')\n",
    "            germanwing_scrash_jsons = glob('../pheme-rnr-dataset/germanwings-crash/**/source-tweet/*.json')\n",
    "            ottawashooting_jsons = glob('../pheme-rnr-dataset/ottawashooting/**/source-tweet/*.json')\n",
    "            sydneysiege_jsons = glob('../pheme-rnr-dataset/sydneysiege/**/source-tweet/*.json')\n",
    "            lists.append([charliehebdo_jsons, ferguson_jsons, germanwing_scrash_jsons, ottawashooting_jsons, sydneysiege_jsons])\n",
    "        elif reaction == True:\n",
    "            charliehebdo_reaction = glob('../pheme-rnr-dataset/charliehebdo/**/reactions/*.json') \n",
    "            ferguson_reaction = glob('../pheme-rnr-dataset/ferguson/**/reactions/*.json')\n",
    "            germanwing_scrash_reaction = glob('../pheme-rnr-dataset/germanwings-crash/**/reactions/*.json')\n",
    "            ottawashooting_reaction = glob('../pheme-rnr-dataset/ottawashooting/**/reactions/*.json')\n",
    "            sydneysiege_reaction = glob('../pheme-rnr-dataset/sydneysiege/**/reactions/*.json')\n",
    "            lists.append([charliehebdo_reaction, ferguson_reaction, germanwing_scrash_reaction, ottawashooting_reaction, sydneysiege_reaction])\n",
    "            \n",
    "        return lists"
   ]
  },
  {
   "source": [
    "## Importing JSON Files and grouping"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(Data of Root tweets) Data: 5802\n(Data of Root tweets) Label: 5802\n"
     ]
    }
   ],
   "source": [
    "jsonFiles = data_loader().getdata()\n",
    "data = cross_val_jsons(jsonFiles[0], isTest = False)\n",
    "data_lists, isRumorLists = extract_data(data)\n",
    "\n",
    "data = data_lists[0]\n",
    "data_y = isRumorLists[0]\n",
    "print(\"(Data of Root tweets) Data: {}\".format(len(data)))\n",
    "print(\"(Data of Root tweets) Label: {}\".format(len(data_y)))"
   ]
  },
  {
   "source": [
    "## Flatten and extract basic features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of total tokens appeared: 88175\nNumber of unique tokens appeared: 8154\n\n"
     ]
    }
   ],
   "source": [
    "df_data = pd.DataFrame(flatten_tweets(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[['has_question', 'has_exclaim', 'has_period','verified']] = df_data[['has_question', 'has_exclaim', 'has_period','verified']].astype(int)"
   ]
  },
  {
   "source": [
    "## Handling inf and NaN value"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3.1020948474234245\nBefore fill: Does the dataset contain NaN value? True\nAfter fill: Does the dataset contain NaN value? False\n"
     ]
    }
   ],
   "source": [
    "for dataset in [df_data]:\n",
    "    dataset['listed_count'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    print(dataset['listed_count'].mean())\n",
    "    print(\"Before fill: Does the dataset contain NaN value? {}\".format(np.any(np.isnan(dataset['listed_count']))))\n",
    "    dataset['listed_count'].fillna(0,inplace=True)\n",
    "    print(\"After fill: Does the dataset contain NaN value? {}\".format(np.any(np.isnan(dataset['listed_count']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.concat([df_data,data_y],axis=1)\n",
    "# df_data.to_csv('./data_notembeded.csv', index = False)"
   ]
  },
  {
   "source": [
    "### Reaction files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reactionFiles = data_loader().getdata(reaction=True,root=False)\n",
    "# data_rt = cross_val_jsons(reactionFiles[0], isTest = False)\n",
    "# data_lists, isRumorLists = extract_data(data_rt)\n",
    "\n",
    "# data_rt = data_lists[0]\n",
    "# data_rt_y = isRumorLists[0]\n",
    "# print(\"(Data of Root tweets) Data: {}\".format(len(data_rt)))\n",
    "# print(\"(Data of Root tweets) Label: {}\".format(len(data_rt_y)))\n",
    "\n",
    "# df_data_rt = pd.DataFrame(flatten_tweets(data_rt))\n",
    "\n",
    "# df_data_rt[['has_question', 'has_exclaim', 'has_period','verified']] = df_data_rt[['has_question', 'has_exclaim', 'has_period','verified']].astype(int)\n",
    "\n",
    "# for dataset in [df_data_rt]:\n",
    "#     dataset['listed_count'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "#     print(dataset['listed_count'].mean())\n",
    "#     print(\"Before fill: Does the dataset contain NaN value? {}\".format(np.any(np.isnan(dataset['listed_count']))))\n",
    "#     dataset['listed_count'].fillna(0,inplace=True)\n",
    "#     print(\"After fill: Does the dataset contain NaN value? {}\".format(np.any(np.isnan(dataset['listed_count']))))\n",
    "# print(data_rt.shape, data_rt_y.shape)\n",
    "# df_data_rt.to_csv('./data_rt_notembeded.csv', index = False)"
   ]
  },
  {
   "source": [
    "# Word2vec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_ = w2v.Word2Vec(\n",
    "    df_data['text_token'],\n",
    "    sg = 1, \n",
    "    seed = 1,\n",
    "    workers = 8,\n",
    "    size = 300,\n",
    "    min_count = 5,\n",
    "    window = 10,\n",
    "    sample = 1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(299798, 440875)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "word2vec_.train(df_data['text_token'], total_examples = word2vec_.corpus_count, epochs = word2vec_.iter)"
   ]
  },
  {
   "source": [
    "# word2vec_.wv.vectors.shape # vocab size / window size"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 75,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_.save('w2v_model_allRoot')"
   ]
  },
  {
   "source": [
    "### Load Word Vector model and Vectorize the sentence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_ = w2v.Word2Vec.load('w2v_model_allRoot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = word2vec_.wv\n",
    "vocabs = word_vectors.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vectors = word_vectors.vectors # here you load vectors for each word in your model\n",
    "w2v_indices = {word: word_vectors.vocab[word].index for word in word_vectors.vocab} # here you load indices - with whom you can find an index of the particular word in your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(line): \n",
    "    words = []\n",
    "    for word in line: # line - iterable, for example list of tokens \n",
    "        try:\n",
    "            w2v_idx = w2v_indices[word]\n",
    "        except KeyError: # if you does not have a vector for this word in your w2v model, continue \n",
    "            continue\n",
    "        words.append(list(w2v_vectors[w2v_idx]))\n",
    "        if not word:\n",
    "            words.append(None)\n",
    "\n",
    "        if len(line) > len(words):\n",
    "            continue\n",
    "    return np.asarray(words)"
   ]
  },
  {
   "source": [
    "## Average of Vectors & Previous features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "df_data['text_token_vec'] = copy.deepcopy(df_data['text_token'])\n",
    "\n",
    "for index, sentence in enumerate(df_data['text_token_vec']):\n",
    "    df_data['text_token_vec'][index] = vectorize(sentence).mean(axis=0)\n",
    "\n",
    "pd.DataFrame(df_data['text_token_vec'].values.tolist()).shape\n",
    "df_data[['text_token','text_token_vec']].head()\n",
    "df_data_avg = pd.DataFrame(df_data['text_token_vec'].values.tolist()).add_prefix('token_avg') #.join(df)\n",
    "df_data_avg = df_data.join(df_data_avg).drop('text_token_vec',axis=1)\n",
    "df_data.drop(['text_token_vec'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                                                       text  \\\n",
       "0  BREAKING: Armed man takes hostage in kosher grocery east of Paris http://t.co/PBs3sMwhLt   \n",
       "1                                     #CharlieHebdo killers dead, confirmed by gendarmerie.   \n",
       "\n",
       "   hasURL                      urls        urls_expanded  hasUserURL  \\\n",
       "0       1  [http://t.co/PBs3sMwhLt]  [http://htz.li/1lI]           1   \n",
       "1       0                        []                   []           1   \n",
       "\n",
       "                      user_url  \\\n",
       "0       http://www.haaretz.com   \n",
       "1  http://www.agnespoirier.org   \n",
       "\n",
       "                                                                     text_token  \\\n",
       "0  [breaking, armed, man, takes, hostage, in, kosher, grocery, east, of, paris]   \n",
       "1                     [charliehebdo, killers, dead, confirmed, by, gendarmerie]   \n",
       "\n",
       "   isNotOnlyText  Noun  Verb  Adjective  Pronoun  FirstPersonPronoun  \\\n",
       "0              1     6     3          0        0                   0   \n",
       "1              0     2     1          2        0                   0   \n",
       "\n",
       "   SecondPersonPronoun  ThirdPersonPronoun  Adverb  Numeral  Conjunction_inj  \\\n",
       "0                    0                   0       0        0                2   \n",
       "1                    0                   0       0        0                1   \n",
       "\n",
       "   Particle  Determiner  Modal  Whs  char_count  word_count  has_question  \\\n",
       "0         0           0      0    0          88          11             0   \n",
       "1         0           0      0    0          53           6             0   \n",
       "\n",
       "   has_exclaim  has_period  capital_ratio  tweet_count  listed_count  \\\n",
       "0            0           1       0.159091     4.803286      3.855943   \n",
       "1            0           1       0.037736     3.031812      2.146128   \n",
       "\n",
       "   follow_ratio   age  verified  isRumor  token_avg0  token_avg1  token_avg2  \\\n",
       "0      5.287349  2126         1        1   -0.006027    0.085144   -0.064036   \n",
       "1      3.672929  1050         0        1    0.048408    0.079971   -0.034599   \n",
       "\n",
       "   token_avg3  token_avg4  token_avg5  token_avg6  token_avg7  token_avg8  \\\n",
       "0   -0.011274    0.076479   -0.018915   -0.098655   -0.080093   -0.086256   \n",
       "1    0.017381    0.129978    0.044376   -0.110858   -0.085569    0.075369   \n",
       "\n",
       "   token_avg9  token_avg10  token_avg11  token_avg12  token_avg13  \\\n",
       "0   -0.062393    -0.174151    -0.007225    -0.184911    -0.033797   \n",
       "1   -0.001091    -0.088124     0.037219    -0.079522    -0.005992   \n",
       "\n",
       "   token_avg14  token_avg15  token_avg16  token_avg17  token_avg18  \\\n",
       "0    -0.112586     0.016870    -0.244381    -0.005288     0.178861   \n",
       "1     0.019553    -0.005698    -0.110534    -0.134521    -0.049864   \n",
       "\n",
       "   token_avg19  token_avg20  token_avg21  token_avg22  token_avg23  \\\n",
       "0     0.152262     0.041770    -0.165306     0.206363    -0.159376   \n",
       "1     0.115772     0.146732    -0.113652     0.161987    -0.097679   \n",
       "\n",
       "   token_avg24  token_avg25  token_avg26  token_avg27  token_avg28  \\\n",
       "0    -0.002186    -0.035903     0.097220    -0.007567     0.047131   \n",
       "1    -0.054023    -0.093517    -0.010226     0.074907    -0.000904   \n",
       "\n",
       "   token_avg29  token_avg30  token_avg31  token_avg32  token_avg33  \\\n",
       "0     0.046416     0.013629    -0.097588     0.177712     0.040532   \n",
       "1     0.078177     0.137360     0.080305    -0.018458    -0.118079   \n",
       "\n",
       "   token_avg34  token_avg35  token_avg36  token_avg37  token_avg38  \\\n",
       "0     0.138038     0.003519    -0.228402    -0.012830    -0.238566   \n",
       "1     0.021411    -0.015279     0.014472    -0.079854    -0.100452   \n",
       "\n",
       "   token_avg39  token_avg40  token_avg41  token_avg42  token_avg43  \\\n",
       "0     0.146355     0.017311    -0.172359    -0.113254    -0.038313   \n",
       "1    -0.035194    -0.117934     0.006172     0.073153     0.111194   \n",
       "\n",
       "   token_avg44  token_avg45  token_avg46  token_avg47  token_avg48  \\\n",
       "0     0.034242    -0.072673     0.186851    -0.272031     0.038934   \n",
       "1     0.011256    -0.027113     0.055541    -0.231259    -0.050898   \n",
       "\n",
       "   token_avg49  token_avg50  token_avg51  token_avg52  token_avg53  \\\n",
       "0    -0.130580    -0.261795    -0.123022     0.222918     0.144743   \n",
       "1    -0.086267    -0.133137     0.041355     0.171183     0.086317   \n",
       "\n",
       "   token_avg54  token_avg55  token_avg56  token_avg57  token_avg58  \\\n",
       "0    -0.080039    -0.045526     0.078636     0.141628    -0.167395   \n",
       "1    -0.165515    -0.064642    -0.065357     0.023706    -0.123599   \n",
       "\n",
       "   token_avg59  token_avg60  token_avg61  token_avg62  token_avg63  \\\n",
       "0     0.043710    -0.152202    -0.019766    -0.022498     0.025560   \n",
       "1    -0.039137    -0.091028     0.035029     0.026342    -0.099426   \n",
       "\n",
       "   token_avg64  token_avg65  token_avg66  token_avg67  token_avg68  \\\n",
       "0     0.048072    -0.117734    -0.020075    -0.173084     0.032999   \n",
       "1     0.111577    -0.066231     0.055743    -0.095827     0.048053   \n",
       "\n",
       "   token_avg69  token_avg70  token_avg71  token_avg72  token_avg73  \\\n",
       "0     0.191838    -0.104420     0.150577     0.039810    -0.100982   \n",
       "1    -0.004885     0.050535     0.171247    -0.090498    -0.027687   \n",
       "\n",
       "   token_avg74  token_avg75  token_avg76  token_avg77  token_avg78  \\\n",
       "0     -0.12647     0.015965    -0.132385    -0.079796     0.128174   \n",
       "1     -0.06355     0.137117    -0.098143     0.012958     0.187908   \n",
       "\n",
       "   token_avg79  token_avg80  token_avg81  token_avg82  token_avg83  \\\n",
       "0     0.156986    -0.102349    -0.185225    -0.188499    -0.051212   \n",
       "1     0.202974     0.027873    -0.037010    -0.050785    -0.049902   \n",
       "\n",
       "   token_avg84  token_avg85  token_avg86  token_avg87  token_avg88  \\\n",
       "0     0.140911    -0.043700     0.275552    -0.060805     0.022670   \n",
       "1     0.061491     0.052127     0.055103     0.004663    -0.043331   \n",
       "\n",
       "   token_avg89  token_avg90  token_avg91  token_avg92  token_avg93  \\\n",
       "0     0.068833    -0.280379     0.055338     0.035075    -0.034295   \n",
       "1    -0.107575    -0.152637     0.027082    -0.006905    -0.010329   \n",
       "\n",
       "   token_avg94  token_avg95  token_avg96  token_avg97  token_avg98  \\\n",
       "0    -0.069489    -0.189265    -0.096716     0.053707     0.110510   \n",
       "1    -0.095275    -0.044555    -0.021584    -0.009144     0.009539   \n",
       "\n",
       "   token_avg99  token_avg100  token_avg101  token_avg102  token_avg103  \\\n",
       "0     0.011938     -0.043480     -0.004786      0.061748     -0.045264   \n",
       "1     0.008779     -0.026399     -0.006481     -0.075531     -0.030742   \n",
       "\n",
       "   token_avg104  token_avg105  token_avg106  token_avg107  token_avg108  \\\n",
       "0      0.016330     -0.065226      0.059246     -0.022661      0.081958   \n",
       "1      0.092718      0.119079      0.021441     -0.027970      0.044172   \n",
       "\n",
       "   token_avg109  token_avg110  token_avg111  token_avg112  token_avg113  \\\n",
       "0     -0.039438     -0.066527      0.137965      0.009647      0.169362   \n",
       "1      0.035002     -0.106987      0.029933     -0.010249      0.038908   \n",
       "\n",
       "   token_avg114  token_avg115  token_avg116  token_avg117  token_avg118  \\\n",
       "0      0.034295     -0.173005      0.108029     -0.029467     -0.165104   \n",
       "1      0.032148     -0.108431     -0.005575     -0.154381     -0.138223   \n",
       "\n",
       "   token_avg119  token_avg120  token_avg121  token_avg122  token_avg123  \\\n",
       "0      0.027369      0.134197      0.051545     -0.141865     -0.201460   \n",
       "1     -0.037262     -0.017161     -0.139140     -0.074505     -0.208994   \n",
       "\n",
       "   token_avg124  token_avg125  token_avg126  token_avg127  token_avg128  \\\n",
       "0      0.028215     -0.094240      0.079178      0.068440      0.121748   \n",
       "1     -0.009623      0.009277      0.057958     -0.026074      0.072247   \n",
       "\n",
       "   token_avg129  token_avg130  token_avg131  token_avg132  token_avg133  \\\n",
       "0     -0.070929     -0.038566      0.203004     -0.240796      0.107883   \n",
       "1      0.035432     -0.065708      0.021020     -0.242554      0.038758   \n",
       "\n",
       "   token_avg134  token_avg135  token_avg136  token_avg137  token_avg138  \\\n",
       "0     -0.014362     -0.073670      0.125341      0.113488      0.024515   \n",
       "1      0.020209     -0.018944     -0.013423     -0.013725      0.010541   \n",
       "\n",
       "   token_avg139  token_avg140  token_avg141  token_avg142  token_avg143  \\\n",
       "0      0.181422      0.028603      0.118485     -0.046544      0.273955   \n",
       "1      0.096907      0.060786      0.037670      0.029103      0.117195   \n",
       "\n",
       "   token_avg144  token_avg145  token_avg146  token_avg147  token_avg148  \\\n",
       "0     -0.165217     -0.148981      0.087634     -0.082968      0.099055   \n",
       "1     -0.110845     -0.035990     -0.096228     -0.091955     -0.006992   \n",
       "\n",
       "   token_avg149  token_avg150  token_avg151  token_avg152  token_avg153  \\\n",
       "0      0.067133     -0.207443      0.188778     -0.140304      0.175239   \n",
       "1      0.025123      0.044565     -0.039459      0.005266      0.099841   \n",
       "\n",
       "   token_avg154  token_avg155  token_avg156  token_avg157  token_avg158  \\\n",
       "0      0.002377      0.297011      0.090668      0.038375     -0.081917   \n",
       "1      0.198137      0.134592      0.018241     -0.053873     -0.092327   \n",
       "\n",
       "   token_avg159  token_avg160  token_avg161  token_avg162  token_avg163  \\\n",
       "0     -0.015315      0.074284      0.039536     -0.027708      0.167059   \n",
       "1     -0.030734      0.003703      0.070691     -0.101937      0.090904   \n",
       "\n",
       "   token_avg164  token_avg165  token_avg166  token_avg167  token_avg168  \\\n",
       "0     -0.126910     -0.038964     -0.040762     -0.009297      0.134586   \n",
       "1     -0.051289      0.019167      0.030310      0.025441      0.046608   \n",
       "\n",
       "   token_avg169  token_avg170  token_avg171  token_avg172  token_avg173  \\\n",
       "0      0.161715      0.013739      0.046886      0.024482     -0.022727   \n",
       "1      0.111566      0.050264      0.026843      0.033931     -0.090407   \n",
       "\n",
       "   token_avg174  token_avg175  token_avg176  token_avg177  token_avg178  \\\n",
       "0      0.086605     -0.001038     -0.099578     -0.018886     -0.178449   \n",
       "1      0.018331     -0.060141     -0.036647     -0.026894     -0.042308   \n",
       "\n",
       "   token_avg179  token_avg180  token_avg181  token_avg182  token_avg183  \\\n",
       "0     -0.159755     -0.138451     -0.074291      0.012966      0.193948   \n",
       "1     -0.102402     -0.198589     -0.064671      0.013905      0.122163   \n",
       "\n",
       "   token_avg184  token_avg185  token_avg186  token_avg187  token_avg188  \\\n",
       "0      0.164944     -0.027220      0.108251      0.081450      0.109513   \n",
       "1      0.176623     -0.053418      0.029371     -0.007728     -0.046862   \n",
       "\n",
       "   token_avg189  token_avg190  token_avg191  token_avg192  token_avg193  \\\n",
       "0     -0.112567     -0.036670     -0.003836      0.144597      0.067950   \n",
       "1      0.009265      0.043138     -0.107665      0.037832      0.101738   \n",
       "\n",
       "   token_avg194  token_avg195  token_avg196  token_avg197  token_avg198  \\\n",
       "0     -0.162956      0.042621     -0.094827     -0.003753     -0.039060   \n",
       "1     -0.073760      0.031898     -0.125621      0.040037      0.024925   \n",
       "\n",
       "   token_avg199  token_avg200  token_avg201  token_avg202  token_avg203  \\\n",
       "0      0.067986      0.042022      0.053498      0.163131      0.092213   \n",
       "1      0.014131     -0.101184     -0.010865      0.121785      0.055652   \n",
       "\n",
       "   token_avg204  token_avg205  token_avg206  token_avg207  token_avg208  \\\n",
       "0     -0.026392     -0.096868     -0.018080      0.059557      0.050017   \n",
       "1     -0.079270      0.013381      0.018037      0.177600     -0.090658   \n",
       "\n",
       "   token_avg209  token_avg210  token_avg211  token_avg212  token_avg213  \\\n",
       "0      0.075161      0.174966      0.044659     -0.136946      0.061222   \n",
       "1      0.012684      0.126213     -0.049539     -0.058885      0.257905   \n",
       "\n",
       "   token_avg214  token_avg215  token_avg216  token_avg217  token_avg218  \\\n",
       "0     -0.027038      0.038045     -0.111500     -0.058696      0.149002   \n",
       "1      0.006776      0.124328     -0.172139      0.066646      0.098914   \n",
       "\n",
       "   token_avg219  token_avg220  token_avg221  token_avg222  token_avg223  \\\n",
       "0      0.100161      0.218628     -0.125748      0.040409     -0.111020   \n",
       "1      0.065558      0.121090     -0.078145     -0.009625     -0.096161   \n",
       "\n",
       "   token_avg224  token_avg225  token_avg226  token_avg227  token_avg228  \\\n",
       "0     -0.043434      0.026371     -0.099695      0.070868      0.313226   \n",
       "1     -0.226444      0.032850     -0.270502      0.119547      0.304222   \n",
       "\n",
       "   token_avg229  token_avg230  token_avg231  token_avg232  token_avg233  \\\n",
       "0      0.116609     -0.079144      0.161327      0.184720     -0.037010   \n",
       "1      0.051219     -0.200845      0.152462      0.210527      0.065105   \n",
       "\n",
       "   token_avg234  token_avg235  token_avg236  token_avg237  token_avg238  \\\n",
       "0     -0.140843     -0.071286      0.031111      0.061864     -0.083245   \n",
       "1     -0.066552     -0.196175      0.092446      0.061139     -0.106833   \n",
       "\n",
       "   token_avg239  token_avg240  token_avg241  token_avg242  token_avg243  \\\n",
       "0     -0.054461      0.073421      0.370187      0.067216     -0.098888   \n",
       "1      0.024038     -0.082177      0.165611      0.164802     -0.075769   \n",
       "\n",
       "   token_avg244  token_avg245  token_avg246  token_avg247  token_avg248  \\\n",
       "0      0.243185      0.017610      0.099520      0.113986     -0.197305   \n",
       "1      0.240306     -0.171144      0.127671      0.278576     -0.304305   \n",
       "\n",
       "   token_avg249  token_avg250  token_avg251  token_avg252  token_avg253  \\\n",
       "0      0.028942     -0.083277     -0.073129      0.044681     -0.061024   \n",
       "1      0.046672     -0.044562     -0.167899      0.167985      0.034055   \n",
       "\n",
       "   token_avg254  token_avg255  token_avg256  token_avg257  token_avg258  \\\n",
       "0     -0.034001     -0.023726     -0.199287     -0.099605      0.147507   \n",
       "1      0.027703     -0.072531     -0.305758     -0.133190      0.057541   \n",
       "\n",
       "   token_avg259  token_avg260  token_avg261  token_avg262  token_avg263  \\\n",
       "0     -0.201318      0.110860     -0.005397      0.054394      0.329516   \n",
       "1     -0.226844      0.057795     -0.042175      0.059909      0.280293   \n",
       "\n",
       "   token_avg264  token_avg265  token_avg266  token_avg267  token_avg268  \\\n",
       "0     -0.061770     -0.328951       0.20297     -0.333822      0.136197   \n",
       "1      0.081186     -0.215158       0.24530     -0.404011      0.059626   \n",
       "\n",
       "   token_avg269  token_avg270  token_avg271  token_avg272  token_avg273  \\\n",
       "0      0.191589      0.063632      0.090266     -0.001651     -0.254338   \n",
       "1      0.001325     -0.040390      0.013898     -0.029683     -0.031351   \n",
       "\n",
       "   token_avg274  token_avg275  token_avg276  token_avg277  token_avg278  \\\n",
       "0      0.143840      0.146425      0.098631     -0.295660     -0.075441   \n",
       "1      0.080107      0.141687      0.082703     -0.186254     -0.150707   \n",
       "\n",
       "   token_avg279  token_avg280  token_avg281  token_avg282  token_avg283  \\\n",
       "0      0.048438     -0.093722      0.026116      0.125520     -0.041295   \n",
       "1     -0.040022     -0.134498     -0.088771      0.157583     -0.048898   \n",
       "\n",
       "   token_avg284  token_avg285  token_avg286  token_avg287  token_avg288  \\\n",
       "0     -0.116765     -0.079263      0.164486      0.099347     -0.197233   \n",
       "1      0.054966     -0.015920      0.137583      0.041392     -0.287289   \n",
       "\n",
       "   token_avg289  token_avg290  token_avg291  token_avg292  token_avg293  \\\n",
       "0      0.194294     -0.004453      0.096356     -0.251950      0.064731   \n",
       "1      0.084412     -0.023835      0.084650     -0.253772      0.123045   \n",
       "\n",
       "   token_avg294  token_avg295  token_avg296  token_avg297  token_avg298  \\\n",
       "0      0.202382      0.506858      0.106635     -0.010196      0.028184   \n",
       "1      0.146030      0.405638      0.086370     -0.001103      0.024664   \n",
       "\n",
       "   token_avg299  \n",
       "0     -0.058375  \n",
       "1     -0.006507  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>hasURL</th>\n      <th>urls</th>\n      <th>urls_expanded</th>\n      <th>hasUserURL</th>\n      <th>user_url</th>\n      <th>text_token</th>\n      <th>isNotOnlyText</th>\n      <th>Noun</th>\n      <th>Verb</th>\n      <th>Adjective</th>\n      <th>Pronoun</th>\n      <th>FirstPersonPronoun</th>\n      <th>SecondPersonPronoun</th>\n      <th>ThirdPersonPronoun</th>\n      <th>Adverb</th>\n      <th>Numeral</th>\n      <th>Conjunction_inj</th>\n      <th>Particle</th>\n      <th>Determiner</th>\n      <th>Modal</th>\n      <th>Whs</th>\n      <th>char_count</th>\n      <th>word_count</th>\n      <th>has_question</th>\n      <th>has_exclaim</th>\n      <th>has_period</th>\n      <th>capital_ratio</th>\n      <th>tweet_count</th>\n      <th>listed_count</th>\n      <th>follow_ratio</th>\n      <th>age</th>\n      <th>verified</th>\n      <th>isRumor</th>\n      <th>token_avg0</th>\n      <th>token_avg1</th>\n      <th>token_avg2</th>\n      <th>token_avg3</th>\n      <th>token_avg4</th>\n      <th>token_avg5</th>\n      <th>token_avg6</th>\n      <th>token_avg7</th>\n      <th>token_avg8</th>\n      <th>token_avg9</th>\n      <th>token_avg10</th>\n      <th>token_avg11</th>\n      <th>token_avg12</th>\n      <th>token_avg13</th>\n      <th>token_avg14</th>\n      <th>token_avg15</th>\n      <th>token_avg16</th>\n      <th>token_avg17</th>\n      <th>token_avg18</th>\n      <th>token_avg19</th>\n      <th>token_avg20</th>\n      <th>token_avg21</th>\n      <th>token_avg22</th>\n      <th>token_avg23</th>\n      <th>token_avg24</th>\n      <th>token_avg25</th>\n      <th>token_avg26</th>\n      <th>token_avg27</th>\n      <th>token_avg28</th>\n      <th>token_avg29</th>\n      <th>token_avg30</th>\n      <th>token_avg31</th>\n      <th>token_avg32</th>\n      <th>token_avg33</th>\n      <th>token_avg34</th>\n      <th>token_avg35</th>\n      <th>token_avg36</th>\n      <th>token_avg37</th>\n      <th>token_avg38</th>\n      <th>token_avg39</th>\n      <th>token_avg40</th>\n      <th>token_avg41</th>\n      <th>token_avg42</th>\n      <th>token_avg43</th>\n      <th>token_avg44</th>\n      <th>token_avg45</th>\n      <th>token_avg46</th>\n      <th>token_avg47</th>\n      <th>token_avg48</th>\n      <th>token_avg49</th>\n      <th>token_avg50</th>\n      <th>token_avg51</th>\n      <th>token_avg52</th>\n      <th>token_avg53</th>\n      <th>token_avg54</th>\n      <th>token_avg55</th>\n      <th>token_avg56</th>\n      <th>token_avg57</th>\n      <th>token_avg58</th>\n      <th>token_avg59</th>\n      <th>token_avg60</th>\n      <th>token_avg61</th>\n      <th>token_avg62</th>\n      <th>token_avg63</th>\n      <th>token_avg64</th>\n      <th>token_avg65</th>\n      <th>token_avg66</th>\n      <th>token_avg67</th>\n      <th>token_avg68</th>\n      <th>token_avg69</th>\n      <th>token_avg70</th>\n      <th>token_avg71</th>\n      <th>token_avg72</th>\n      <th>token_avg73</th>\n      <th>token_avg74</th>\n      <th>token_avg75</th>\n      <th>token_avg76</th>\n      <th>token_avg77</th>\n      <th>token_avg78</th>\n      <th>token_avg79</th>\n      <th>token_avg80</th>\n      <th>token_avg81</th>\n      <th>token_avg82</th>\n      <th>token_avg83</th>\n      <th>token_avg84</th>\n      <th>token_avg85</th>\n      <th>token_avg86</th>\n      <th>token_avg87</th>\n      <th>token_avg88</th>\n      <th>token_avg89</th>\n      <th>token_avg90</th>\n      <th>token_avg91</th>\n      <th>token_avg92</th>\n      <th>token_avg93</th>\n      <th>token_avg94</th>\n      <th>token_avg95</th>\n      <th>token_avg96</th>\n      <th>token_avg97</th>\n      <th>token_avg98</th>\n      <th>token_avg99</th>\n      <th>token_avg100</th>\n      <th>token_avg101</th>\n      <th>token_avg102</th>\n      <th>token_avg103</th>\n      <th>token_avg104</th>\n      <th>token_avg105</th>\n      <th>token_avg106</th>\n      <th>token_avg107</th>\n      <th>token_avg108</th>\n      <th>token_avg109</th>\n      <th>token_avg110</th>\n      <th>token_avg111</th>\n      <th>token_avg112</th>\n      <th>token_avg113</th>\n      <th>token_avg114</th>\n      <th>token_avg115</th>\n      <th>token_avg116</th>\n      <th>token_avg117</th>\n      <th>token_avg118</th>\n      <th>token_avg119</th>\n      <th>token_avg120</th>\n      <th>token_avg121</th>\n      <th>token_avg122</th>\n      <th>token_avg123</th>\n      <th>token_avg124</th>\n      <th>token_avg125</th>\n      <th>token_avg126</th>\n      <th>token_avg127</th>\n      <th>token_avg128</th>\n      <th>token_avg129</th>\n      <th>token_avg130</th>\n      <th>token_avg131</th>\n      <th>token_avg132</th>\n      <th>token_avg133</th>\n      <th>token_avg134</th>\n      <th>token_avg135</th>\n      <th>token_avg136</th>\n      <th>token_avg137</th>\n      <th>token_avg138</th>\n      <th>token_avg139</th>\n      <th>token_avg140</th>\n      <th>token_avg141</th>\n      <th>token_avg142</th>\n      <th>token_avg143</th>\n      <th>token_avg144</th>\n      <th>token_avg145</th>\n      <th>token_avg146</th>\n      <th>token_avg147</th>\n      <th>token_avg148</th>\n      <th>token_avg149</th>\n      <th>token_avg150</th>\n      <th>token_avg151</th>\n      <th>token_avg152</th>\n      <th>token_avg153</th>\n      <th>token_avg154</th>\n      <th>token_avg155</th>\n      <th>token_avg156</th>\n      <th>token_avg157</th>\n      <th>token_avg158</th>\n      <th>token_avg159</th>\n      <th>token_avg160</th>\n      <th>token_avg161</th>\n      <th>token_avg162</th>\n      <th>token_avg163</th>\n      <th>token_avg164</th>\n      <th>token_avg165</th>\n      <th>token_avg166</th>\n      <th>token_avg167</th>\n      <th>token_avg168</th>\n      <th>token_avg169</th>\n      <th>token_avg170</th>\n      <th>token_avg171</th>\n      <th>token_avg172</th>\n      <th>token_avg173</th>\n      <th>token_avg174</th>\n      <th>token_avg175</th>\n      <th>token_avg176</th>\n      <th>token_avg177</th>\n      <th>token_avg178</th>\n      <th>token_avg179</th>\n      <th>token_avg180</th>\n      <th>token_avg181</th>\n      <th>token_avg182</th>\n      <th>token_avg183</th>\n      <th>token_avg184</th>\n      <th>token_avg185</th>\n      <th>token_avg186</th>\n      <th>token_avg187</th>\n      <th>token_avg188</th>\n      <th>token_avg189</th>\n      <th>token_avg190</th>\n      <th>token_avg191</th>\n      <th>token_avg192</th>\n      <th>token_avg193</th>\n      <th>token_avg194</th>\n      <th>token_avg195</th>\n      <th>token_avg196</th>\n      <th>token_avg197</th>\n      <th>token_avg198</th>\n      <th>token_avg199</th>\n      <th>token_avg200</th>\n      <th>token_avg201</th>\n      <th>token_avg202</th>\n      <th>token_avg203</th>\n      <th>token_avg204</th>\n      <th>token_avg205</th>\n      <th>token_avg206</th>\n      <th>token_avg207</th>\n      <th>token_avg208</th>\n      <th>token_avg209</th>\n      <th>token_avg210</th>\n      <th>token_avg211</th>\n      <th>token_avg212</th>\n      <th>token_avg213</th>\n      <th>token_avg214</th>\n      <th>token_avg215</th>\n      <th>token_avg216</th>\n      <th>token_avg217</th>\n      <th>token_avg218</th>\n      <th>token_avg219</th>\n      <th>token_avg220</th>\n      <th>token_avg221</th>\n      <th>token_avg222</th>\n      <th>token_avg223</th>\n      <th>token_avg224</th>\n      <th>token_avg225</th>\n      <th>token_avg226</th>\n      <th>token_avg227</th>\n      <th>token_avg228</th>\n      <th>token_avg229</th>\n      <th>token_avg230</th>\n      <th>token_avg231</th>\n      <th>token_avg232</th>\n      <th>token_avg233</th>\n      <th>token_avg234</th>\n      <th>token_avg235</th>\n      <th>token_avg236</th>\n      <th>token_avg237</th>\n      <th>token_avg238</th>\n      <th>token_avg239</th>\n      <th>token_avg240</th>\n      <th>token_avg241</th>\n      <th>token_avg242</th>\n      <th>token_avg243</th>\n      <th>token_avg244</th>\n      <th>token_avg245</th>\n      <th>token_avg246</th>\n      <th>token_avg247</th>\n      <th>token_avg248</th>\n      <th>token_avg249</th>\n      <th>token_avg250</th>\n      <th>token_avg251</th>\n      <th>token_avg252</th>\n      <th>token_avg253</th>\n      <th>token_avg254</th>\n      <th>token_avg255</th>\n      <th>token_avg256</th>\n      <th>token_avg257</th>\n      <th>token_avg258</th>\n      <th>token_avg259</th>\n      <th>token_avg260</th>\n      <th>token_avg261</th>\n      <th>token_avg262</th>\n      <th>token_avg263</th>\n      <th>token_avg264</th>\n      <th>token_avg265</th>\n      <th>token_avg266</th>\n      <th>token_avg267</th>\n      <th>token_avg268</th>\n      <th>token_avg269</th>\n      <th>token_avg270</th>\n      <th>token_avg271</th>\n      <th>token_avg272</th>\n      <th>token_avg273</th>\n      <th>token_avg274</th>\n      <th>token_avg275</th>\n      <th>token_avg276</th>\n      <th>token_avg277</th>\n      <th>token_avg278</th>\n      <th>token_avg279</th>\n      <th>token_avg280</th>\n      <th>token_avg281</th>\n      <th>token_avg282</th>\n      <th>token_avg283</th>\n      <th>token_avg284</th>\n      <th>token_avg285</th>\n      <th>token_avg286</th>\n      <th>token_avg287</th>\n      <th>token_avg288</th>\n      <th>token_avg289</th>\n      <th>token_avg290</th>\n      <th>token_avg291</th>\n      <th>token_avg292</th>\n      <th>token_avg293</th>\n      <th>token_avg294</th>\n      <th>token_avg295</th>\n      <th>token_avg296</th>\n      <th>token_avg297</th>\n      <th>token_avg298</th>\n      <th>token_avg299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BREAKING: Armed man takes hostage in kosher grocery east of Paris http://t.co/PBs3sMwhLt</td>\n      <td>1</td>\n      <td>[http://t.co/PBs3sMwhLt]</td>\n      <td>[http://htz.li/1lI]</td>\n      <td>1</td>\n      <td>http://www.haaretz.com</td>\n      <td>[breaking, armed, man, takes, hostage, in, kosher, grocery, east, of, paris]</td>\n      <td>1</td>\n      <td>6</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>88</td>\n      <td>11</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.159091</td>\n      <td>4.803286</td>\n      <td>3.855943</td>\n      <td>5.287349</td>\n      <td>2126</td>\n      <td>1</td>\n      <td>1</td>\n      <td>-0.006027</td>\n      <td>0.085144</td>\n      <td>-0.064036</td>\n      <td>-0.011274</td>\n      <td>0.076479</td>\n      <td>-0.018915</td>\n      <td>-0.098655</td>\n      <td>-0.080093</td>\n      <td>-0.086256</td>\n      <td>-0.062393</td>\n      <td>-0.174151</td>\n      <td>-0.007225</td>\n      <td>-0.184911</td>\n      <td>-0.033797</td>\n      <td>-0.112586</td>\n      <td>0.016870</td>\n      <td>-0.244381</td>\n      <td>-0.005288</td>\n      <td>0.178861</td>\n      <td>0.152262</td>\n      <td>0.041770</td>\n      <td>-0.165306</td>\n      <td>0.206363</td>\n      <td>-0.159376</td>\n      <td>-0.002186</td>\n      <td>-0.035903</td>\n      <td>0.097220</td>\n      <td>-0.007567</td>\n      <td>0.047131</td>\n      <td>0.046416</td>\n      <td>0.013629</td>\n      <td>-0.097588</td>\n      <td>0.177712</td>\n      <td>0.040532</td>\n      <td>0.138038</td>\n      <td>0.003519</td>\n      <td>-0.228402</td>\n      <td>-0.012830</td>\n      <td>-0.238566</td>\n      <td>0.146355</td>\n      <td>0.017311</td>\n      <td>-0.172359</td>\n      <td>-0.113254</td>\n      <td>-0.038313</td>\n      <td>0.034242</td>\n      <td>-0.072673</td>\n      <td>0.186851</td>\n      <td>-0.272031</td>\n      <td>0.038934</td>\n      <td>-0.130580</td>\n      <td>-0.261795</td>\n      <td>-0.123022</td>\n      <td>0.222918</td>\n      <td>0.144743</td>\n      <td>-0.080039</td>\n      <td>-0.045526</td>\n      <td>0.078636</td>\n      <td>0.141628</td>\n      <td>-0.167395</td>\n      <td>0.043710</td>\n      <td>-0.152202</td>\n      <td>-0.019766</td>\n      <td>-0.022498</td>\n      <td>0.025560</td>\n      <td>0.048072</td>\n      <td>-0.117734</td>\n      <td>-0.020075</td>\n      <td>-0.173084</td>\n      <td>0.032999</td>\n      <td>0.191838</td>\n      <td>-0.104420</td>\n      <td>0.150577</td>\n      <td>0.039810</td>\n      <td>-0.100982</td>\n      <td>-0.12647</td>\n      <td>0.015965</td>\n      <td>-0.132385</td>\n      <td>-0.079796</td>\n      <td>0.128174</td>\n      <td>0.156986</td>\n      <td>-0.102349</td>\n      <td>-0.185225</td>\n      <td>-0.188499</td>\n      <td>-0.051212</td>\n      <td>0.140911</td>\n      <td>-0.043700</td>\n      <td>0.275552</td>\n      <td>-0.060805</td>\n      <td>0.022670</td>\n      <td>0.068833</td>\n      <td>-0.280379</td>\n      <td>0.055338</td>\n      <td>0.035075</td>\n      <td>-0.034295</td>\n      <td>-0.069489</td>\n      <td>-0.189265</td>\n      <td>-0.096716</td>\n      <td>0.053707</td>\n      <td>0.110510</td>\n      <td>0.011938</td>\n      <td>-0.043480</td>\n      <td>-0.004786</td>\n      <td>0.061748</td>\n      <td>-0.045264</td>\n      <td>0.016330</td>\n      <td>-0.065226</td>\n      <td>0.059246</td>\n      <td>-0.022661</td>\n      <td>0.081958</td>\n      <td>-0.039438</td>\n      <td>-0.066527</td>\n      <td>0.137965</td>\n      <td>0.009647</td>\n      <td>0.169362</td>\n      <td>0.034295</td>\n      <td>-0.173005</td>\n      <td>0.108029</td>\n      <td>-0.029467</td>\n      <td>-0.165104</td>\n      <td>0.027369</td>\n      <td>0.134197</td>\n      <td>0.051545</td>\n      <td>-0.141865</td>\n      <td>-0.201460</td>\n      <td>0.028215</td>\n      <td>-0.094240</td>\n      <td>0.079178</td>\n      <td>0.068440</td>\n      <td>0.121748</td>\n      <td>-0.070929</td>\n      <td>-0.038566</td>\n      <td>0.203004</td>\n      <td>-0.240796</td>\n      <td>0.107883</td>\n      <td>-0.014362</td>\n      <td>-0.073670</td>\n      <td>0.125341</td>\n      <td>0.113488</td>\n      <td>0.024515</td>\n      <td>0.181422</td>\n      <td>0.028603</td>\n      <td>0.118485</td>\n      <td>-0.046544</td>\n      <td>0.273955</td>\n      <td>-0.165217</td>\n      <td>-0.148981</td>\n      <td>0.087634</td>\n      <td>-0.082968</td>\n      <td>0.099055</td>\n      <td>0.067133</td>\n      <td>-0.207443</td>\n      <td>0.188778</td>\n      <td>-0.140304</td>\n      <td>0.175239</td>\n      <td>0.002377</td>\n      <td>0.297011</td>\n      <td>0.090668</td>\n      <td>0.038375</td>\n      <td>-0.081917</td>\n      <td>-0.015315</td>\n      <td>0.074284</td>\n      <td>0.039536</td>\n      <td>-0.027708</td>\n      <td>0.167059</td>\n      <td>-0.126910</td>\n      <td>-0.038964</td>\n      <td>-0.040762</td>\n      <td>-0.009297</td>\n      <td>0.134586</td>\n      <td>0.161715</td>\n      <td>0.013739</td>\n      <td>0.046886</td>\n      <td>0.024482</td>\n      <td>-0.022727</td>\n      <td>0.086605</td>\n      <td>-0.001038</td>\n      <td>-0.099578</td>\n      <td>-0.018886</td>\n      <td>-0.178449</td>\n      <td>-0.159755</td>\n      <td>-0.138451</td>\n      <td>-0.074291</td>\n      <td>0.012966</td>\n      <td>0.193948</td>\n      <td>0.164944</td>\n      <td>-0.027220</td>\n      <td>0.108251</td>\n      <td>0.081450</td>\n      <td>0.109513</td>\n      <td>-0.112567</td>\n      <td>-0.036670</td>\n      <td>-0.003836</td>\n      <td>0.144597</td>\n      <td>0.067950</td>\n      <td>-0.162956</td>\n      <td>0.042621</td>\n      <td>-0.094827</td>\n      <td>-0.003753</td>\n      <td>-0.039060</td>\n      <td>0.067986</td>\n      <td>0.042022</td>\n      <td>0.053498</td>\n      <td>0.163131</td>\n      <td>0.092213</td>\n      <td>-0.026392</td>\n      <td>-0.096868</td>\n      <td>-0.018080</td>\n      <td>0.059557</td>\n      <td>0.050017</td>\n      <td>0.075161</td>\n      <td>0.174966</td>\n      <td>0.044659</td>\n      <td>-0.136946</td>\n      <td>0.061222</td>\n      <td>-0.027038</td>\n      <td>0.038045</td>\n      <td>-0.111500</td>\n      <td>-0.058696</td>\n      <td>0.149002</td>\n      <td>0.100161</td>\n      <td>0.218628</td>\n      <td>-0.125748</td>\n      <td>0.040409</td>\n      <td>-0.111020</td>\n      <td>-0.043434</td>\n      <td>0.026371</td>\n      <td>-0.099695</td>\n      <td>0.070868</td>\n      <td>0.313226</td>\n      <td>0.116609</td>\n      <td>-0.079144</td>\n      <td>0.161327</td>\n      <td>0.184720</td>\n      <td>-0.037010</td>\n      <td>-0.140843</td>\n      <td>-0.071286</td>\n      <td>0.031111</td>\n      <td>0.061864</td>\n      <td>-0.083245</td>\n      <td>-0.054461</td>\n      <td>0.073421</td>\n      <td>0.370187</td>\n      <td>0.067216</td>\n      <td>-0.098888</td>\n      <td>0.243185</td>\n      <td>0.017610</td>\n      <td>0.099520</td>\n      <td>0.113986</td>\n      <td>-0.197305</td>\n      <td>0.028942</td>\n      <td>-0.083277</td>\n      <td>-0.073129</td>\n      <td>0.044681</td>\n      <td>-0.061024</td>\n      <td>-0.034001</td>\n      <td>-0.023726</td>\n      <td>-0.199287</td>\n      <td>-0.099605</td>\n      <td>0.147507</td>\n      <td>-0.201318</td>\n      <td>0.110860</td>\n      <td>-0.005397</td>\n      <td>0.054394</td>\n      <td>0.329516</td>\n      <td>-0.061770</td>\n      <td>-0.328951</td>\n      <td>0.20297</td>\n      <td>-0.333822</td>\n      <td>0.136197</td>\n      <td>0.191589</td>\n      <td>0.063632</td>\n      <td>0.090266</td>\n      <td>-0.001651</td>\n      <td>-0.254338</td>\n      <td>0.143840</td>\n      <td>0.146425</td>\n      <td>0.098631</td>\n      <td>-0.295660</td>\n      <td>-0.075441</td>\n      <td>0.048438</td>\n      <td>-0.093722</td>\n      <td>0.026116</td>\n      <td>0.125520</td>\n      <td>-0.041295</td>\n      <td>-0.116765</td>\n      <td>-0.079263</td>\n      <td>0.164486</td>\n      <td>0.099347</td>\n      <td>-0.197233</td>\n      <td>0.194294</td>\n      <td>-0.004453</td>\n      <td>0.096356</td>\n      <td>-0.251950</td>\n      <td>0.064731</td>\n      <td>0.202382</td>\n      <td>0.506858</td>\n      <td>0.106635</td>\n      <td>-0.010196</td>\n      <td>0.028184</td>\n      <td>-0.058375</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>#CharlieHebdo killers dead, confirmed by gendarmerie.</td>\n      <td>0</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>1</td>\n      <td>http://www.agnespoirier.org</td>\n      <td>[charliehebdo, killers, dead, confirmed, by, gendarmerie]</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>53</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.037736</td>\n      <td>3.031812</td>\n      <td>2.146128</td>\n      <td>3.672929</td>\n      <td>1050</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.048408</td>\n      <td>0.079971</td>\n      <td>-0.034599</td>\n      <td>0.017381</td>\n      <td>0.129978</td>\n      <td>0.044376</td>\n      <td>-0.110858</td>\n      <td>-0.085569</td>\n      <td>0.075369</td>\n      <td>-0.001091</td>\n      <td>-0.088124</td>\n      <td>0.037219</td>\n      <td>-0.079522</td>\n      <td>-0.005992</td>\n      <td>0.019553</td>\n      <td>-0.005698</td>\n      <td>-0.110534</td>\n      <td>-0.134521</td>\n      <td>-0.049864</td>\n      <td>0.115772</td>\n      <td>0.146732</td>\n      <td>-0.113652</td>\n      <td>0.161987</td>\n      <td>-0.097679</td>\n      <td>-0.054023</td>\n      <td>-0.093517</td>\n      <td>-0.010226</td>\n      <td>0.074907</td>\n      <td>-0.000904</td>\n      <td>0.078177</td>\n      <td>0.137360</td>\n      <td>0.080305</td>\n      <td>-0.018458</td>\n      <td>-0.118079</td>\n      <td>0.021411</td>\n      <td>-0.015279</td>\n      <td>0.014472</td>\n      <td>-0.079854</td>\n      <td>-0.100452</td>\n      <td>-0.035194</td>\n      <td>-0.117934</td>\n      <td>0.006172</td>\n      <td>0.073153</td>\n      <td>0.111194</td>\n      <td>0.011256</td>\n      <td>-0.027113</td>\n      <td>0.055541</td>\n      <td>-0.231259</td>\n      <td>-0.050898</td>\n      <td>-0.086267</td>\n      <td>-0.133137</td>\n      <td>0.041355</td>\n      <td>0.171183</td>\n      <td>0.086317</td>\n      <td>-0.165515</td>\n      <td>-0.064642</td>\n      <td>-0.065357</td>\n      <td>0.023706</td>\n      <td>-0.123599</td>\n      <td>-0.039137</td>\n      <td>-0.091028</td>\n      <td>0.035029</td>\n      <td>0.026342</td>\n      <td>-0.099426</td>\n      <td>0.111577</td>\n      <td>-0.066231</td>\n      <td>0.055743</td>\n      <td>-0.095827</td>\n      <td>0.048053</td>\n      <td>-0.004885</td>\n      <td>0.050535</td>\n      <td>0.171247</td>\n      <td>-0.090498</td>\n      <td>-0.027687</td>\n      <td>-0.06355</td>\n      <td>0.137117</td>\n      <td>-0.098143</td>\n      <td>0.012958</td>\n      <td>0.187908</td>\n      <td>0.202974</td>\n      <td>0.027873</td>\n      <td>-0.037010</td>\n      <td>-0.050785</td>\n      <td>-0.049902</td>\n      <td>0.061491</td>\n      <td>0.052127</td>\n      <td>0.055103</td>\n      <td>0.004663</td>\n      <td>-0.043331</td>\n      <td>-0.107575</td>\n      <td>-0.152637</td>\n      <td>0.027082</td>\n      <td>-0.006905</td>\n      <td>-0.010329</td>\n      <td>-0.095275</td>\n      <td>-0.044555</td>\n      <td>-0.021584</td>\n      <td>-0.009144</td>\n      <td>0.009539</td>\n      <td>0.008779</td>\n      <td>-0.026399</td>\n      <td>-0.006481</td>\n      <td>-0.075531</td>\n      <td>-0.030742</td>\n      <td>0.092718</td>\n      <td>0.119079</td>\n      <td>0.021441</td>\n      <td>-0.027970</td>\n      <td>0.044172</td>\n      <td>0.035002</td>\n      <td>-0.106987</td>\n      <td>0.029933</td>\n      <td>-0.010249</td>\n      <td>0.038908</td>\n      <td>0.032148</td>\n      <td>-0.108431</td>\n      <td>-0.005575</td>\n      <td>-0.154381</td>\n      <td>-0.138223</td>\n      <td>-0.037262</td>\n      <td>-0.017161</td>\n      <td>-0.139140</td>\n      <td>-0.074505</td>\n      <td>-0.208994</td>\n      <td>-0.009623</td>\n      <td>0.009277</td>\n      <td>0.057958</td>\n      <td>-0.026074</td>\n      <td>0.072247</td>\n      <td>0.035432</td>\n      <td>-0.065708</td>\n      <td>0.021020</td>\n      <td>-0.242554</td>\n      <td>0.038758</td>\n      <td>0.020209</td>\n      <td>-0.018944</td>\n      <td>-0.013423</td>\n      <td>-0.013725</td>\n      <td>0.010541</td>\n      <td>0.096907</td>\n      <td>0.060786</td>\n      <td>0.037670</td>\n      <td>0.029103</td>\n      <td>0.117195</td>\n      <td>-0.110845</td>\n      <td>-0.035990</td>\n      <td>-0.096228</td>\n      <td>-0.091955</td>\n      <td>-0.006992</td>\n      <td>0.025123</td>\n      <td>0.044565</td>\n      <td>-0.039459</td>\n      <td>0.005266</td>\n      <td>0.099841</td>\n      <td>0.198137</td>\n      <td>0.134592</td>\n      <td>0.018241</td>\n      <td>-0.053873</td>\n      <td>-0.092327</td>\n      <td>-0.030734</td>\n      <td>0.003703</td>\n      <td>0.070691</td>\n      <td>-0.101937</td>\n      <td>0.090904</td>\n      <td>-0.051289</td>\n      <td>0.019167</td>\n      <td>0.030310</td>\n      <td>0.025441</td>\n      <td>0.046608</td>\n      <td>0.111566</td>\n      <td>0.050264</td>\n      <td>0.026843</td>\n      <td>0.033931</td>\n      <td>-0.090407</td>\n      <td>0.018331</td>\n      <td>-0.060141</td>\n      <td>-0.036647</td>\n      <td>-0.026894</td>\n      <td>-0.042308</td>\n      <td>-0.102402</td>\n      <td>-0.198589</td>\n      <td>-0.064671</td>\n      <td>0.013905</td>\n      <td>0.122163</td>\n      <td>0.176623</td>\n      <td>-0.053418</td>\n      <td>0.029371</td>\n      <td>-0.007728</td>\n      <td>-0.046862</td>\n      <td>0.009265</td>\n      <td>0.043138</td>\n      <td>-0.107665</td>\n      <td>0.037832</td>\n      <td>0.101738</td>\n      <td>-0.073760</td>\n      <td>0.031898</td>\n      <td>-0.125621</td>\n      <td>0.040037</td>\n      <td>0.024925</td>\n      <td>0.014131</td>\n      <td>-0.101184</td>\n      <td>-0.010865</td>\n      <td>0.121785</td>\n      <td>0.055652</td>\n      <td>-0.079270</td>\n      <td>0.013381</td>\n      <td>0.018037</td>\n      <td>0.177600</td>\n      <td>-0.090658</td>\n      <td>0.012684</td>\n      <td>0.126213</td>\n      <td>-0.049539</td>\n      <td>-0.058885</td>\n      <td>0.257905</td>\n      <td>0.006776</td>\n      <td>0.124328</td>\n      <td>-0.172139</td>\n      <td>0.066646</td>\n      <td>0.098914</td>\n      <td>0.065558</td>\n      <td>0.121090</td>\n      <td>-0.078145</td>\n      <td>-0.009625</td>\n      <td>-0.096161</td>\n      <td>-0.226444</td>\n      <td>0.032850</td>\n      <td>-0.270502</td>\n      <td>0.119547</td>\n      <td>0.304222</td>\n      <td>0.051219</td>\n      <td>-0.200845</td>\n      <td>0.152462</td>\n      <td>0.210527</td>\n      <td>0.065105</td>\n      <td>-0.066552</td>\n      <td>-0.196175</td>\n      <td>0.092446</td>\n      <td>0.061139</td>\n      <td>-0.106833</td>\n      <td>0.024038</td>\n      <td>-0.082177</td>\n      <td>0.165611</td>\n      <td>0.164802</td>\n      <td>-0.075769</td>\n      <td>0.240306</td>\n      <td>-0.171144</td>\n      <td>0.127671</td>\n      <td>0.278576</td>\n      <td>-0.304305</td>\n      <td>0.046672</td>\n      <td>-0.044562</td>\n      <td>-0.167899</td>\n      <td>0.167985</td>\n      <td>0.034055</td>\n      <td>0.027703</td>\n      <td>-0.072531</td>\n      <td>-0.305758</td>\n      <td>-0.133190</td>\n      <td>0.057541</td>\n      <td>-0.226844</td>\n      <td>0.057795</td>\n      <td>-0.042175</td>\n      <td>0.059909</td>\n      <td>0.280293</td>\n      <td>0.081186</td>\n      <td>-0.215158</td>\n      <td>0.24530</td>\n      <td>-0.404011</td>\n      <td>0.059626</td>\n      <td>0.001325</td>\n      <td>-0.040390</td>\n      <td>0.013898</td>\n      <td>-0.029683</td>\n      <td>-0.031351</td>\n      <td>0.080107</td>\n      <td>0.141687</td>\n      <td>0.082703</td>\n      <td>-0.186254</td>\n      <td>-0.150707</td>\n      <td>-0.040022</td>\n      <td>-0.134498</td>\n      <td>-0.088771</td>\n      <td>0.157583</td>\n      <td>-0.048898</td>\n      <td>0.054966</td>\n      <td>-0.015920</td>\n      <td>0.137583</td>\n      <td>0.041392</td>\n      <td>-0.287289</td>\n      <td>0.084412</td>\n      <td>-0.023835</td>\n      <td>0.084650</td>\n      <td>-0.253772</td>\n      <td>0.123045</td>\n      <td>0.146030</td>\n      <td>0.405638</td>\n      <td>0.086370</td>\n      <td>-0.001103</td>\n      <td>0.024664</td>\n      <td>-0.006507</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "df_data_avg.head(2)"
   ]
  },
  {
   "source": [
    "# Doc2Vec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The Dimension of a vector is: 300\n"
     ]
    }
   ],
   "source": [
    "#Doc2vec 실행\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(df_data['text_token'])]\n",
    "Doc2vec_model = Doc2Vec(vector_size=300, min_alpha=0.025, window=10, min_count=1, workers=4, epochs=120) #documents,\n",
    "Doc2vec_model.build_vocab(documents)\n",
    "\n",
    "Doc2vec_model.train(documents, epochs=Doc2vec_model.epochs, total_examples=Doc2vec_model.corpus_count)\n",
    "\n",
    "print(\"The Dimension of a vector is: {}\".format(len(Doc2vec_model.docvecs[0]))) # document vector의 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_text(index):\n",
    "    similar = Doc2vec_model.docvecs.most_similar(index)\n",
    "\n",
    "    print(\"The quried text: \\n\\n{} \\n\\nis most similar to the text:\\n\\n{}\".format(df_data['text'][index],df_data['text'][similar[0][0]]))\n",
    "\n",
    "# most_similar_text(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc2vec_model.save('./d2v_model_whole')"
   ]
  },
  {
   "source": [
    "## Infer the document vectors from trained Doc2Vec Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Doc2vec_model = Doc2Vec.load('./d2v_model_whole')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 31,
   "outputs": []
  },
  {
   "source": [
    "### Train data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['text_token_doc'] = copy.deepcopy(df_data['text_token'])\n",
    "\n",
    "for index, sentence in enumerate(df_data['text_token_doc']):\n",
    "    df_data['text_token_doc'][index] = Doc2vec_model.infer_vector(df_data['text_token_doc'][index],steps=50)\n",
    "\n",
    "df_data_doc = pd.DataFrame(df_data['text_token_doc'].values.tolist()).add_prefix('doc_vec')\n",
    "df_data_doc = df_data.join(df_data_doc).drop('text_token_doc',axis=1)"
   ]
  },
  {
   "source": [
    "# Exporting data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Doc2vec Vector Feature set'''\n",
    "df_data_avg.to_csv('./data_avg.csv', index = False)\n",
    "df_data_doc.to_csv('./data_doc.csv', index = False)"
   ]
  },
  {
   "source": [
    "# Validation data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(Data of Root tweets) X: 390 y: 390\n",
      "Number of total tokens appeared: 5637\n",
      "Number of unique tokens appeared: 1516\n",
      "\n",
      "Before fill: Does the dataset contain NaN value? True\n",
      "After fill: Does the dataset contain NaN value? False\n",
      "\n",
      "Dropping tweets with short length (<10)....\n",
      " (390, 34)\n",
      " -> (328, 34)\n"
     ]
    }
   ],
   "source": [
    "gurlitt_jsons = glob('../PHEME/all-rnr-annotated-threads/gurlitt-all-rnr-threads/**/source-tweets/*.json')\n",
    "ebolaessien_jsons = glob('../PHEME/all-rnr-annotated-threads/ebola-essien-all-rnr-threads/**/source-tweets/*.json')\n",
    "putinmissing_jsons = glob('../PHEME/all-rnr-annotated-threads/putinmissing-all-rnr-threads/**/source-tweets/*.json')\n",
    "\n",
    "added_files = [gurlitt_jsons, ebolaessien_jsons, putinmissing_jsons]\n",
    "\n",
    "valid_data = cross_val_jsons(added_files, False)\n",
    "data_lists, isRumorLists = extract_data(valid_data)\n",
    "X_valid = data_lists[0]\n",
    "y_valid = isRumorLists[0]\n",
    "print(\"(Data of Root tweets) X: {} y: {}\".format(len(X_valid),len(y_valid)))\n",
    "\n",
    "df_valid_X = pd.DataFrame(flatten_tweets(X_valid))\n",
    "df_valid = pd.concat([df_valid_X,y_valid],axis=1)\n",
    "df_valid[['has_question', 'has_exclaim', 'has_period','verified']] = df_valid[['has_question', 'has_exclaim', 'has_period','verified']].astype(int)\n",
    "\n",
    "for dataset in [df_valid]:\n",
    "    dataset['listed_count'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    # print(dataset['listed_count'].mean())\n",
    "    print(\"Before fill: Does the dataset contain NaN value? {}\".format(np.any(np.isnan(dataset['listed_count']))))\n",
    "    dataset['listed_count'].fillna(0,inplace=True)\n",
    "    print(\"After fill: Does the dataset contain NaN value? {}\".format(np.any(np.isnan(dataset['listed_count']))))\n",
    "print(\"\\nDropping tweets with short length (<10)....\\n\",df_valid.shape)\n",
    "df_valid.to_csv('./data_valid_notembeded.csv', index = False)\n",
    "\n",
    "df_valid.drop(df_valid[df_valid['word_count'] < 10].index, inplace=True)\n",
    "df_valid.reset_index(drop=True, inplace=True)\n",
    "print(\" ->\",df_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_ = w2v.Word2Vec.load('w2v_model_whole')\n",
    "word_vectors = word2vec_.wv\n",
    "w2v_vectors = word_vectors.vectors # here you load vectors for each word in your model\n",
    "w2v_indices = {word: word_vectors.vocab[word].index for word in word_vectors.vocab} # here you load indices - with whom you can find an index of the particular word in your model \n",
    "\n",
    "df_valid['text_token_vec'] = copy.deepcopy(df_valid['text_token'])\n",
    "\n",
    "for index, sentence in enumerate(df_valid['text_token_vec']):\n",
    "    df_valid['text_token_vec'][index] = vectorize(sentence).mean(axis=0)\n",
    "\n",
    "avg = pd.DataFrame(df_valid['text_token_vec'].values.tolist()).add_prefix('token_avg')\n",
    "df_valid_avg = df_valid.join(avg).drop('text_token_vec',axis=1)\n",
    "\n",
    "df_valid.drop(['text_token_vec'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc2vec_model = Doc2Vec.load('./d2v_model_whole')\n",
    "df_valid['text_token_doc'] = copy.deepcopy(df_valid['text_token'])\n",
    "\n",
    "for index, sentence in enumerate(df_valid['text_token_doc']):\n",
    "    df_valid['text_token_doc'][index] = Doc2vec_model.infer_vector(df_valid['text_token_doc'][index],steps=50)\n",
    "\n",
    "doc = pd.DataFrame(df_valid['text_token_doc'].values.tolist()).add_prefix('doc_vec')\n",
    "df_valid_doc = df_valid.join(doc).drop('text_token_doc',axis=1)\n",
    "\n",
    "df_valid.drop(['text_token_doc'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_avg.to_csv('./valid_avg.csv', index = False)\n",
    "df_valid_doc.to_csv('./valid_doc.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}