{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "## Options:\n",
    "### Pretrained model:\n",
    "- word2vec-ruscorpora-300\t\n",
    "- glove-twitter-100\t\n",
    "### Tokenization\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "from nltk.corpus import stopwords as stpdfa\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import gensim\n",
    "import gensim.models.word2vec as w2v\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"./data/_PHEME_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5802, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>Event</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1080</th>\n      <td>#ICYMI, Christopher Hitchens on the case for m...</td>\n      <td>charliehebdo</td>\n    </tr>\n    <tr>\n      <th>4120</th>\n      <td>DEVELOPING NEWS: Soldier shot at War Memorial....</td>\n      <td>ottawashooting</td>\n    </tr>\n    <tr>\n      <th>5004</th>\n      <td>RT @tomsteinfort: Terrifying photo of hostages...</td>\n      <td>sydneysiege</td>\n    </tr>\n    <tr>\n      <th>2305</th>\n      <td>#Ferguson chief said the officer was unaware o...</td>\n      <td>ferguson</td>\n    </tr>\n    <tr>\n      <th>4898</th>\n      <td>BREAKING: 2 people have run out of Sydney buil...</td>\n      <td>sydneysiege</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                                   text           Event\n1080  #ICYMI, Christopher Hitchens on the case for m...    charliehebdo\n4120  DEVELOPING NEWS: Soldier shot at War Memorial....  ottawashooting\n5004  RT @tomsteinfort: Terrifying photo of hostages...     sydneysiege\n2305  #Ferguson chief said the officer was unaware o...        ferguson\n4898  BREAKING: 2 people have run out of Sydney buil...     sydneysiege"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(raw_data.shape)\n",
    "raw_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0                                                       BREAKING: Armed man takes hostage in kosher grocery east of Paris http://t.co/PBs3sMwhLt\n1                                                                                          #CharlieHebdo killers dead, confirmed by gendarmerie.\n2       Top French cartoonists Charb, Cabu, Wolinski, Tignous confirmed among dead in #Paris #CharlieHebdo attack. Editor is critically wounded.\n3    Police have surrounded the area where the #CharlieHebdo attack suspects are believed to be: http://t.co/3tGXEIX4F2\\nhttps://t.co/aBSezf2QWS\n4                          PHOTO: Armed gunmen face police officers near #CharlieHebdo HQ in Paris http://t.co/3Jsosc7yl3 http://t.co/iOpVNO6Iq0\nName: text, dtype: object"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.text.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = []\n",
    "for sent in raw_data.text:\n",
    "    # print(tweet_tokenizer.tokenize(sent))\n",
    "    tweet_tokens.append([tweet_tokenizer.tokenize(sent)])\n",
    "    # tweet_tokens.append(tweet_tokenizer.tokenize(sent))\n",
    "df_tokens = pd.DataFrame(tweet_tokens, columns=['token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[BREAKING, :, Armed, man, takes, hostage, in, kosher, grocery, east, of, Paris, http://t.co/PBs3sMwhLt]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[#CharlieHebdo, killers, dead, ,, confirmed, by, gendarmerie, .]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[Top, French, cartoonists, Charb, ,, Cabu, ,, Wolinski, ,, Tignous, confirmed, among, dead, in, #Paris, #CharlieHebdo, attack, ., Editor, is, critically, wounded, .]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[Police, have, surrounded, the, area, where, the, #CharlieHebdo, attack, suspects, are, believed, to, be, :, http://t.co/3tGXEIX4F2, https://t.co/aBSezf2QWS]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[PHOTO, :, Armed, gunmen, face, police, officers, near, #CharlieHebdo, HQ, in, Paris, http://t.co/3Jsosc7yl3, http://t.co/iOpVNO6Iq0]</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                                                                                                                                                   token\n0                                                                [BREAKING, :, Armed, man, takes, hostage, in, kosher, grocery, east, of, Paris, http://t.co/PBs3sMwhLt]\n1                                                                                                       [#CharlieHebdo, killers, dead, ,, confirmed, by, gendarmerie, .]\n2  [Top, French, cartoonists, Charb, ,, Cabu, ,, Wolinski, ,, Tignous, confirmed, among, dead, in, #Paris, #CharlieHebdo, attack, ., Editor, is, critically, wounded, .]\n3          [Police, have, surrounded, the, area, where, the, #CharlieHebdo, attack, suspects, are, believed, to, be, :, http://t.co/3tGXEIX4F2, https://t.co/aBSezf2QWS]\n4                                  [PHOTO, :, Armed, gunmen, face, police, officers, near, #CharlieHebdo, HQ, in, Paris, http://t.co/3Jsosc7yl3, http://t.co/iOpVNO6Iq0]"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Pretrained model for Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching pretrained Model and Convert the Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "# corpus = api.load('text8')\n",
    "# wv = api.load('word2vec-google-news-300')\n",
    "# fasttext-wiki-news-subwords-300'\n",
    "#  'glove-twitter-200',\n",
    "model = api.load('glove-twitter-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7fa0739797f0>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import inspect\n",
    "# print(inspect.getsource(wv.__class__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True) #without *norm_only* param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "[ 0.50779  -1.0274    0.48136  -0.09417   0.44837  -0.52291   0.51498\n",
      " -0.038927  0.35867  -0.065994]\n"
     ]
    }
   ],
   "source": [
    "dog = model['dog']\n",
    "print(dog.shape)\n",
    "print(dog[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_object = model.wv\n",
    "w2v_vectors = w2v_object.vectors # here you load vectors for each word in your model\n",
    "w2v_indices = {word: w2v_object.vocab[word].index for word in w2v_object.vocab} # here you load indices - with whom you can find an index of the particular word in your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(line): \n",
    "    words = []\n",
    "    for word in line: # line - iterable, for example list of tokens \n",
    "        try:\n",
    "            w2v_idx = w2v_indices[word]\n",
    "        except KeyError: # if you does not have a vector for this word in your w2v model, continue \n",
    "            continue\n",
    "        words.append(list(w2v_vectors[w2v_idx]))\n",
    "        if not word:\n",
    "            words.append(None)\n",
    "\n",
    "        if len(line) > len(words):\n",
    "            continue\n",
    "    return np.asarray(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 1:  #CharlieHebdo killers dead, confirmed by gendarmerie.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'#CharlieHebdo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-be454167033a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tweet 1: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Indice of '{}': {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(\"Indice of '{}': {}\".format(raw_data['text_token'][1][0], w2v_vectors[w2v_indices[raw_data['text_token'][1][0]]]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print(\"Indice of '{}': {}\".format(raw_data['text_token'][1][1], w2v_indices[raw_data['text_token'][1][1]]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(\"Indice of '{}': {}\".format(raw_data['text_token'][1][1], w2v_vectors[w2v_indices[raw_data['text_token'][1][1]]]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '#CharlieHebdo'"
     ]
    }
   ],
   "source": [
    "print(\"Tweet 1: \", raw_data['text'][1])\n",
    "print(\"Indice of '{}': {}\".format(df_tokens['token'][1][0], w2v_indices[df_tokens['token'][1][0]]))\n",
    "# print(\"Indice of '{}': {}\".format(raw_data['text_token'][1][0], w2v_vectors[w2v_indices[raw_data['text_token'][1][0]]]))\n",
    "# print(\"Indice of '{}': {}\".format(raw_data['text_token'][1][1], w2v_indices[raw_data['text_token'][1][1]]))\n",
    "# print(\"Indice of '{}': {}\".format(raw_data['text_token'][1][1], w2v_vectors[w2v_indices[raw_data['text_token'][1][1]]]))\n",
    "# print(\"\\nVector of the first headline:\\n\", vectorize(raw_data['text_token'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rosetta",
   "language": "python",
   "name": "rosetta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}