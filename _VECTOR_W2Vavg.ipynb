{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "## Options:\n",
    "### Pretrained model:\n",
    "- word2vec-ruscorpora-300\t\n",
    "- glove-twitter-200\n",
    "\n",
    "### Tokenization: How to manage OOV?\n",
    "- Link: '&'\n",
    "- Tag: '#'\n",
    "- Mention: '@' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collective Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "import gensim\n",
    "import gensim.models.word2vec as w2v\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load('glove-twitter-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-77925cc2d5b2>:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  w2v_object = model.wv\n"
     ]
    }
   ],
   "source": [
    "w2v_object = model.wv\n",
    "w2v_vectors = w2v_object.vectors # here you load vectors for each word in your model\n",
    "w2v_indices = {word: w2v_object.vocab[word].index for word in w2v_object.vocab} # here you load indices - with whom you can find an index of the particular word in your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(line):\n",
    "    words = []\n",
    "    for word in line:  # line - iterable, for example list of tokens\n",
    "        try:\n",
    "            w2v_idx = w2v_indices[word]\n",
    "        except KeyError:  # if you does not have a vector for this word in your w2v model, continue\n",
    "            words.append(list(np.zeros(200,)))\n",
    "            continue\n",
    "        words.append(list(w2v_vectors[w2v_idx]))\n",
    "        if not word:\n",
    "            words.append(None)\n",
    "\n",
    "        if len(line) > len(words):\n",
    "            continue\n",
    "    return np.asarray(words)\n",
    "\n",
    "\n",
    "def get_W2V_AVG(raw_data):\n",
    "    tweet_tokenizer = TweetTokenizer()\n",
    "    tweet_tokens = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "   \n",
    "    for sent in raw_data.text:\n",
    "        sent = re.sub(r\"http\\S+\", \"&\", sent)\n",
    "        sent = re.sub(r\"@\\S+\", \"@\", sent)\n",
    "        sent = re.sub(r\"#\\S+\", \"#\", sent)\n",
    "        sent = re.sub(r'([^\\s\\w@#&]|_)+', '', sent)\n",
    "        # sent = re.sub('', '', sent.lower())\n",
    "        # print(tweet_tokenizer.tokenize(sent))\n",
    "        # sent = [tweet_tokenizer.tokenize(sent)]\n",
    "        sent = [tweet_tokenizer.tokenize(sent.lower())]\n",
    "        temp = [token for token in sent[0] if not token in stop_words]\n",
    "        tweet_tokens.append([temp])\n",
    "        # tweet_tokens.append(tweet_tokenizer.tokenize(sent))\n",
    "    df_tokens = pd.DataFrame(tweet_tokens, columns=['token'])\n",
    "    df_tokens['token_vec'] = copy.deepcopy(df_tokens['token'])\n",
    "\n",
    "    for index, sent in enumerate(df_tokens['token_vec']):\n",
    "        df_tokens['token_vec'][index] = vectorize(sent).mean(axis=0)\n",
    "\n",
    "    df_temp = pd.DataFrame(\n",
    "        df_tokens['token_vec'].values.tolist()).add_prefix('vec_avg')\n",
    "\n",
    "    df_tokens = df_tokens.join(df_temp).drop('token_vec', axis=1)\n",
    "    return pd.DataFrame(df_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"./data/_PHEME_text.csv\")\n",
    "raw_RHI = pd.read_csv(\"./data/_RHI_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHEME_W2V_AVG = get_W2V_AVG(raw_data)\n",
    "RHI_W2V_avg = get_W2V_AVG(raw_RHI)\n",
    "# RHI_W2V_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the result\n",
    "# PHEME_W2V_AVG.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHEME_W2V_AVG.to_csv('./data/_PHEME_text_AVGw2v.csv', index = False)\n",
    "RHI_W2V_avg.to_csv('./data/_RHI_text_AVGw2v.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_RHI = pd.read_csv(\"./data/_RHS_text_AVGw2v.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token</th>\n      <th>vec_avg0</th>\n      <th>vec_avg1</th>\n      <th>vec_avg2</th>\n      <th>vec_avg3</th>\n      <th>vec_avg4</th>\n      <th>vec_avg5</th>\n      <th>vec_avg6</th>\n      <th>vec_avg7</th>\n      <th>vec_avg8</th>\n      <th>vec_avg9</th>\n      <th>vec_avg10</th>\n      <th>vec_avg11</th>\n      <th>vec_avg12</th>\n      <th>vec_avg13</th>\n      <th>vec_avg14</th>\n      <th>vec_avg15</th>\n      <th>vec_avg16</th>\n      <th>vec_avg17</th>\n      <th>vec_avg18</th>\n      <th>vec_avg19</th>\n      <th>vec_avg20</th>\n      <th>vec_avg21</th>\n      <th>vec_avg22</th>\n      <th>vec_avg23</th>\n      <th>vec_avg24</th>\n      <th>vec_avg25</th>\n      <th>vec_avg26</th>\n      <th>vec_avg27</th>\n      <th>vec_avg28</th>\n      <th>vec_avg29</th>\n      <th>vec_avg30</th>\n      <th>vec_avg31</th>\n      <th>vec_avg32</th>\n      <th>vec_avg33</th>\n      <th>vec_avg34</th>\n      <th>vec_avg35</th>\n      <th>vec_avg36</th>\n      <th>vec_avg37</th>\n      <th>vec_avg38</th>\n      <th>vec_avg39</th>\n      <th>vec_avg40</th>\n      <th>vec_avg41</th>\n      <th>vec_avg42</th>\n      <th>vec_avg43</th>\n      <th>vec_avg44</th>\n      <th>vec_avg45</th>\n      <th>vec_avg46</th>\n      <th>vec_avg47</th>\n      <th>vec_avg48</th>\n      <th>vec_avg49</th>\n      <th>vec_avg50</th>\n      <th>vec_avg51</th>\n      <th>vec_avg52</th>\n      <th>vec_avg53</th>\n      <th>vec_avg54</th>\n      <th>vec_avg55</th>\n      <th>vec_avg56</th>\n      <th>vec_avg57</th>\n      <th>vec_avg58</th>\n      <th>vec_avg59</th>\n      <th>vec_avg60</th>\n      <th>vec_avg61</th>\n      <th>vec_avg62</th>\n      <th>vec_avg63</th>\n      <th>vec_avg64</th>\n      <th>vec_avg65</th>\n      <th>vec_avg66</th>\n      <th>vec_avg67</th>\n      <th>vec_avg68</th>\n      <th>vec_avg69</th>\n      <th>vec_avg70</th>\n      <th>vec_avg71</th>\n      <th>vec_avg72</th>\n      <th>vec_avg73</th>\n      <th>vec_avg74</th>\n      <th>vec_avg75</th>\n      <th>vec_avg76</th>\n      <th>vec_avg77</th>\n      <th>vec_avg78</th>\n      <th>vec_avg79</th>\n      <th>vec_avg80</th>\n      <th>vec_avg81</th>\n      <th>vec_avg82</th>\n      <th>vec_avg83</th>\n      <th>vec_avg84</th>\n      <th>vec_avg85</th>\n      <th>vec_avg86</th>\n      <th>vec_avg87</th>\n      <th>vec_avg88</th>\n      <th>vec_avg89</th>\n      <th>vec_avg90</th>\n      <th>vec_avg91</th>\n      <th>vec_avg92</th>\n      <th>vec_avg93</th>\n      <th>vec_avg94</th>\n      <th>vec_avg95</th>\n      <th>vec_avg96</th>\n      <th>vec_avg97</th>\n      <th>vec_avg98</th>\n      <th>vec_avg99</th>\n      <th>vec_avg100</th>\n      <th>vec_avg101</th>\n      <th>vec_avg102</th>\n      <th>vec_avg103</th>\n      <th>vec_avg104</th>\n      <th>vec_avg105</th>\n      <th>vec_avg106</th>\n      <th>vec_avg107</th>\n      <th>vec_avg108</th>\n      <th>vec_avg109</th>\n      <th>vec_avg110</th>\n      <th>vec_avg111</th>\n      <th>vec_avg112</th>\n      <th>vec_avg113</th>\n      <th>vec_avg114</th>\n      <th>vec_avg115</th>\n      <th>vec_avg116</th>\n      <th>vec_avg117</th>\n      <th>vec_avg118</th>\n      <th>vec_avg119</th>\n      <th>vec_avg120</th>\n      <th>vec_avg121</th>\n      <th>vec_avg122</th>\n      <th>vec_avg123</th>\n      <th>vec_avg124</th>\n      <th>vec_avg125</th>\n      <th>vec_avg126</th>\n      <th>vec_avg127</th>\n      <th>vec_avg128</th>\n      <th>vec_avg129</th>\n      <th>vec_avg130</th>\n      <th>vec_avg131</th>\n      <th>vec_avg132</th>\n      <th>vec_avg133</th>\n      <th>vec_avg134</th>\n      <th>vec_avg135</th>\n      <th>vec_avg136</th>\n      <th>vec_avg137</th>\n      <th>vec_avg138</th>\n      <th>vec_avg139</th>\n      <th>vec_avg140</th>\n      <th>vec_avg141</th>\n      <th>vec_avg142</th>\n      <th>vec_avg143</th>\n      <th>vec_avg144</th>\n      <th>vec_avg145</th>\n      <th>vec_avg146</th>\n      <th>vec_avg147</th>\n      <th>vec_avg148</th>\n      <th>vec_avg149</th>\n      <th>vec_avg150</th>\n      <th>vec_avg151</th>\n      <th>vec_avg152</th>\n      <th>vec_avg153</th>\n      <th>vec_avg154</th>\n      <th>vec_avg155</th>\n      <th>vec_avg156</th>\n      <th>vec_avg157</th>\n      <th>vec_avg158</th>\n      <th>vec_avg159</th>\n      <th>vec_avg160</th>\n      <th>vec_avg161</th>\n      <th>vec_avg162</th>\n      <th>vec_avg163</th>\n      <th>vec_avg164</th>\n      <th>vec_avg165</th>\n      <th>vec_avg166</th>\n      <th>vec_avg167</th>\n      <th>vec_avg168</th>\n      <th>vec_avg169</th>\n      <th>vec_avg170</th>\n      <th>vec_avg171</th>\n      <th>vec_avg172</th>\n      <th>vec_avg173</th>\n      <th>vec_avg174</th>\n      <th>vec_avg175</th>\n      <th>vec_avg176</th>\n      <th>vec_avg177</th>\n      <th>vec_avg178</th>\n      <th>vec_avg179</th>\n      <th>vec_avg180</th>\n      <th>vec_avg181</th>\n      <th>vec_avg182</th>\n      <th>vec_avg183</th>\n      <th>vec_avg184</th>\n      <th>vec_avg185</th>\n      <th>vec_avg186</th>\n      <th>vec_avg187</th>\n      <th>vec_avg188</th>\n      <th>vec_avg189</th>\n      <th>vec_avg190</th>\n      <th>vec_avg191</th>\n      <th>vec_avg192</th>\n      <th>vec_avg193</th>\n      <th>vec_avg194</th>\n      <th>vec_avg195</th>\n      <th>vec_avg196</th>\n      <th>vec_avg197</th>\n      <th>vec_avg198</th>\n      <th>vec_avg199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>['breaking', 'armed', 'man', 'takes', 'hostage', 'kosher', 'grocery', 'east', 'paris', '&amp;']</td>\n      <td>-0.245181</td>\n      <td>-0.01027</td>\n      <td>0.116429</td>\n      <td>0.159705</td>\n      <td>-0.100017</td>\n      <td>-0.088065</td>\n      <td>0.242909</td>\n      <td>-0.043744</td>\n      <td>0.086057</td>\n      <td>-0.133616</td>\n      <td>-0.165871</td>\n      <td>0.124254</td>\n      <td>-0.656505</td>\n      <td>0.249567</td>\n      <td>0.171354</td>\n      <td>-0.020197</td>\n      <td>0.048338</td>\n      <td>-0.104669</td>\n      <td>0.016054</td>\n      <td>-0.262492</td>\n      <td>0.097819</td>\n      <td>-0.144589</td>\n      <td>-0.118979</td>\n      <td>-0.075988</td>\n      <td>0.009608</td>\n      <td>0.774872</td>\n      <td>0.189386</td>\n      <td>-0.051733</td>\n      <td>-0.148943</td>\n      <td>-0.029019</td>\n      <td>-0.146035</td>\n      <td>-0.113401</td>\n      <td>0.016945</td>\n      <td>-0.157495</td>\n      <td>0.159048</td>\n      <td>0.358597</td>\n      <td>-0.050946</td>\n      <td>0.079987</td>\n      <td>0.305156</td>\n      <td>-0.110494</td>\n      <td>0.123272</td>\n      <td>0.001231</td>\n      <td>0.016253</td>\n      <td>-0.106919</td>\n      <td>0.030864</td>\n      <td>0.152466</td>\n      <td>0.074397</td>\n      <td>-0.033293</td>\n      <td>-0.058297</td>\n      <td>0.053371</td>\n      <td>-0.50394</td>\n      <td>0.008548</td>\n      <td>-0.090691</td>\n      <td>0.069851</td>\n      <td>-0.23457</td>\n      <td>0.151345</td>\n      <td>-0.198295</td>\n      <td>0.01931</td>\n      <td>-0.06298</td>\n      <td>0.08632</td>\n      <td>0.111909</td>\n      <td>0.095357</td>\n      <td>0.076761</td>\n      <td>-0.008932</td>\n      <td>0.170908</td>\n      <td>-0.052601</td>\n      <td>0.182794</td>\n      <td>0.083385</td>\n      <td>-0.042883</td>\n      <td>-0.031243</td>\n      <td>-0.117652</td>\n      <td>-0.030746</td>\n      <td>0.118263</td>\n      <td>0.065533</td>\n      <td>0.132332</td>\n      <td>0.052125</td>\n      <td>-0.008038</td>\n      <td>-0.022878</td>\n      <td>-0.028015</td>\n      <td>-0.04078</td>\n      <td>0.381744</td>\n      <td>-0.078668</td>\n      <td>0.191174</td>\n      <td>0.14731</td>\n      <td>-0.098038</td>\n      <td>-0.14875</td>\n      <td>-0.117466</td>\n      <td>0.215932</td>\n      <td>-0.029577</td>\n      <td>-0.114086</td>\n      <td>-0.015651</td>\n      <td>-0.02498</td>\n      <td>0.155709</td>\n      <td>0.089995</td>\n      <td>-0.089634</td>\n      <td>0.115466</td>\n      <td>0.040305</td>\n      <td>-0.009728</td>\n      <td>0.013069</td>\n      <td>0.034053</td>\n      <td>0.282391</td>\n      <td>-0.129498</td>\n      <td>-0.119499</td>\n      <td>-0.273537</td>\n      <td>0.178535</td>\n      <td>-0.301966</td>\n      <td>0.080896</td>\n      <td>0.071021</td>\n      <td>0.123112</td>\n      <td>-0.173997</td>\n      <td>0.074667</td>\n      <td>-0.099763</td>\n      <td>0.070493</td>\n      <td>0.01303</td>\n      <td>0.003937</td>\n      <td>-0.235772</td>\n      <td>-0.051655</td>\n      <td>0.141754</td>\n      <td>-0.180339</td>\n      <td>0.307101</td>\n      <td>0.001445</td>\n      <td>-0.023391</td>\n      <td>-0.060489</td>\n      <td>0.103714</td>\n      <td>-0.207719</td>\n      <td>-0.1599</td>\n      <td>0.07702</td>\n      <td>0.156897</td>\n      <td>0.366401</td>\n      <td>0.327158</td>\n      <td>-0.244054</td>\n      <td>0.089152</td>\n      <td>-0.127359</td>\n      <td>-0.480328</td>\n      <td>-0.016111</td>\n      <td>-0.284418</td>\n      <td>0.091102</td>\n      <td>0.058873</td>\n      <td>0.133842</td>\n      <td>-0.068824</td>\n      <td>-0.053446</td>\n      <td>0.227917</td>\n      <td>-0.200994</td>\n      <td>-0.189614</td>\n      <td>-0.152296</td>\n      <td>-0.004471</td>\n      <td>-0.018664</td>\n      <td>-0.084851</td>\n      <td>-0.008542</td>\n      <td>0.144377</td>\n      <td>0.03496</td>\n      <td>-0.080191</td>\n      <td>-3.096044</td>\n      <td>-0.072925</td>\n      <td>0.216484</td>\n      <td>-0.036533</td>\n      <td>-0.013923</td>\n      <td>0.101394</td>\n      <td>0.125695</td>\n      <td>-0.034054</td>\n      <td>-0.160577</td>\n      <td>0.173289</td>\n      <td>0.001495</td>\n      <td>0.011303</td>\n      <td>0.187776</td>\n      <td>-0.171213</td>\n      <td>-0.123493</td>\n      <td>-0.052816</td>\n      <td>0.324482</td>\n      <td>-0.222219</td>\n      <td>-0.24465</td>\n      <td>0.051662</td>\n      <td>-0.038799</td>\n      <td>0.183936</td>\n      <td>-0.046773</td>\n      <td>-0.591883</td>\n      <td>0.137641</td>\n      <td>-0.120051</td>\n      <td>-0.324571</td>\n      <td>0.191564</td>\n      <td>-0.13271</td>\n      <td>-0.163548</td>\n      <td>-0.253983</td>\n      <td>0.091246</td>\n      <td>0.114669</td>\n      <td>0.025024</td>\n      <td>0.130568</td>\n      <td>-0.295191</td>\n      <td>0.089739</td>\n      <td>0.198975</td>\n      <td>0.063809</td>\n      <td>-0.181146</td>\n      <td>0.087133</td>\n      <td>-0.096525</td>\n      <td>-0.092077</td>\n      <td>-0.11412</td>\n      <td>0.123087</td>\n      <td>-0.032704</td>\n      <td>0.242617</td>\n      <td>-0.019656</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                                                                         token  \\\n0  ['breaking', 'armed', 'man', 'takes', 'hostage', 'kosher', 'grocery', 'east', 'paris', '&']   \n\n   vec_avg0  vec_avg1  vec_avg2  vec_avg3  vec_avg4  vec_avg5  vec_avg6  \\\n0 -0.245181  -0.01027  0.116429  0.159705 -0.100017 -0.088065  0.242909   \n\n   vec_avg7  vec_avg8  vec_avg9  vec_avg10  vec_avg11  vec_avg12  vec_avg13  \\\n0 -0.043744  0.086057 -0.133616  -0.165871   0.124254  -0.656505   0.249567   \n\n   vec_avg14  vec_avg15  vec_avg16  vec_avg17  vec_avg18  vec_avg19  \\\n0   0.171354  -0.020197   0.048338  -0.104669   0.016054  -0.262492   \n\n   vec_avg20  vec_avg21  vec_avg22  vec_avg23  vec_avg24  vec_avg25  \\\n0   0.097819  -0.144589  -0.118979  -0.075988   0.009608   0.774872   \n\n   vec_avg26  vec_avg27  vec_avg28  vec_avg29  vec_avg30  vec_avg31  \\\n0   0.189386  -0.051733  -0.148943  -0.029019  -0.146035  -0.113401   \n\n   vec_avg32  vec_avg33  vec_avg34  vec_avg35  vec_avg36  vec_avg37  \\\n0   0.016945  -0.157495   0.159048   0.358597  -0.050946   0.079987   \n\n   vec_avg38  vec_avg39  vec_avg40  vec_avg41  vec_avg42  vec_avg43  \\\n0   0.305156  -0.110494   0.123272   0.001231   0.016253  -0.106919   \n\n   vec_avg44  vec_avg45  vec_avg46  vec_avg47  vec_avg48  vec_avg49  \\\n0   0.030864   0.152466   0.074397  -0.033293  -0.058297   0.053371   \n\n   vec_avg50  vec_avg51  vec_avg52  vec_avg53  vec_avg54  vec_avg55  \\\n0   -0.50394   0.008548  -0.090691   0.069851   -0.23457   0.151345   \n\n   vec_avg56  vec_avg57  vec_avg58  vec_avg59  vec_avg60  vec_avg61  \\\n0  -0.198295    0.01931   -0.06298    0.08632   0.111909   0.095357   \n\n   vec_avg62  vec_avg63  vec_avg64  vec_avg65  vec_avg66  vec_avg67  \\\n0   0.076761  -0.008932   0.170908  -0.052601   0.182794   0.083385   \n\n   vec_avg68  vec_avg69  vec_avg70  vec_avg71  vec_avg72  vec_avg73  \\\n0  -0.042883  -0.031243  -0.117652  -0.030746   0.118263   0.065533   \n\n   vec_avg74  vec_avg75  vec_avg76  vec_avg77  vec_avg78  vec_avg79  \\\n0   0.132332   0.052125  -0.008038  -0.022878  -0.028015   -0.04078   \n\n   vec_avg80  vec_avg81  vec_avg82  vec_avg83  vec_avg84  vec_avg85  \\\n0   0.381744  -0.078668   0.191174    0.14731  -0.098038   -0.14875   \n\n   vec_avg86  vec_avg87  vec_avg88  vec_avg89  vec_avg90  vec_avg91  \\\n0  -0.117466   0.215932  -0.029577  -0.114086  -0.015651   -0.02498   \n\n   vec_avg92  vec_avg93  vec_avg94  vec_avg95  vec_avg96  vec_avg97  \\\n0   0.155709   0.089995  -0.089634   0.115466   0.040305  -0.009728   \n\n   vec_avg98  vec_avg99  vec_avg100  vec_avg101  vec_avg102  vec_avg103  \\\n0   0.013069   0.034053    0.282391   -0.129498   -0.119499   -0.273537   \n\n   vec_avg104  vec_avg105  vec_avg106  vec_avg107  vec_avg108  vec_avg109  \\\n0    0.178535   -0.301966    0.080896    0.071021    0.123112   -0.173997   \n\n   vec_avg110  vec_avg111  vec_avg112  vec_avg113  vec_avg114  vec_avg115  \\\n0    0.074667   -0.099763    0.070493     0.01303    0.003937   -0.235772   \n\n   vec_avg116  vec_avg117  vec_avg118  vec_avg119  vec_avg120  vec_avg121  \\\n0   -0.051655    0.141754   -0.180339    0.307101    0.001445   -0.023391   \n\n   vec_avg122  vec_avg123  vec_avg124  vec_avg125  vec_avg126  vec_avg127  \\\n0   -0.060489    0.103714   -0.207719     -0.1599     0.07702    0.156897   \n\n   vec_avg128  vec_avg129  vec_avg130  vec_avg131  vec_avg132  vec_avg133  \\\n0    0.366401    0.327158   -0.244054    0.089152   -0.127359   -0.480328   \n\n   vec_avg134  vec_avg135  vec_avg136  vec_avg137  vec_avg138  vec_avg139  \\\n0   -0.016111   -0.284418    0.091102    0.058873    0.133842   -0.068824   \n\n   vec_avg140  vec_avg141  vec_avg142  vec_avg143  vec_avg144  vec_avg145  \\\n0   -0.053446    0.227917   -0.200994   -0.189614   -0.152296   -0.004471   \n\n   vec_avg146  vec_avg147  vec_avg148  vec_avg149  vec_avg150  vec_avg151  \\\n0   -0.018664   -0.084851   -0.008542    0.144377     0.03496   -0.080191   \n\n   vec_avg152  vec_avg153  vec_avg154  vec_avg155  vec_avg156  vec_avg157  \\\n0   -3.096044   -0.072925    0.216484   -0.036533   -0.013923    0.101394   \n\n   vec_avg158  vec_avg159  vec_avg160  vec_avg161  vec_avg162  vec_avg163  \\\n0    0.125695   -0.034054   -0.160577    0.173289    0.001495    0.011303   \n\n   vec_avg164  vec_avg165  vec_avg166  vec_avg167  vec_avg168  vec_avg169  \\\n0    0.187776   -0.171213   -0.123493   -0.052816    0.324482   -0.222219   \n\n   vec_avg170  vec_avg171  vec_avg172  vec_avg173  vec_avg174  vec_avg175  \\\n0    -0.24465    0.051662   -0.038799    0.183936   -0.046773   -0.591883   \n\n   vec_avg176  vec_avg177  vec_avg178  vec_avg179  vec_avg180  vec_avg181  \\\n0    0.137641   -0.120051   -0.324571    0.191564    -0.13271   -0.163548   \n\n   vec_avg182  vec_avg183  vec_avg184  vec_avg185  vec_avg186  vec_avg187  \\\n0   -0.253983    0.091246    0.114669    0.025024    0.130568   -0.295191   \n\n   vec_avg188  vec_avg189  vec_avg190  vec_avg191  vec_avg192  vec_avg193  \\\n0    0.089739    0.198975    0.063809   -0.181146    0.087133   -0.096525   \n\n   vec_avg194  vec_avg195  vec_avg196  vec_avg197  vec_avg198  vec_avg199  \n0   -0.092077    -0.11412    0.123087   -0.032704    0.242617   -0.019656  "
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_RHI.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"./data/_PHEME_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5802, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>Event</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1080</th>\n      <td>#ICYMI, Christopher Hitchens on the case for m...</td>\n      <td>charliehebdo</td>\n    </tr>\n    <tr>\n      <th>4120</th>\n      <td>DEVELOPING NEWS: Soldier shot at War Memorial....</td>\n      <td>ottawashooting</td>\n    </tr>\n    <tr>\n      <th>5004</th>\n      <td>RT @tomsteinfort: Terrifying photo of hostages...</td>\n      <td>sydneysiege</td>\n    </tr>\n    <tr>\n      <th>2305</th>\n      <td>#Ferguson chief said the officer was unaware o...</td>\n      <td>ferguson</td>\n    </tr>\n    <tr>\n      <th>4898</th>\n      <td>BREAKING: 2 people have run out of Sydney buil...</td>\n      <td>sydneysiege</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                                   text           Event\n1080  #ICYMI, Christopher Hitchens on the case for m...    charliehebdo\n4120  DEVELOPING NEWS: Soldier shot at War Memorial....  ottawashooting\n5004  RT @tomsteinfort: Terrifying photo of hostages...     sydneysiege\n2305  #Ferguson chief said the officer was unaware o...        ferguson\n4898  BREAKING: 2 people have run out of Sydney buil...     sydneysiege"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(raw_data.shape)\n",
    "raw_data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = []\n",
    "\n",
    "for sent in raw_data.text:\n",
    "    sent = re.sub(r\"http\\S+\", \"&\", sent)\n",
    "    sent = re.sub(r\"@\\S+\", \"@\", sent)\n",
    "    sent = re.sub(r\"#\\S+\", \"#\", sent)\n",
    "    sent = re.sub(r'([^\\s\\w@#&]|_)+','', sent)\n",
    "    sent = re.sub('','', sent.lower())\n",
    "    # print(tweet_tokenizer.tokenize(sent))\n",
    "    sent = [tweet_tokenizer.tokenize(sent)]\n",
    "    # sent = [tweet_tokenizer.tokenize(sent.lower())]\n",
    "    tweet_tokens.append(sent)\n",
    "    # tweet_tokens.append(tweet_tokenizer.tokenize(sent))\n",
    "df_tokens = pd.DataFrame(tweet_tokens, columns=['token'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[breaking, armed, man, takes, hostage, in, kosher, grocery, east, of, paris, &amp;]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[#, killers, dead, confirmed, by, gendarmerie]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[top, french, cartoonists, charb, cabu, wolinski, tignous, confirmed, among, dead, in, #, #, attack, editor, is, critically, wounded]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[police, have, surrounded, the, area, where, the, #, attack, suspects, are, believed, to, be, &amp;, &amp;]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[photo, armed, gunmen, face, police, officers, near, #, hq, in, paris, &amp;, &amp;]</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                                                                                                                   token\n0                                                        [breaking, armed, man, takes, hostage, in, kosher, grocery, east, of, paris, &]\n1                                                                                         [#, killers, dead, confirmed, by, gendarmerie]\n2  [top, french, cartoonists, charb, cabu, wolinski, tignous, confirmed, among, dead, in, #, #, attack, editor, is, critically, wounded]\n3                                    [police, have, surrounded, the, area, where, the, #, attack, suspects, are, believed, to, be, &, &]\n4                                                           [photo, armed, gunmen, face, police, officers, near, #, hq, in, paris, &, &]"
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Pretrained model for Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching pretrained Model and Convert the Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "# corpus = api.load('text8')\n",
    "# wv = api.load('word2vec-google-news-300')\n",
    "# fasttext-wiki-news-subwords-300'\n",
    "#  'glove-twitter-200',\n",
    "model = api.load('glove-twitter-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7fa0739797f0>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import inspect\n",
    "# print(inspect.getsource(wv.__class__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_object = model.wv\n",
    "w2v_vectors = w2v_object.vectors # here you load vectors for each word in your model\n",
    "w2v_indices = {word: w2v_object.vocab[word].index for word in w2v_object.vocab} # here you load indices - with whom you can find an index of the particular word in your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(line): \n",
    "    words = []\n",
    "    for word in line: # line - iterable, for example list of tokens \n",
    "        try:\n",
    "            w2v_idx = w2v_indices[word]\n",
    "        except KeyError: # if you does not have a vector for this word in your w2v model, continue \n",
    "            words.append(list(np.zeros(200,)))\n",
    "            continue\n",
    "        words.append(list(w2v_vectors[w2v_idx]))\n",
    "        if not word:\n",
    "            words.append(None)\n",
    "\n",
    "        if len(line) > len(words):\n",
    "            continue\n",
    "    return np.asarray(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Tweet 1: \", raw_data['text'][1])\n",
    "# print(\"Indice of '{}': {}\".format(df_tokens['token'][1][0], w2v_indices[df_tokens['token'][1][0]]))\n",
    "# # print(\"Indice of '{}': {}\".format(raw_data['text_token'][1][0], w2v_vectors[w2v_indices[raw_data['text_token'][1][0]]]))\n",
    "# # print(\"Indice of '{}': {}\".format(raw_data['text_token'][1][1], w2v_indices[raw_data['text_token'][1][1]]))\n",
    "# # print(\"Indice of '{}': {}\".format(raw_data['text_token'][1][1], w2v_vectors[w2v_indices[raw_data['text_token'][1][1]]]))\n",
    "# # print(\"\\nVector of the first headline:\\n\", vectorize(raw_data['text_token'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "df_tokens['token_vec'] = copy.deepcopy(df_tokens['token'])\n",
    "\n",
    "for index, sent in enumerate(df_tokens['token_vec']):\n",
    "    df_tokens['token_vec'][index] = vectorize(sent).mean(axis=0)\n",
    "\n",
    "# df_test[['text_token','text_token_vec']].head()\n",
    "\n",
    "df_temp = pd.DataFrame(df_tokens['token_vec'].values.tolist()).add_prefix('vec_avg')\n",
    "\n",
    "df_tokens = df_tokens.join(df_temp).drop('token_vec',axis=1)\n",
    "# df_test.drop('text_token_vec',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fc4a52621661>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "df_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rosetta",
   "language": "python",
   "name": "rosetta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}