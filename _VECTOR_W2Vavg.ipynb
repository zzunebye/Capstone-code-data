{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "## Options:\n",
    "### Pretrained model:\n",
    "- word2vec-ruscorpora-300\t\n",
    "- glove-twitter-200\n",
    "\n",
    "### Tokenization: How to manage OOV?\n",
    "- Link: '&'\n",
    "- Tag: '#'\n",
    "- Mention: '@' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import SnowballStemmer\n",
    "\n",
    "import gensim\n",
    "import gensim.models.word2vec as w2v\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from fetchData import fetchdata, cv_events\n",
    "import __MLP\n",
    "import __Preprocessing\n",
    "# from __MLP import getSamplers, convert_df_to_unsqueezed_tensor, train_sequential, clf_report\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "# from __Preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 400)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load('glove-twitter-200')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ! Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Replaces contractions from a string to their equivalents \"\"\"\n",
    "def replaceContraction(text):\n",
    "    contraction_patterns = [ (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "                            (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not') ]\n",
    "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
    "    for (pattern, repl) in patterns:\n",
    "        (text, count) = re.subn(pattern, repl, text)\n",
    "    return text\n",
    "\n",
    "def getTokenization(raw_data):\n",
    "\n",
    "    lmt = WordNetLemmatizer()\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    freqdist = nltk.FreqDist()\n",
    "    tweet_tokenizer = TweetTokenizer()\n",
    "    tweet_tokens = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    for sent in raw_data.text:\n",
    "\n",
    "        # 기존\n",
    "        sent = replaceContraction(sent)\n",
    "        sent = re.sub(r\"(www\\S+)|(http\\S+)\", \"HTTPURL\", sent)\n",
    "        sent = re.sub(r\"[A-Za-z0-9]+\", \"@USE\", sent)\n",
    "        sent = re.sub(r\"(#)(\\S+)\", r'\\1 \\2', sent)\n",
    "        sent = re.sub(r'([^\\s\\w@#&]|_)+', '', sent)\n",
    "\n",
    "        # sent = re.sub('@[^\\s]+','atUser',sent)\n",
    "        # sent = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','url',sent)\n",
    "\n",
    "        # sent = re.sub('', '', sent.lower())\n",
    "        # sent = [tweet_tokenizer.tokenize(sent)]\n",
    "        sent = tweet_tokenizer.tokenize(sent.lower())\n",
    "        # sent = [stemmer.stem(token) for token in sent]\n",
    "        # sent = [lmt.lemmatize(token) for token in sent]\n",
    "\n",
    "        temp = [token for token in sent if not token in stop_words]\n",
    "        tweet_tokens.append([temp])\n",
    "        # tweet_tokens.append(tweet_tokenizer.tokenize(sent))\n",
    "    df_tokens = pd.DataFrame(tweet_tokens, columns=['token'])\n",
    "    return df_tokens\n",
    "\n",
    "def getTokenization_less(raw_data):\n",
    "\n",
    "    lmt = WordNetLemmatizer()\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    freqdist = nltk.FreqDist()\n",
    "    tweet_tokenizer = TweetTokenizer()\n",
    "    tweet_tokens = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    for sent in raw_data.text:\n",
    "\n",
    "        # 기존\n",
    "        sent = re.sub(r\"(www\\S+)|(http\\S+)\", \"*\", sent)\n",
    "        sent = re.sub(r\"@\\S+\", \"@\", sent)\n",
    "        sent = re.sub(r\"(#)(\\S+)\", r'\\1 \\2', sent)\n",
    "        sent = re.sub(r'([^\\s\\w@#&]|_)+', '', sent)\n",
    "\n",
    "        # sent = re.sub('@[^\\s]+','atUser',sent)\n",
    "        # sent = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','url',sent)\n",
    "\n",
    "        # sent = re.sub('', '', sent.lower())\n",
    "        # sent = [tweet_tokenizer.tokenize(sent)]\n",
    "        sent = replaceContraction(sent)\n",
    "        sent = tweet_tokenizer.tokenize(sent.lower())\n",
    "        # sent = [stemmer.stem(token) for token in sent]\n",
    "        # sent = [lmt.lemmatize(token) for token in sent]\n",
    "\n",
    "        temp = [token for token in sent if not token in stop_words]\n",
    "        tweet_tokens.append([temp])\n",
    "        # tweet_tokens.append(tweet_tokenizer.tokenize(sent))\n",
    "    df_tokens = pd.DataFrame(tweet_tokens, columns=['token'])\n",
    "    return df_tokens\n",
    "\n",
    "def getObjectW2V(model):\n",
    "    w2v_object = model.wv\n",
    "    w2v_vectors = w2v_object.vectors # here you load vectors for each word in your model\n",
    "    w2v_indices = {word: w2v_object.vocab[word].index for word in w2v_object.vocab} # here you load indices - with whom you can find an index of the particular word in your model \n",
    "    return w2v_object, w2v_vectors, w2v_indices\n",
    "\n",
    "def get_W2V_AVG(df_tokens):\n",
    "    import copy\n",
    "    df_tokens['token_vec'] = copy.deepcopy(df_tokens['token'])\n",
    "    \n",
    "\n",
    "    for index, sent in enumerate(df_tokens['token_vec']):\n",
    "        df_tokens['token_vec'][index] = vectorize(sent).mean(axis=0)\n",
    "\n",
    "    df_temp = pd.DataFrame(\n",
    "        df_tokens['token_vec'].values.tolist()).add_prefix('vec_avg')\n",
    "\n",
    "    # df_tokens = df_tokens.join(df_temp).drop('token_vec', axis=1)\n",
    "    df_temp = pd.DataFrame(df_tokens['token_vec'].values.tolist()).add_prefix('vec_avg')\n",
    "    df_tokens = df_tokens.join(df_temp).drop('token_vec', axis=1)\n",
    "    # df_test.drop('text_token_vec',axis=1, inplace=True)\n",
    "\n",
    "    return df_tokens\n",
    "    # return pd.DataFrame(df_tokens)\n",
    "\n",
    "def vectorize(line):\n",
    "    words = []\n",
    "    for word in line:  # line - iterable, for example list of tokens\n",
    "        try:\n",
    "            w2v_idx = w2v_indices[word]\n",
    "        except KeyError:  # if you does not have a vector for this word in your w2v model, continue\n",
    "            words.append(list(np.zeros(200,)))\n",
    "            continue\n",
    "        words.append(list(w2v_vectors[w2v_idx]))\n",
    "        if not word:\n",
    "            words.append(None)\n",
    "\n",
    "        if len(line) > len(words):\n",
    "            continue\n",
    "    return np.asarray(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ! COVERSION: PHEME | PHEMEext | RHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_RHI = pd.read_csv(\"./data/_RHI_text.csv\")\n",
    "raw_PHEME = pd.read_csv(\"./data/_PHEME_text.csv\")\n",
    "raw_PHEMEext = pd.read_csv(\"./data/_PHEMEext_text.csv\")\n",
    "\n",
    "datasets = [raw_PHEME, raw_PHEMEext]\n",
    "\n",
    "# for i, dataset in enumerate(datasets): datasets[i] = getTokenization(dataset)\n",
    "datasets = [getTokenization_less(dataset) for dataset in datasets]\n",
    "# datasets = [get_W2V_AVG(dataset) for dataset in datasets]\n",
    "# w2v_object, w2v_vectors, w2v_indices = getObjectW2V(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[micheal, essien, denying, ebola, rumours, like]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[truth, internet, rumours, contracted, ebolai, well, andamp, im, gud, andamp, training, usual, tomorrow, #, falsenews]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[essien, lawyers, considering, file, lawsuit, nigerian, media, reported, fake, ebola, story]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[good, news, rumours, michael, essien, contracted, ebola, virus, false]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[milan, stated, reports, essien, ebola, completely, false, @]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>618</th>\n      <td>[franz, marc, horses, update, #, gurlitt, nazitainted, #, art, trove, via, @]</td>\n    </tr>\n    <tr>\n      <th>619</th>\n      <td>[munich, district, court, confirmed, application, cornelius, gurlitts, cousin, certificate, inheritance, details, today]</td>\n    </tr>\n    <tr>\n      <th>620</th>\n      <td>[gurlitt, collection, go, many, people, sounding, upcoming, press, conference, monday]</td>\n    </tr>\n    <tr>\n      <th>621</th>\n      <td>[possible, nazi, art, transfer, riles, jewish, groups, course, transfer, gurlitt, trove, swiss, museum]</td>\n    </tr>\n    <tr>\n      <th>622</th>\n      <td>[gurlitt, collection, sold, benefit, jewish, organisations, art, newspaper]</td>\n    </tr>\n  </tbody>\n</table>\n<p>623 rows × 1 columns</p>\n</div>",
      "text/plain": "                                                                                                                        token\n0                                                                            [micheal, essien, denying, ebola, rumours, like]\n1      [truth, internet, rumours, contracted, ebolai, well, andamp, im, gud, andamp, training, usual, tomorrow, #, falsenews]\n2                                [essien, lawyers, considering, file, lawsuit, nigerian, media, reported, fake, ebola, story]\n3                                                     [good, news, rumours, michael, essien, contracted, ebola, virus, false]\n4                                                               [milan, stated, reports, essien, ebola, completely, false, @]\n..                                                                                                                        ...\n618                                             [franz, marc, horses, update, #, gurlitt, nazitainted, #, art, trove, via, @]\n619  [munich, district, court, confirmed, application, cornelius, gurlitts, cousin, certificate, inheritance, details, today]\n620                                    [gurlitt, collection, go, many, people, sounding, upcoming, press, conference, monday]\n621                   [possible, nazi, art, transfer, riles, jewish, groups, course, transfer, gurlitt, trove, swiss, museum]\n622                                               [gurlitt, collection, sold, benefit, jewish, organisations, art, newspaper]\n\n[623 rows x 1 columns]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T&S+5 RT @CharlesMBlow AlaskaReport: \"Todd and Sarah Palin to divorce: Affairs on both sides\" Can this be true? http://tinyurl.com/nt6gfs\n"
     ]
    }
   ],
   "source": [
    "for i in raw_RHI.text:\n",
    "    if (i.find(\"&\") == True):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [get_W2V_AVG(dataset) for dataset in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = get_W2V_AVG(datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[0].to_csv('./data/_RHI_text_AVGw2v.csv', index = False)\n",
    "datasets[1].to_csv('./data/_PHEME_text_AVGw2v.csv', index = False)\n",
    "datasets[2].to_csv('./data/_PHEMEext_text_AVGw2v.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 알아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"./data/_PHEME_text.csv\")\n",
    "raw_RHI = pd.read_csv(\"./data/_RHI_text.csv\")\n",
    "\n",
    "lmt = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "freqdist = nltk.FreqDist()\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\"\"\" Replaces contractions from a string to their equivalents \"\"\"\n",
    "contraction_patterns = [ (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "                         (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not') ]\n",
    "def replaceContraction(text):\n",
    "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
    "    for (pattern, repl) in patterns:\n",
    "        (text, count) = re.subn(pattern, repl, text)\n",
    "    return text\n",
    "\n",
    "for sent in raw_data.text:\n",
    "\n",
    "    sent = re.sub(r\"http\\S+\", \"&\", sent)\n",
    "    # sent = re.sub(r\"@\\S+\", \"@\", sent)\n",
    "    sent = re.sub(r\"(#)(\\S+)\", r'\\1 \\2', sent)\n",
    "\n",
    "    sent = re.sub(r'([^\\s\\w@#&]|_)+', '', sent)\n",
    "    sent = re.sub('@[^\\s]+','atUser',sent)\n",
    "    # sent = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','url',sent)\n",
    "    # sent = re.sub(r'#([^\\s]+)', r'\\1', sent)\n",
    "\n",
    "    sent = replaceContraction(sent)\n",
    "\n",
    "    # sent = re.sub('', '', sent.lower())\n",
    "    # print(tweet_tokenizer.tokenize(sent))\n",
    "    # sent = [tweet_tokenizer.tokenize(sent)]\n",
    "    sent = tweet_tokenizer.tokenize(sent.lower())\n",
    "    sent = [stemmer.stem(token) for token in sent]\n",
    "    # sent = [lmt.lemmatize(token) for token in sent]\n",
    "\n",
    "    temp = [token for token in sent if not token in stop_words]\n",
    "    tweet_tokens.append([temp])\n",
    "    # tweet_tokens.append(tweet_tokenizer.tokenize(sent))\n",
    "df_tokens = pd.DataFrame(tweet_tokens, columns=['token'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"./data/_PHEME_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5802, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>Event</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1080</th>\n      <td>#ICYMI, Christopher Hitchens on the case for m...</td>\n      <td>charliehebdo</td>\n    </tr>\n    <tr>\n      <th>4120</th>\n      <td>DEVELOPING NEWS: Soldier shot at War Memorial....</td>\n      <td>ottawashooting</td>\n    </tr>\n    <tr>\n      <th>5004</th>\n      <td>RT @tomsteinfort: Terrifying photo of hostages...</td>\n      <td>sydneysiege</td>\n    </tr>\n    <tr>\n      <th>2305</th>\n      <td>#Ferguson chief said the officer was unaware o...</td>\n      <td>ferguson</td>\n    </tr>\n    <tr>\n      <th>4898</th>\n      <td>BREAKING: 2 people have run out of Sydney buil...</td>\n      <td>sydneysiege</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                                   text           Event\n1080  #ICYMI, Christopher Hitchens on the case for m...    charliehebdo\n4120  DEVELOPING NEWS: Soldier shot at War Memorial....  ottawashooting\n5004  RT @tomsteinfort: Terrifying photo of hostages...     sydneysiege\n2305  #Ferguson chief said the officer was unaware o...        ferguson\n4898  BREAKING: 2 people have run out of Sydney buil...     sydneysiege"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(raw_data.shape)\n",
    "raw_data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = []\n",
    "\n",
    "for sent in raw_data.text:\n",
    "    sent = re.sub(r\"http\\S+\", \"&\", sent)\n",
    "    sent = re.sub(r\"@\\S+\", \"@\", sent)\n",
    "    sent = re.sub(r\"#\\S+\", \"#\", sent)\n",
    "    sent = re.sub(r'([^\\s\\w@#&]|_)+','', sent)\n",
    "    sent = re.sub('','', sent.lower())\n",
    "    # print(tweet_tokenizer.tokenize(sent))\n",
    "    sent = [tweet_tokenizer.tokenize(sent)]\n",
    "    # sent = [tweet_tokenizer.tokenize(sent.lower())]\n",
    "    tweet_tokens.append(sent)\n",
    "    # tweet_tokens.append(tweet_tokenizer.tokenize(sent))\n",
    "df_tokens = pd.DataFrame(tweet_tokens, columns=['token'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[breaking, armed, man, takes, hostage, in, kosher, grocery, east, of, paris, &amp;]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[#, killers, dead, confirmed, by, gendarmerie]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[top, french, cartoonists, charb, cabu, wolinski, tignous, confirmed, among, dead, in, #, #, attack, editor, is, critically, wounded]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[police, have, surrounded, the, area, where, the, #, attack, suspects, are, believed, to, be, &amp;, &amp;]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[photo, armed, gunmen, face, police, officers, near, #, hq, in, paris, &amp;, &amp;]</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                                                                                                                   token\n0                                                        [breaking, armed, man, takes, hostage, in, kosher, grocery, east, of, paris, &]\n1                                                                                         [#, killers, dead, confirmed, by, gendarmerie]\n2  [top, french, cartoonists, charb, cabu, wolinski, tignous, confirmed, among, dead, in, #, #, attack, editor, is, critically, wounded]\n3                                    [police, have, surrounded, the, area, where, the, #, attack, suspects, are, believed, to, be, &, &]\n4                                                           [photo, armed, gunmen, face, police, officers, near, #, hq, in, paris, &, &]"
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Pretrained model for Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching pretrained Model and Convert the Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "# corpus = api.load('text8')\n",
    "# wv = api.load('word2vec-google-news-300')\n",
    "# fasttext-wiki-news-subwords-300'\n",
    "#  'glove-twitter-200',\n",
    "model = api.load('glove-twitter-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7fa0739797f0>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import inspect\n",
    "# print(inspect.getsource(wv.__class__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_object = model.wv\n",
    "w2v_vectors = w2v_object.vectors # here you load vectors for each word in your model\n",
    "w2v_indices = {word: w2v_object.vocab[word].index for word in w2v_object.vocab} # here you load indices - with whom you can find an index of the particular word in your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(line): \n",
    "    words = []\n",
    "    for word in line: # line - iterable, for example list of tokens \n",
    "        try:\n",
    "            w2v_idx = w2v_indices[word]\n",
    "        except KeyError: # if you does not have a vector for this word in your w2v model, continue \n",
    "            words.append(list(np.zeros(200,)))\n",
    "            continue\n",
    "        words.append(list(w2v_vectors[w2v_idx]))\n",
    "        if not word:\n",
    "            words.append(None)\n",
    "\n",
    "        if len(line) > len(words):\n",
    "            continue\n",
    "    return np.asarray(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Tweet 1: \", raw_data['text'][1])\n",
    "# print(\"Indice of '{}': {}\".format(df_tokens['token'][1][0], w2v_indices[df_tokens['token'][1][0]]))\n",
    "# # print(\"Indice of '{}': {}\".format(raw_data['text_token'][1][0], w2v_vectors[w2v_indices[raw_data['text_token'][1][0]]]))\n",
    "# # print(\"Indice of '{}': {}\".format(raw_data['text_token'][1][1], w2v_indices[raw_data['text_token'][1][1]]))\n",
    "# # print(\"Indice of '{}': {}\".format(raw_data['text_token'][1][1], w2v_vectors[w2v_indices[raw_data['text_token'][1][1]]]))\n",
    "# # print(\"\\nVector of the first headline:\\n\", vectorize(raw_data['text_token'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "df_tokens['token_vec'] = copy.deepcopy(df_tokens['token'])\n",
    "\n",
    "for index, sent in enumerate(df_tokens['token_vec']):\n",
    "    df_tokens['token_vec'][index] = vectorize(sent).mean(axis=0)\n",
    "\n",
    "# df_test[['text_token','text_token_vec']].head()\n",
    "\n",
    "df_temp = pd.DataFrame(df_tokens['token_vec'].values.tolist()).add_prefix('vec_avg')\n",
    "\n",
    "df_tokens = df_tokens.join(df_temp).drop('token_vec',axis=1)\n",
    "# df_test.drop('text_token_vec',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fc4a52621661>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "df_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rosetta",
   "language": "python",
   "name": "rosetta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}