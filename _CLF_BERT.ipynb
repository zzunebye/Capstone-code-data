{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzunebye/Capstone-code-data/blob/main/_CLF_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQSdijCsmtPH",
        "outputId": "194e4d67-6b6f-4247-f51c-93f2e1fefc07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "4FaLWxNckcCp",
        "outputId": "631efc0a-9468-478e-9910-c8986bc8d05b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/MyDrive/My Drive/Capstone/code_data\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-1836b0b99c66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 실행시 등장하는 URL을 클릭하여 허용해주면 인증KEY가 나타난다. 복사하여 URL아래 빈칸에 붙여넣으면 마운트에 성공하게된다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./MyDrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;34m': timeout during initial read of root folder; for more info: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             'https://research.google.com/colaboratory/faq.html#drive-timeout')\n\u001b[0;32m--> 256\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m       \u001b[0;31m# Not already authorized, so do the authorization dance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "\n",
        "# 실행시 등장하는 URL을 클릭하여 허용해주면 인증KEY가 나타난다. 복사하여 URL아래 빈칸에 붙여넣으면 마운트에 성공하게된다.\n",
        "from google.colab import drive\n",
        "drive.mount('./MyDrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTiePp9YlA8R",
        "outputId": "8afeef62-c6b8-4425-f66d-d5c78e5ef155"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 107] Transport endpoint is not connected: 'MyDrive/MyDrive/Capstone/code_data'\n",
            "/content/MyDrive/MyDrive/Capstone/code_data\n"
          ]
        }
      ],
      "source": [
        "cd MyDrive/MyDrive/Capstone/code_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ml-UXy1nkI_x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAXZNl6bmM7J",
        "outputId": "9331ff98-909b-4c74-df65-b2f9f7e90be6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ls: cannot open directory '.': Transport endpoint is not connected\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtIMlTiMUFzS"
      },
      "source": [
        "# TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adkKLRH4kI_7",
        "outputId": "b2382fb4-6ff7-4030-842a-d1e15e6bc565"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "# Uncomment to download \"stopwords\"\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def text_preprocessing(s):\n",
        "    \"\"\"\n",
        "    - Lowercase the sentence\n",
        "    - Change \"'t\" to \"not\"\n",
        "    - Remove \"@name\"\n",
        "    - Isolate and remove punctuations except \"?\"\n",
        "    - Remove other special characters\n",
        "    - Remove stop words except \"not\" and \"can\"\n",
        "    - Remove trailing whitespace\n",
        "    \"\"\"\n",
        "    s = s.lower()\n",
        "    # Change 't to 'not'\n",
        "    s = re.sub(r\"\\'t\", \" not\", s)\n",
        "    # Remove @name\n",
        "    s = re.sub(r'(@.*?)[\\s]', ' ', s)\n",
        "    # Isolate and remove punctuations except '?'\n",
        "    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', s)\n",
        "    s = re.sub(r'[^\\w\\s\\?]', ' ', s)\n",
        "    # Remove some special characters\n",
        "    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n",
        "    # Remove stopwords except 'not' and 'can'\n",
        "    s = \" \".join([word for word in s.split()\n",
        "                  if word not in stopwords.words('english')\n",
        "                  or word in ['not', 'can']])\n",
        "    # Remove trailing whitespace\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    \n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YahIdy3XkI_9",
        "outputId": "0e75a5bd-c64c-4394-a1c0-709ae6ef0ccc"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-838a357a3484>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'from sklearn.feature_extraction.text import TfidfVectorizer\\n\\n# Preprocess text\\nX_train_preprocessed = np.array([text_preprocessing(text) for text in X])\\nX_val_preprocessed = np.array([text_preprocessing(text) for text in test_X])\\n\\n# Calculate TF-IDF\\ntf_idf = TfidfVectorizer(ngram_range=(1, 3),\\n                         binary=True,\\n                         smooth_idf=False)\\nX_train_tfidf = tf_idf.fit_transform(X_train_preprocessed)\\nX_val_tfidf = tf_idf.transform(X_val_preprocessed)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-88ed875ee9d6>\u001b[0m in \u001b[0;36mtext_preprocessing\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([\\;\\:\\|•«\\n])'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Remove stopwords except 'not' and 'can'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     s = \" \".join([word for word in s.split()\n\u001b[0m\u001b[1;32m     28\u001b[0m                   \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                   or word in ['not', 'can']])\n",
            "\u001b[0;32m<ipython-input-19-88ed875ee9d6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Remove stopwords except 'not' and 'can'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     s = \" \".join([word for word in s.split()\n\u001b[0;32m---> 28\u001b[0;31m                   \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                   or word in ['not', 'can']])\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Remove trailing whitespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0m\u001b[1;32m     23\u001b[0m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0;32m---> 23\u001b[0;31m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Preprocess text\n",
        "X_train_preprocessed = np.array([text_preprocessing(text) for text in X])\n",
        "X_val_preprocessed = np.array([text_preprocessing(text) for text in test_X])\n",
        "\n",
        "# Calculate TF-IDF\n",
        "tf_idf = TfidfVectorizer(ngram_range=(1, 3),\n",
        "                         binary=True,\n",
        "                         smooth_idf=False)\n",
        "X_train_tfidf = tf_idf.fit_transform(X_train_preprocessed)\n",
        "X_val_tfidf = tf_idf.transform(X_val_preprocessed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86QdYvNrkI_9",
        "outputId": "4884b136-aa77-410d-f6e9-e0e201376b02"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-6ca3cc28359f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_train_tfidf.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLOxl1JDkI_9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "def get_auc_CV(model):\n",
        "    \"\"\"\n",
        "    Return the average AUC score from cross-validation.\n",
        "    \"\"\"\n",
        "    # Set KFold to shuffle data before the split\n",
        "    kf = StratifiedKFold(5, shuffle=True, random_state=1)\n",
        "\n",
        "    # Get AUC scores\n",
        "    auc = cross_val_score(\n",
        "        model, X_train_tfidf, y_train, scoring=\"roc_auc\", cv=kf)\n",
        "\n",
        "    return auc.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "3Bi1JS2IkI_9",
        "outputId": "6c986067-c6ca-4f99-9732-d2302aed588e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best alpha:  1.0\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "res = pd.Series([get_auc_CV(MultinomialNB(i))\n",
        "                 for i in np.arange(1, 10, 0.1)],\n",
        "                index=np.arange(1, 10, 0.1))\n",
        "\n",
        "best_alpha = np.round(res.idxmax(), 2)\n",
        "print('Best alpha: ', best_alpha)\n",
        "\n",
        "plt.plot(res)\n",
        "plt.title('AUC vs. Alpha')\n",
        "plt.xlabel('Alpha')\n",
        "plt.ylabel('AUC')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "PZi8GU6dkI_-",
        "outputId": "eae6e8c0-0ffe-49aa-922c-ab0ab749b5e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.9015\n",
            "Accuracy: 82.10%\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Compute predicted probabilities\n",
        "nb_model = MultinomialNB(alpha=1.8)\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "probs = nb_model.predict_proba(X_val_tfidf)\n",
        "\n",
        "# Evaluate the classifier\n",
        "evaluate_roc(probs, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "MKMBotb3lbfB",
        "outputId": "53155806-9c82-4a9b-b1d3-19917cce2562"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.5743\n",
            "Accuracy: 50.00%\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Compute predicted probabilities\n",
        "\n",
        "# X_data = pd.concat([X_train_tfidf, X_val_tfidf], axis=0)\n",
        "# y_data = pd.concat([X_train_tfidf, X_val_tfidf], axis=0)\n",
        "\n",
        "X_test = test_data.text.values\n",
        "y_test = test_data.isRumor.values\n",
        "\n",
        "X_preprocessed = np.array([text_preprocessing(text) for text in X])\n",
        "X_tfidf = tf_idf.transform(X_preprocessed)\n",
        "X_test_preprocessed = np.array([text_preprocessing(text) for text in X_test])\n",
        "X_test_tfidf = tf_idf.transform(X_test_preprocessed)\n",
        "\n",
        "nb_model = MultinomialNB(alpha=1.8)\n",
        "nb_model.fit(X_tfidf, y)\n",
        "probs = nb_model.predict_proba(X_test_tfidf)\n",
        "\n",
        "# Evaluate the classifier\n",
        "evaluate_roc(probs, test_data.isRumor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCYueNHxlO5C",
        "outputId": "a05dc3c3-72e8-4665-afc6-85b7249591d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      1.00      0.66       189\n",
            "           1       1.00      0.03      0.06       201\n",
            "\n",
            "    accuracy                           0.50       390\n",
            "   macro avg       0.75      0.51      0.36       390\n",
            "weighted avg       0.75      0.50      0.35       390\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "preds = np.argmax(probs, axis = 1)\n",
        "print(classification_report(test_data.isRumor, preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9jeKNBykI_-"
      },
      "source": [
        "# Fine-tuning BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wx6TKCN2kI_-"
      },
      "outputs": [],
      "source": [
        "def evaluate_roc(probs, y_true):\n",
        "    \"\"\"\n",
        "    - Print AUC and accuracy on the test set\n",
        "    - Plot ROC\n",
        "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
        "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
        "    \"\"\"\n",
        "    preds = probs[:, 1]\n",
        "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(f'AUC: {roc_auc:.4f}')\n",
        "\n",
        "    # Get accuracy over the test set\n",
        "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "\n",
        "    # Plot ROC AUC\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.plot([0, 1], [0, 1], 'r--')\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def text_preprocessing(text):\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "    # Remove '@name'\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
        "    text = re.sub(r\"(#)(\\S+)\", r'\\1 \\2', text)\n",
        "\n",
        "    text = re.sub(r\"http\\S+\", \"*\", text)  # http link -> '*'\n",
        "\n",
        "    # text = re.sub(r\"@\\S+\", \"@\", text)   # mention -> '@'\n",
        "    text = re.sub(r\"@[^\\s]+\", \"@\", text)   # mention -> '@'\n",
        "\n",
        "    # sent = re.sub(r\"(#)(\\S+)\", r'\\1 \\2', sent)\n",
        "    # sent = re.sub(r'([^\\s\\w@#\\*]|_)+', '', sent) # Erasing Special Characters\n",
        "\n",
        "    # Replace '&amp;' with '&'\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Create a function to tokenize a set of texts\n",
        "\n",
        "\n",
        "def preprocessing_for_bert(data):\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs (빈 리스트 2개 생성)\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in data:\n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=text_preprocessing(sent),  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            pad_to_max_length=True,         # Pad sentence to max length\n",
        "            # return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True      # Return attention mask\n",
        "        )\n",
        "\n",
        "        # Add the outputs to the lists (위의 빈 리스트에 상응하는 값 추가)\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors (리스트들을 텐서화)\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "# Create the BertClassfier class\n",
        "\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = 768, 50, 2\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "\n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "def initialize_model(epochs=4):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertClassifier(freeze_bert=False)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),\n",
        "                      lr=5e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0,  # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler, criterion\n",
        "\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts += 1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(\n",
        "                t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(\n",
        "                    f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "\n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJjwums-kI2t"
      },
      "source": [
        "## Pre-Proecess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwH4PQ90kI2v"
      },
      "outputs": [],
      "source": [
        "raw_text = pd.read_csv('./data/_PHEME_text.csv')\n",
        "y = pd.read_csv('./data/_PHEME_target.csv')\n",
        "data = pd.concat([raw_text.text, y], axis=1).reset_index(drop=True)\n",
        "val = pd.read_csv('data/_PHEMEext_text.csv')\n",
        "\n",
        "X_train = data.text.values\n",
        "y_train = data.target.values\n",
        "\n",
        "X_val = val.drop(['Event'],axis=1).text.values\n",
        "y_val = val.target.values\n",
        "\n",
        "rhi_data = pd.read_csv('data/_RHI_text.csv')\n",
        "rhi_y = pd.read_csv('data/_RHI_text.csv')\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# # X_train, X_val, y_train, y_val =\\\n",
        "# #     train_test_split(X, y, test_size=0.1, random_state=2020)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHN2kh2rkI_-",
        "outputId": "50916f5b-b215-42db-f2ae-669afd37068a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:  Micheal Essien denying the Ebola rumours like https://t.co/H2E1TAzeha\n",
            "Processed:  micheal essien denying the ebola rumours like *\n"
          ]
        }
      ],
      "source": [
        "# Print sentence 0\n",
        "print('Original: ', X_val[0])\n",
        "print('Processed: ', text_preprocessing(X_val[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65,
          "referenced_widgets": [
            "1bd1e9fe81934cf3885154196e620c03",
            "bd458a99a85e4a13afd1cf36b0a57ef7",
            "f954f9c5e5954c2bb96b5993a9395109",
            "ce478aa49888428c8b784e7389e191f4",
            "0660f1c2cada41d3bdfe2589d7184b56",
            "0c3cc67272e24e0c807af12b592c3a55",
            "f228d1ac81f6413a997d164c52f2ce02",
            "930e5993b2fb4a07943e2d0006d54dba"
          ]
        },
        "id": "b_elwoqWkI_-",
        "outputId": "7e17b2ef-50e3-43e5-c787-a80ad06ad132"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1bd1e9fe81934cf3885154196e620c03",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeUcrmthkI__",
        "outputId": "636bd926-dfb1-4240-9a4d-0ad1d58715e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length:  69\n"
          ]
        }
      ],
      "source": [
        "# Concatenate train data and test data\n",
        "all_tweets = np.concatenate([data.text.values, test_data.text.values])\n",
        "\n",
        "# Encode our concatenated data\n",
        "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_tweets]\n",
        "\n",
        "# Find the maximum length\n",
        "max_len = max([len(sent) for sent in encoded_tweets])\n",
        "print('Max length: ', max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPdV4V0QkJAA",
        "outputId": "a11ef879-a82e-437b-ff09-72ab2fdf897a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Tokenizing data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "# Specify `MAX_LEN`\n",
        "MAX_LEN = 64\n",
        "\n",
        "# # Print sentence 0 and its encoded token ids\n",
        "# token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n",
        "# print('Original: ', X[0])\n",
        "# print('\\nToken IDs: ', token_ids)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "print('\\nTokenizing data...')\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7hatUZskJAA"
      },
      "outputs": [],
      "source": [
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train)\n",
        "val_labels = torch.tensor(y_val)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rIFQkBukI2w"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnC8COuokJAB"
      },
      "outputs": [],
      "source": [
        "set_seed(42)    # Set seed for reproducibility\n",
        "bert_classifier, optimizer, scheduler, loss_fn = initialize_model(epochs=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lMzlYbokJAC",
        "outputId": "16455e97-7a49-4940-d596-00e12c50fffd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.545895   |     -      |     -     |   13.60  \n",
            "   1    |   40    |   0.431758   |     -      |     -     |   12.85  \n",
            "   1    |   60    |   0.419819   |     -      |     -     |   12.92  \n",
            "   1    |   80    |   0.408936   |     -      |     -     |   12.91  \n",
            "   1    |   100   |   0.364633   |     -      |     -     |   12.93  \n",
            "   1    |   120   |   0.350773   |     -      |     -     |   12.95  \n",
            "   1    |   140   |   0.348678   |     -      |     -     |   12.96  \n",
            "   1    |   160   |   0.342240   |     -      |     -     |   12.95  \n",
            "   1    |   180   |   0.320228   |     -      |     -     |   12.97  \n",
            "   1    |   181   |   0.209871   |     -      |     -     |   0.29   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.392390   |  0.885340  |   46.80   |  120.61  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.227295   |     -      |     -     |   13.57  \n",
            "   2    |   40    |   0.247884   |     -      |     -     |   12.95  \n",
            "   2    |   60    |   0.231736   |     -      |     -     |   12.95  \n",
            "   2    |   80    |   0.240648   |     -      |     -     |   12.96  \n",
            "   2    |   100   |   0.185628   |     -      |     -     |   12.95  \n",
            "   2    |   120   |   0.247057   |     -      |     -     |   12.95  \n",
            "   2    |   140   |   0.217445   |     -      |     -     |   12.95  \n",
            "   2    |   160   |   0.190913   |     -      |     -     |   12.95  \n",
            "   2    |   180   |   0.194698   |     -      |     -     |   12.96  \n",
            "   2    |   181   |   0.285361   |     -      |     -     |   0.28   \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.220762   |  2.083310  |   35.35   |  120.78  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   3    |   20    |   0.122823   |     -      |     -     |   13.58  \n",
            "   3    |   40    |   0.121248   |     -      |     -     |   12.92  \n",
            "   3    |   60    |   0.152879   |     -      |     -     |   12.93  \n",
            "   3    |   80    |   0.098159   |     -      |     -     |   12.91  \n",
            "   3    |   100   |   0.120083   |     -      |     -     |   12.93  \n",
            "   3    |   120   |   0.076811   |     -      |     -     |   12.94  \n",
            "   3    |   140   |   0.105630   |     -      |     -     |   12.92  \n",
            "   3    |   160   |   0.120352   |     -      |     -     |   12.92  \n",
            "   3    |   180   |   0.097088   |     -      |     -     |   12.94  \n",
            "   3    |   181   |   0.076236   |     -      |     -     |   0.28   \n",
            "----------------------------------------------------------------------\n",
            "   3    |    -    |   0.112640   |  2.375853  |   40.62   |  120.57  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   4    |   20    |   0.072653   |     -      |     -     |   13.56  \n",
            "   4    |   40    |   0.105584   |     -      |     -     |   12.91  \n",
            "   4    |   60    |   0.038721   |     -      |     -     |   12.86  \n",
            "   4    |   80    |   0.079732   |     -      |     -     |   12.90  \n",
            "   4    |   100   |   0.050438   |     -      |     -     |   12.90  \n",
            "   4    |   120   |   0.053360   |     -      |     -     |   12.88  \n",
            "   4    |   140   |   0.065261   |     -      |     -     |   12.93  \n",
            "   4    |   160   |   0.058267   |     -      |     -     |   12.93  \n",
            "   4    |   180   |   0.056957   |     -      |     -     |   12.88  \n",
            "   4    |   181   |   0.396380   |     -      |     -     |   0.29   \n",
            "----------------------------------------------------------------------\n",
            "   4    |    -    |   0.066420   |  2.963038  |   37.70   |  120.33  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=4, evaluation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiESwlsRkJAC"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def bert_predict(model, test_dataloader):\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        all_logits.append(logits)\n",
        "    \n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "POlisyX_kJAC",
        "outputId": "86f2cd35-b976-481b-dfb1-74687ce66d53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.7720\n",
            "Accuracy: 34.23%\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Compute predicted probabilities on the test set\n",
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "\n",
        "# Evaluate the Bert classifier\n",
        "evaluate_roc(probs, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORseUrD3kJAC"
      },
      "outputs": [],
      "source": [
        "torch.save(bert_classifier.state_dict(), './Model/BERT_raw_to_fine_tune_ord.pt')\n",
        "# torch.save(bert_classifier.state_dict(), './BERT_raw_to_fine_tune_test.pt')\n",
        "# torch.save(model.state_dict, 'model.pt') # saving state dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmCy5mzZkI2w"
      },
      "source": [
        "## Training (full)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGnU5iiokJAC",
        "outputId": "9deeaa79-ac83-4dbb-c632-64ac737a1b5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.519470   |     -      |     -     |   8.42   \n",
            "   1    |   40    |   0.458738   |     -      |     -     |   8.27   \n",
            "   1    |   60    |   0.411981   |     -      |     -     |   8.16   \n",
            "   1    |   80    |   0.388299   |     -      |     -     |   7.95   \n",
            "   1    |   100   |   0.378233   |     -      |     -     |   7.75   \n",
            "   1    |   120   |   0.321618   |     -      |     -     |   7.62   \n",
            "   1    |   140   |   0.334600   |     -      |     -     |   7.57   \n",
            "   1    |   160   |   0.355007   |     -      |     -     |   7.52   \n",
            "   1    |   180   |   0.343738   |     -      |     -     |   7.55   \n",
            "   1    |   181   |   0.211648   |     -      |     -     |   0.14   \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.214941   |     -      |     -     |   7.98   \n",
            "   2    |   40    |   0.221853   |     -      |     -     |   7.71   \n",
            "   2    |   60    |   0.216489   |     -      |     -     |   7.76   \n",
            "   2    |   80    |   0.241423   |     -      |     -     |   7.82   \n",
            "   2    |   100   |   0.216051   |     -      |     -     |   7.84   \n",
            "   2    |   120   |   0.194518   |     -      |     -     |   7.83   \n",
            "   2    |   140   |   0.224164   |     -      |     -     |   7.81   \n",
            "   2    |   160   |   0.193176   |     -      |     -     |   7.74   \n",
            "   2    |   180   |   0.231373   |     -      |     -     |   7.72   \n",
            "   2    |   181   |   0.057531   |     -      |     -     |   0.14   \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "# Concatenate the train set and the validation set\n",
        "full_train_data = torch.utils.data.ConcatDataset([train_data, val_data])\n",
        "full_train_sampler = RandomSampler(full_train_data)\n",
        "full_train_dataloader = DataLoader(full_train_data, sampler=full_train_sampler, batch_size=32)\n",
        "\n",
        "# Train the Bert Classifier on the entire training data\n",
        "set_seed(42)\n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
        "train(bert_classifier, full_train_dataloader, epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7YdsbbKkI2w"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtvXnLkXUYAT",
        "outputId": "4f27822d-d09d-48b6-aa4b-50aa38640e26"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 48,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "PATH = './Model/BERT_raw_to_fine_tune_ord.pt'\n",
        "# bn_state_dict = torch.load('./BERT_raw_to_fine_tune_ord.pt')\n",
        "# bert_classifier.load_state_dict(bn_state_dict)\n",
        "bert_classifier.load_state_dict(torch.load(PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbGz7NFrkJAC",
        "outputId": "0e45c9fa-f670-4af1-d3ca-b644022a292f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "#  Run `preprocessing_for_bert` on the test set\n",
        "print('Tokenizing data...')\n",
        "test_inputs, test_masks = preprocessing_for_bert(test_data.text)\n",
        "\n",
        "# Create the DataLoader for our test set\n",
        "test_dataset = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHgcr79RkJAC",
        "outputId": "db2fa9a0-d788-466c-b4b3-750278f6e15a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tweets predicted as Rumor:  54\n"
          ]
        }
      ],
      "source": [
        "# Compute predicted probabilities on the test set\n",
        "probs = bert_predict(bert_classifier, test_dataloader)\n",
        "\n",
        "# Get predictions from the probabilities\n",
        "threshold = 0.5\n",
        "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
        "\n",
        "# Number of tweets predicted non-negative\n",
        "print(\"Number of tweets predicted as Rumor: \", preds.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "X9dcv6x5cyQd",
        "outputId": "aca38ff3-5101-4bbb-d0ec-a738d17f65ba"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.925041</td>\n",
              "      <td>0.074959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.994930</td>\n",
              "      <td>0.005070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.980860</td>\n",
              "      <td>0.019140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.952689</td>\n",
              "      <td>0.047311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.967151</td>\n",
              "      <td>0.032849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>480</th>\n",
              "      <td>0.870599</td>\n",
              "      <td>0.129401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>481</th>\n",
              "      <td>0.996000</td>\n",
              "      <td>0.004000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>482</th>\n",
              "      <td>0.994768</td>\n",
              "      <td>0.005232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>483</th>\n",
              "      <td>0.996281</td>\n",
              "      <td>0.003719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>484</th>\n",
              "      <td>0.997700</td>\n",
              "      <td>0.002300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>485 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1\n",
              "0    0.925041  0.074959\n",
              "1    0.994930  0.005070\n",
              "2    0.980860  0.019140\n",
              "3    0.952689  0.047311\n",
              "4    0.967151  0.032849\n",
              "..        ...       ...\n",
              "480  0.870599  0.129401\n",
              "481  0.996000  0.004000\n",
              "482  0.994768  0.005232\n",
              "483  0.996281  0.003719\n",
              "484  0.997700  0.002300\n",
              "\n",
              "[485 rows x 2 columns]"
            ]
          },
          "execution_count": 54,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame(probs)#.idxmax(axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Fv_8oE2fmJT",
        "outputId": "71da4a04-3e04-4271-ed71-238e332a5ad5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.98      0.42       116\n",
            "           1       0.96      0.14      0.25       369\n",
            "\n",
            "    accuracy                           0.34       485\n",
            "   macro avg       0.61      0.56      0.33       485\n",
            "weighted avg       0.80      0.34      0.29       485\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "preds = np.argmax(probs, axis = 1)\n",
        "print(classification_report(test_data.target, preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        },
        "id": "vtUsm5tDkJAC",
        "outputId": "d1938b91-f9e0-4b5f-d957-a6f8ab845a92"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>isRumor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Family claims #CorneliusGurlitt was mentally u...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>The plot thickens: #Gurlitt's cousin claims Mu...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>Munich District Court has confirmed the applic...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>Munich District Court has confirmed the applic...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>Unconfirmed reports claim that Michael Essien ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>AC Milan have denied reports that midfielder M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>Breaking news: Ghana international and AC Mila...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>BREAKING: Unconfirmed reports claim AC Milan m...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>AC Milan midfielder Michael Essien has been di...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>AC Milan have confirmed that the reports about...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>Putin \"disappearance\" Rumor: He Is In Switzerl...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>Well, this might explain everything: Kabayeva ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>Swiss Paper #Putin missing due to daughter bor...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>And you thought it was Putin's back? Alina Kab...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>Who's the daddy? One explanation for #Putin's ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>Reports claim that the military may be organiz...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>\"Putin's continued absence suggests a palace c...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>Rumor is that Primakov orchestrated coup again...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>RT @L0gg0l: Rumors in Switzerland: #Putin abse...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>210</th>\n",
              "      <td>Coup scenario:Ivanov killed Nemtsov,blamed it ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>218</th>\n",
              "      <td>According to this article http://t.co/pWZaRxHy...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223</th>\n",
              "      <td>No Russian leader as charismatic as Putin exce...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>228</th>\n",
              "      <td>Maybe Putin has just been on paternity leave? ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232</th>\n",
              "      <td>NEWEST #Putin rumour, his girlfriend just gave...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>Rumour: #Ivanov to take over for #Putin in #Ru...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>252</th>\n",
              "      <td>Rumours emerging that a coup has taken place i...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253</th>\n",
              "      <td>Coup? RT @jimgeraghty: Rumors all Russian mili...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>264</th>\n",
              "      <td>Фейк?! или #Путина #убили! According to report...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>Reports claim Putin disappeared due to impendi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>This man was seen in Grozny yesterday evening ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>372</th>\n",
              "      <td>#PutinDead Putin unfortunately not dead, altho...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text  isRumor\n",
              "7    Family claims #CorneliusGurlitt was mentally u...        1\n",
              "110  The plot thickens: #Gurlitt's cousin claims Mu...        0\n",
              "113  Munich District Court has confirmed the applic...        0\n",
              "134  Munich District Court has confirmed the applic...        0\n",
              "144  Unconfirmed reports claim that Michael Essien ...        1\n",
              "147  AC Milan have denied reports that midfielder M...        1\n",
              "148  Breaking news: Ghana international and AC Mila...        1\n",
              "149  BREAKING: Unconfirmed reports claim AC Milan m...        1\n",
              "150  AC Milan midfielder Michael Essien has been di...        1\n",
              "151  AC Milan have confirmed that the reports about...        1\n",
              "157  Putin \"disappearance\" Rumor: He Is In Switzerl...        1\n",
              "165  Well, this might explain everything: Kabayeva ...        1\n",
              "167  Swiss Paper #Putin missing due to daughter bor...        1\n",
              "171  And you thought it was Putin's back? Alina Kab...        1\n",
              "173  Who's the daddy? One explanation for #Putin's ...        1\n",
              "175  Reports claim that the military may be organiz...        1\n",
              "190  \"Putin's continued absence suggests a palace c...        1\n",
              "194  Rumor is that Primakov orchestrated coup again...        1\n",
              "208  RT @L0gg0l: Rumors in Switzerland: #Putin abse...        1\n",
              "210  Coup scenario:Ivanov killed Nemtsov,blamed it ...        1\n",
              "218  According to this article http://t.co/pWZaRxHy...        1\n",
              "223  No Russian leader as charismatic as Putin exce...        1\n",
              "228  Maybe Putin has just been on paternity leave? ...        1\n",
              "232  NEWEST #Putin rumour, his girlfriend just gave...        1\n",
              "247  Rumour: #Ivanov to take over for #Putin in #Ru...        1\n",
              "252  Rumours emerging that a coup has taken place i...        1\n",
              "253  Coup? RT @jimgeraghty: Rumors all Russian mili...        1\n",
              "264  Фейк?! или #Путина #убили! According to report...        1\n",
              "275  Reports claim Putin disappeared due to impendi...        1\n",
              "298  This man was seen in Grozny yesterday evening ...        0\n",
              "372  #PutinDead Putin unfortunately not dead, altho...        0"
            ]
          },
          "execution_count": 73,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output = test_data[preds==1]\n",
        "output\n",
        "# list(output.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ggmjfp2ZVeV-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whaUxM0Gr6dG"
      },
      "source": [
        "## Testing (RHI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mx97JzRr_lD"
      },
      "outputs": [],
      "source": [
        "# raw_text = pd.read_csv('./data/_RHI_text.csv')\n",
        "# # y = pd.read_csv('./data/_RHI_target.csv')\n",
        "# data = pd.concat([raw_text.text, y], axis=1).reset_index(drop=True)\n",
        "# val = pd.read_csv('data/_PHEMEext_text.csv')\n",
        "\n",
        "# X_train = data.text.values\n",
        "# y_train = data.target.values\n",
        "\n",
        "# X_val = val.drop(['Event'],axis=1).text.values\n",
        "# y_val = val.target.values\n",
        "\n",
        "rhi_data = pd.read_csv('data/_RHI_text.csv')\n",
        "rhi_y = pd.read_csv('data/_RHI_text.csv')\n",
        "\n",
        "X_test = rhi_data.text.values\n",
        "y_test = rhi_y.isRumor.values\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# # X_train, X_val, y_train, y_val =\\\n",
        "# #     train_test_split(X, y, test_size=0.1, random_state=2020)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PltGfRMr6dH",
        "outputId": "6ff4c2a5-71d5-41a8-bf69-e11f9ef05f84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 58,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "PATH = './Model/BERT_raw_to_fine_tune_ord.pt'\n",
        "# bn_state_dict = torch.load('./BERT_raw_to_fine_tune_ord.pt')\n",
        "# bert_classifier.load_state_dict(bn_state_dict)\n",
        "bert_classifier.load_state_dict(torch.load(PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExKeyBFfr6dI",
        "outputId": "5b12ea54-d51c-4ad7-eb76-1005665f443b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "#  Run `preprocessing_for_bert` on the test set\n",
        "print('Tokenizing data...')\n",
        "test_inputs, test_masks = preprocessing_for_bert(X_test)\n",
        "\n",
        "# Create the DataLoader for our test set\n",
        "test_dataset = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "test_labels = torch.tensor(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKgdcM3er6dI",
        "outputId": "677aa6d6-0d72-4c30-db66-86eea28c40fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tweets predicted as Rumor:  91\n"
          ]
        }
      ],
      "source": [
        "# Compute predicted probabilities on the test set\n",
        "probs = bert_predict(bert_classifier, test_dataloader)\n",
        "\n",
        "# Get predictions from the probabilities\n",
        "threshold = 0.5\n",
        "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
        "\n",
        "# Number of tweets predicted non-negative\n",
        "print(\"Number of tweets predicted as Rumor: \", preds.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "jlQV-70er6dI",
        "outputId": "871cb1b6-b7d5-4085-b7dc-803b5876147d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.483309</td>\n",
              "      <td>0.516691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.997296</td>\n",
              "      <td>0.002704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.997502</td>\n",
              "      <td>0.002498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.927679</td>\n",
              "      <td>0.072321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.997721</td>\n",
              "      <td>0.002279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5222</th>\n",
              "      <td>0.997374</td>\n",
              "      <td>0.002626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5223</th>\n",
              "      <td>0.998007</td>\n",
              "      <td>0.001993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5224</th>\n",
              "      <td>0.996135</td>\n",
              "      <td>0.003865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5225</th>\n",
              "      <td>0.993574</td>\n",
              "      <td>0.006426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5226</th>\n",
              "      <td>0.996873</td>\n",
              "      <td>0.003127</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5227 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0         1\n",
              "0     0.483309  0.516691\n",
              "1     0.997296  0.002704\n",
              "2     0.997502  0.002498\n",
              "3     0.927679  0.072321\n",
              "4     0.997721  0.002279\n",
              "...        ...       ...\n",
              "5222  0.997374  0.002626\n",
              "5223  0.998007  0.001993\n",
              "5224  0.996135  0.003865\n",
              "5225  0.993574  0.006426\n",
              "5226  0.996873  0.003127\n",
              "\n",
              "[5227 rows x 2 columns]"
            ]
          },
          "execution_count": 72,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame(probs)#.idxmax(axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "fXQYp8rar6dI",
        "outputId": "c89d7715-2a3e-4bc4-8ecf-a4ce40ff22b5"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-26d5925c3c70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "preds = np.argmax(probs, axis = 1)\n",
        "print(classification_report(y_test, preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkXv5vrcr6dJ",
        "outputId": "1fd3d2e5-4d6d-4107-be15-2c3f1b5bbb63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4682"
            ]
          },
          "execution_count": 78,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output = y_test[preds==0].sum()\n",
        "output\n",
        "# list(output.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "uJ_f_mK0yjkA",
        "outputId": "d62921bf-1b6a-4bf0-a215-d41bfd11922d"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-0370dfaf5e11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2895\u001b[0m         \u001b[0;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m         \u001b[0;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2942\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2943\u001b[0m             raise ValueError(\n\u001b[0;32m-> 2944\u001b[0;31m                 \u001b[0;34mf\"Item wrong length {len(key)} instead of {len(self.index)}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2945\u001b[0m             )\n\u001b[1;32m   2946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Item wrong length 5227 instead of 485."
          ]
        }
      ],
      "source": [
        "test_data[preds==0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNAzuZdm-5eY"
      },
      "source": [
        "# BERT 최종\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RkOFN0Tpp5s"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKhXQU5BE2YM"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "%matplotlib inline\n",
        "from transformers import BertTokenizer\n",
        "from sklearn.metrics import accuracy_score, roc_curve, auc, classification_report, f1_score\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "import random\n",
        "import time\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgmOuPxRFhof"
      },
      "outputs": [],
      "source": [
        "def getDevice():\n",
        "  if torch.cuda.is_available():       \n",
        "      device = torch.device(\"cuda\")\n",
        "      print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "      print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "  else:\n",
        "      print('No GPU available, using the CPU instead.')\n",
        "      device = torch.device(\"cpu\")\n",
        "  return device\n",
        "\n",
        "def evaluate_roc(probs, y_true):\n",
        "    \"\"\"\n",
        "    - Print AUC and accuracy on the test set\n",
        "    - Plot ROC\n",
        "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
        "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
        "    \"\"\"\n",
        "    preds = probs[:, 1]\n",
        "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(f'AUC: {roc_auc:.4f}')\n",
        "\n",
        "    # Get accuracy over the test set\n",
        "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "\n",
        "    # Plot ROC AUC\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.plot([0, 1], [0, 1], 'r--')\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()\n",
        "\n",
        "def text_preprocessing(text): # Create a function to tokenize a set of texts\n",
        "\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "    # Remove '@name'\n",
        "    text = text.lower()\n",
        "    # text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
        "    # text = re.sub(r\"(#)(\\S+)\", r'\\1 \\2', text)\n",
        "\n",
        "    # text = re.sub(r\"http\\S+\", \"*\", text)  # http link -> '*'\n",
        "\n",
        "    # text = re.sub(r\"@\\S+\", \"@\", text)   # mention -> '@'\n",
        "    # text = re.sub(r\"@[^\\s]+\", \"@\", text)   # mention -> '@'\n",
        "\n",
        "    # sent = re.sub(r\"(#)(\\S+)\", r'\\1 \\2', sent)\n",
        "    # sent = re.sub(r'([^\\s\\w@#\\*]|_)+', '', sent) # Erasing Special Characters\n",
        "\n",
        "    # Replace '&amp;' with '&'\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def preprocessing_for_bert(data): \n",
        "\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs (빈 리스트 2개 생성)\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in data:\n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=text_preprocessing(sent),  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            # max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            pad_to_max_length=True,         # Pad sentence to max length\n",
        "            # return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True,      # Return attention mask\n",
        "\n",
        "            # max_length=True,                  # Max length to truncate/pad\n",
        "            padding='max_length'\n",
        "        )\n",
        "\n",
        "        # Add the outputs to the lists (위의 빈 리스트에 상응하는 값 추가)\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors (리스트들을 텐서화)\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "class BertClassifier(nn.Module): # Create the BertClassfier class\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = 768, 50, 2\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "\n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits\n",
        "\n",
        "def initialize_model(epochs=4):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertClassifier(freeze_bert=False)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),\n",
        "                      lr=5e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0,  # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler, criterion\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts += 1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(\n",
        "                t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(\n",
        "                    f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "\n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy\n",
        "\n",
        "def bert_predict(model, test_dataloader):\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        all_logits.append(logits)\n",
        "    \n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return probs\n",
        "\n",
        "def data_process(X_train, y_train, X_val, y_val, batch_size=16):\n",
        "  # Concatenate train data and test data\n",
        "  all_tweets = np.concatenate([X_train, X_val])\n",
        "\n",
        "  # Encode our concatenated data\n",
        "  encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_tweets]\n",
        "\n",
        "  # Find the maximum length\n",
        "  max_len = max([len(sent) for sent in encoded_tweets])\n",
        "  print('Max length: ', max_len)\n",
        "\n",
        "  # Specify `MAX_LEN`\n",
        "  MAX_LEN = max_len\n",
        "\n",
        "  # Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "  print('\\nTokenizing data...')\n",
        "  train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
        "  val_inputs, val_masks = preprocessing_for_bert(X_val)\n",
        "\n",
        "  # Convert other data types to torch.Tensor\n",
        "  train_labels = torch.tensor(y_train)\n",
        "  val_labels = torch.tensor(y_val)\n",
        "\n",
        "  # For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "  batch_size = 8\n",
        "\n",
        "  # Create the DataLoader for our training set\n",
        "  train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Create the DataLoader for our validation set\n",
        "  val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "  val_sampler = SequentialSampler(val_data)\n",
        "  val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "\n",
        "  return train_dataloader, val_dataloader\n",
        "\n",
        "def train_process(train_dataloader, val_dataloader, epoch=4):\n",
        "  set_seed(42)    # Set seed for reproducibility\n",
        "  bert_classifier, optimizer, scheduler, loss_fn = initialize_model(epochs=epoch)\n",
        "\n",
        "  train(bert_classifier, train_dataloader, loss_fn, epochs=4, evaluation=True)\n",
        "\n",
        "  return bert_classifier\n",
        "\n",
        "def testing_process(bert_classifier, X_val, y_val):\n",
        "  #  Run `preprocessing_for_bert` on the test set\n",
        "  print('Tokenizing data...')\n",
        "  test_inputs, test_masks = preprocessing_for_bert(X_val)\n",
        "\n",
        "  # Create the DataLoader for our test set\n",
        "  test_dataset = TensorDataset(test_inputs, test_masks)\n",
        "  test_sampler = SequentialSampler(test_dataset)\n",
        "  test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)\n",
        "\n",
        "  # Compute predicted probabilities on the test set\n",
        "  probs = bert_predict(bert_classifier, test_dataloader)\n",
        "\n",
        "  # Get predictions from the probabilities\n",
        "  threshold = 0.5\n",
        "  preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
        "\n",
        "  # Number of tweets predicted non-negative\n",
        "  print(\"Number of tweets predicted as Rumor: \", preds.sum())\n",
        "\n",
        "  preds = np.argmax(probs, axis = 1)\n",
        "  print('Accuracy Score:\\t',accuracy_score(y_val, preds))\n",
        "  print('Precision Score:\\t', str(precision_score(y_val,preds)))\n",
        "  print('Recall Score:\\t\\t' + str(recall_score(y_val,preds)))\n",
        "  print('F1 Score:\\t',f1_score(y_val, preds, zero_division=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk3m9D6kpsOP"
      },
      "source": [
        "## Executions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAAH77uL_FiS"
      },
      "outputs": [],
      "source": [
        "raw_text = pd.read_csv('./data/_PHEME_text.csv')\n",
        "y = pd.read_csv('./data/_PHEME_target.csv')\n",
        "data = pd.concat([raw_text.text, y], axis=1).reset_index(drop=True)\n",
        "val = pd.read_csv('data/_PHEMEext_text.csv')\n",
        "\n",
        "X_train = data.text.values\n",
        "y_train = data.target.values\n",
        "\n",
        "X_val = val.drop(['Event'],axis=1).text.values\n",
        "y_val = val.target.values\n",
        "\n",
        "rhi_data = pd.read_csv('data/_RHI_text.csv').text.values\n",
        "rhi_y = pd.read_csv('data/_RHI_text.csv').isRumor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K84HzvLMbP0X",
        "outputId": "94fc8e1a-8b64-4147-e898-009599c44ac1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n",
            "Max length:  69\n",
            "\n",
            "Tokenizing data...\n"
          ]
        }
      ],
      "source": [
        "device = getDevice()\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "train_dataloader, val_dataloader = data_process(X_train, y_train, X_val, y_val, batch_size=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFfTQEHZ_Hc6"
      },
      "outputs": [],
      "source": [
        "set_seed(42)    # Set seed for reproducibility\n",
        "bert_classifier, optimizer, scheduler, loss_fn = initialize_model(epochs=3)\n",
        "# train(bert_classifier, train_dataloader, val_dataloader, epochs=1, evaluation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyxq0fPUvbDA",
        "outputId": "dac36902-7194-473b-867d-c44d37555a7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.629314   |     -      |     -     |   17.30  \n",
            "   1    |   40    |   0.553115   |     -      |     -     |   17.16  \n",
            "   1    |   60    |   0.588918   |     -      |     -     |   16.53  \n",
            "   1    |   80    |   0.500162   |     -      |     -     |   16.13  \n",
            "   1    |   100   |   0.512859   |     -      |     -     |   16.06  \n",
            "   1    |   120   |   0.545202   |     -      |     -     |   16.25  \n",
            "   1    |   140   |   0.431342   |     -      |     -     |   16.48  \n",
            "   1    |   160   |   0.406685   |     -      |     -     |   16.51  \n",
            "   1    |   180   |   0.529125   |     -      |     -     |   16.36  \n",
            "   1    |   200   |   0.490212   |     -      |     -     |   16.24  \n",
            "   1    |   220   |   0.484178   |     -      |     -     |   16.25  \n",
            "   1    |   240   |   0.468012   |     -      |     -     |   16.33  \n",
            "   1    |   260   |   0.473001   |     -      |     -     |   16.38  \n",
            "   1    |   280   |   0.413844   |     -      |     -     |   16.40  \n",
            "   1    |   300   |   0.547014   |     -      |     -     |   16.36  \n",
            "   1    |   320   |   0.397528   |     -      |     -     |   16.29  \n",
            "   1    |   340   |   0.429567   |     -      |     -     |   16.27  \n",
            "   1    |   360   |   0.355043   |     -      |     -     |   16.27  \n",
            "   1    |   380   |   0.368029   |     -      |     -     |   16.33  \n",
            "   1    |   400   |   0.375655   |     -      |     -     |   16.37  \n",
            "   1    |   420   |   0.414914   |     -      |     -     |   16.37  \n",
            "   1    |   440   |   0.343276   |     -      |     -     |   16.37  \n",
            "   1    |   460   |   0.338237   |     -      |     -     |   16.38  \n",
            "   1    |   480   |   0.387037   |     -      |     -     |   16.39  \n",
            "   1    |   500   |   0.353207   |     -      |     -     |   16.38  \n",
            "   1    |   520   |   0.319626   |     -      |     -     |   16.39  \n",
            "   1    |   540   |   0.534802   |     -      |     -     |   16.38  \n",
            "   1    |   560   |   0.333614   |     -      |     -     |   16.37  \n",
            "   1    |   580   |   0.337307   |     -      |     -     |   16.39  \n",
            "   1    |   600   |   0.386578   |     -      |     -     |   16.39  \n",
            "   1    |   620   |   0.318854   |     -      |     -     |   16.37  \n",
            "   1    |   640   |   0.430609   |     -      |     -     |   16.37  \n",
            "   1    |   660   |   0.301156   |     -      |     -     |   16.33  \n",
            "   1    |   680   |   0.324869   |     -      |     -     |   16.35  \n",
            "   1    |   700   |   0.447027   |     -      |     -     |   16.35  \n",
            "   1    |   720   |   0.303918   |     -      |     -     |   16.36  \n",
            "   1    |   725   |   0.455132   |     -      |     -     |   3.51   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.427523   |  2.352970  |   28.07   |  612.67  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.273147   |     -      |     -     |   17.13  \n",
            "   2    |   40    |   0.242201   |     -      |     -     |   16.33  \n",
            "   2    |   60    |   0.309965   |     -      |     -     |   16.33  \n",
            "   2    |   80    |   0.323439   |     -      |     -     |   16.35  \n",
            "   2    |   100   |   0.255190   |     -      |     -     |   16.31  \n",
            "   2    |   120   |   0.190140   |     -      |     -     |   16.34  \n",
            "   2    |   140   |   0.255846   |     -      |     -     |   16.33  \n",
            "   2    |   160   |   0.430966   |     -      |     -     |   16.33  \n",
            "   2    |   180   |   0.208292   |     -      |     -     |   16.34  \n",
            "   2    |   200   |   0.294808   |     -      |     -     |   16.31  \n",
            "   2    |   220   |   0.347223   |     -      |     -     |   16.30  \n",
            "   2    |   240   |   0.314548   |     -      |     -     |   16.25  \n",
            "   2    |   260   |   0.238003   |     -      |     -     |   16.26  \n",
            "   2    |   280   |   0.317204   |     -      |     -     |   16.26  \n",
            "   2    |   300   |   0.248775   |     -      |     -     |   16.27  \n",
            "   2    |   320   |   0.286750   |     -      |     -     |   16.27  \n",
            "   2    |   340   |   0.204693   |     -      |     -     |   16.31  \n",
            "   2    |   360   |   0.284120   |     -      |     -     |   16.31  \n",
            "   2    |   380   |   0.179818   |     -      |     -     |   16.32  \n",
            "   2    |   400   |   0.252450   |     -      |     -     |   16.33  \n",
            "   2    |   420   |   0.290128   |     -      |     -     |   16.27  \n",
            "   2    |   440   |   0.237999   |     -      |     -     |   16.28  \n",
            "   2    |   460   |   0.126027   |     -      |     -     |   16.24  \n",
            "   2    |   480   |   0.273371   |     -      |     -     |   16.23  \n",
            "   2    |   500   |   0.269467   |     -      |     -     |   16.25  \n",
            "   2    |   520   |   0.256358   |     -      |     -     |   16.27  \n",
            "   2    |   540   |   0.257195   |     -      |     -     |   16.24  \n",
            "   2    |   560   |   0.293283   |     -      |     -     |   16.25  \n",
            "   2    |   580   |   0.177041   |     -      |     -     |   16.26  \n",
            "   2    |   600   |   0.396848   |     -      |     -     |   16.32  \n",
            "   2    |   620   |   0.193316   |     -      |     -     |   16.30  \n",
            "   2    |   640   |   0.308464   |     -      |     -     |   16.31  \n",
            "   2    |   660   |   0.153723   |     -      |     -     |   16.30  \n",
            "   2    |   680   |   0.241623   |     -      |     -     |   16.32  \n",
            "   2    |   700   |   0.256835   |     -      |     -     |   16.32  \n",
            "   2    |   720   |   0.231722   |     -      |     -     |   16.33  \n",
            "   2    |   725   |   0.114021   |     -      |     -     |   3.52   \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.260693   |  2.892816  |   30.74   |  609.92  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   3    |   20    |   0.184743   |     -      |     -     |   17.10  \n",
            "   3    |   40    |   0.071133   |     -      |     -     |   16.30  \n",
            "   3    |   60    |   0.080171   |     -      |     -     |   16.26  \n",
            "   3    |   80    |   0.165196   |     -      |     -     |   16.28  \n",
            "   3    |   100   |   0.183444   |     -      |     -     |   16.26  \n",
            "   3    |   120   |   0.108059   |     -      |     -     |   16.24  \n",
            "   3    |   140   |   0.054661   |     -      |     -     |   16.24  \n",
            "   3    |   160   |   0.196549   |     -      |     -     |   16.24  \n",
            "   3    |   180   |   0.140945   |     -      |     -     |   16.27  \n",
            "   3    |   200   |   0.108247   |     -      |     -     |   16.24  \n",
            "   3    |   220   |   0.237730   |     -      |     -     |   16.25  \n",
            "   3    |   240   |   0.111115   |     -      |     -     |   16.30  \n",
            "   3    |   260   |   0.119129   |     -      |     -     |   16.26  \n",
            "   3    |   280   |   0.180386   |     -      |     -     |   16.31  \n",
            "   3    |   300   |   0.122873   |     -      |     -     |   16.32  \n",
            "   3    |   320   |   0.151681   |     -      |     -     |   16.30  \n",
            "   3    |   340   |   0.132506   |     -      |     -     |   16.28  \n",
            "   3    |   360   |   0.223705   |     -      |     -     |   16.27  \n",
            "   3    |   380   |   0.089148   |     -      |     -     |   16.23  \n",
            "   3    |   400   |   0.217655   |     -      |     -     |   16.26  \n",
            "   3    |   420   |   0.186604   |     -      |     -     |   16.25  \n",
            "   3    |   440   |   0.074641   |     -      |     -     |   16.25  \n",
            "   3    |   460   |   0.008564   |     -      |     -     |   16.24  \n",
            "   3    |   480   |   0.257192   |     -      |     -     |   16.23  \n",
            "   3    |   500   |   0.131824   |     -      |     -     |   16.25  \n",
            "   3    |   520   |   0.106689   |     -      |     -     |   16.26  \n",
            "   3    |   540   |   0.098494   |     -      |     -     |   16.23  \n",
            "   3    |   560   |   0.162358   |     -      |     -     |   16.24  \n",
            "   3    |   580   |   0.111314   |     -      |     -     |   16.25  \n",
            "   3    |   600   |   0.040556   |     -      |     -     |   16.26  \n",
            "   3    |   620   |   0.244624   |     -      |     -     |   16.28  \n",
            "   3    |   640   |   0.115019   |     -      |     -     |   16.27  \n",
            "   3    |   660   |   0.105975   |     -      |     -     |   16.29  \n",
            "   3    |   680   |   0.135292   |     -      |     -     |   16.27  \n",
            "   3    |   700   |   0.058514   |     -      |     -     |   16.25  \n",
            "   3    |   720   |   0.109798   |     -      |     -     |   16.23  \n",
            "   3    |   725   |   0.009366   |     -      |     -     |   3.50   \n",
            "----------------------------------------------------------------------\n",
            "   3    |    -    |   0.133281   |  2.890345  |   45.70   |  608.56  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=3, evaluation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwDvSUU-GrFW"
      },
      "outputs": [],
      "source": [
        "torch.save(bert_classifier.state_dict(), './Model/BERT_raw_to_fine_tune_ord4.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "16ynZzt_A95T",
        "outputId": "4ca200ac-fb0b-4323-d0aa-31b79eaa888b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.7044\n",
            "Accuracy: 45.36%\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Compute predicted probabilities on the test set\n",
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "\n",
        "# Evaluate the Bert classifier\n",
        "evaluate_roc(probs, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNR9pBOAGz8p",
        "outputId": "97c24e9c-27c5-4917-b4fc-1204c36845b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing data...\n",
            "Number of tweets predicted as Rumor:  122\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.92      0.45       116\n",
            "           1       0.93      0.31      0.46       369\n",
            "\n",
            "    accuracy                           0.45       485\n",
            "   macro avg       0.61      0.61      0.45       485\n",
            "weighted avg       0.78      0.45      0.46       485\n",
            "\n",
            "0.4536082474226804\n",
            "0.4602851323828921\n"
          ]
        }
      ],
      "source": [
        "PATH = './Model/BERT_raw_to_fine_tune_ord4.pt'\n",
        "bert_classifier.load_state_dict(torch.load(PATH))\n",
        "testing_process(bert_classifier, X_val, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eKaw3r2bAwg",
        "outputId": "e84554cf-46e2-4590-a4fe-84043acc2cc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing data...\n",
            "Number of tweets predicted as Rumor:  795\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.10      0.94      0.18       475\n",
            "           1       0.96      0.16      0.28      4752\n",
            "\n",
            "    accuracy                           0.23      5227\n",
            "   macro avg       0.53      0.55      0.23      5227\n",
            "weighted avg       0.89      0.23      0.27      5227\n",
            "\n",
            "0.23225559594413622\n",
            "0.27654588065621055\n"
          ]
        }
      ],
      "source": [
        "rhi_data = pd.read_csv('data/_RHI_text.csv')\n",
        "rhi_y = pd.read_csv('data/_RHI_text.csv')\n",
        "X_test = rhi_data.text.values\n",
        "y_test = rhi_y.isRumor.values\n",
        "\n",
        "PATH = './Model/BERT_raw_to_fine_tune_ord4.pt'\n",
        "bert_classifier.load_state_dict(torch.load(PATH))\n",
        "testing_process(bert_classifier, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSx6mKV2bgB_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2_qMkhDfb-6"
      },
      "source": [
        "# BERTweet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMD4JSaapG-T"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VoalIiSJfhWq"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "%matplotlib inline\n",
        "from sklearn.metrics import accuracy_score, roc_curve, auc, classification_report, f1_score\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "import random\n",
        "import time\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbsgwY9ZfhWq"
      },
      "outputs": [],
      "source": [
        "def getDevice():\n",
        "  if torch.cuda.is_available():       \n",
        "      device = torch.device(\"cuda\")\n",
        "      print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "      print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "  else:\n",
        "      print('No GPU available, using the CPU instead.')\n",
        "      device = torch.device(\"cpu\")\n",
        "  return device\n",
        "\n",
        "def evaluate_roc(probs, y_true):\n",
        "    \"\"\"\n",
        "    - Print AUC and accuracy on the test set\n",
        "    - Plot ROC\n",
        "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
        "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
        "    \"\"\"\n",
        "    preds = probs[:, 1]\n",
        "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(f'AUC: {roc_auc:.4f}')\n",
        "\n",
        "    # Get accuracy over the test set\n",
        "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "\n",
        "    # Plot ROC AUC\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.plot([0, 1], [0, 1], 'r--')\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()\n",
        "\n",
        "def text_preprocessing(text): # Create a function to tokenize a set of texts\n",
        "\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "    # Remove '@name'\n",
        "    text = text.lower()\n",
        "    # text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
        "    # text = re.sub(r\"(#)(\\S+)\", r'\\1 \\2', text)\n",
        "\n",
        "    # text = re.sub(r\"http\\S+\", \"*\", text)  # http link -> '*'\n",
        "\n",
        "    # text = re.sub(r\"@\\S+\", \"@\", text)   # mention -> '@'\n",
        "    # text = re.sub(r\"@[^\\s]+\", \"@\", text)   # mention -> '@'\n",
        "\n",
        "    # sent = re.sub(r\"(#)(\\S+)\", r'\\1 \\2', sent)\n",
        "    # sent = re.sub(r'([^\\s\\w@#\\*]|_)+', '', sent) # Erasing Special Characters\n",
        "\n",
        "    # Replace '&amp;' with '&'\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def preprocessing_for_bert(data): \n",
        "\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs (빈 리스트 2개 생성)\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in data:\n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=text_preprocessing(sent),  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            # max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            pad_to_max_length=True,         # Pad sentence to max length\n",
        "            # return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True,      # Return attention mask\n",
        "\n",
        "            # max_length=True,                  # Max length to truncate/pad\n",
        "            padding='max_length'\n",
        "        )\n",
        "\n",
        "        # Add the outputs to the lists (위의 빈 리스트에 상응하는 값 추가)\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors (리스트들을 텐서화)\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "\n",
        "def initialize_model(epochs=4):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertTweetClassifier(freeze_bert=False)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),\n",
        "                      lr=5e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0,  # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler, criterion\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts += 1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(\n",
        "                t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(\n",
        "                    f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "\n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy\n",
        "\n",
        "def bert_predict(model, test_dataloader):\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        all_logits.append(logits)\n",
        "    \n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return probs\n",
        "\n",
        "def data_process(X_train, y_train, X_val, y_val, batch_size=16):\n",
        "  # Concatenate train data and test data\n",
        "  all_tweets = np.concatenate([X_train, X_val])\n",
        "\n",
        "  # Encode our concatenated data\n",
        "  encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_tweets]\n",
        "\n",
        "  # Find the maximum length\n",
        "  max_len = max([len(sent) for sent in encoded_tweets])\n",
        "  print('Max length: ', max_len)\n",
        "\n",
        "  # Specify `MAX_LEN`\n",
        "  MAX_LEN = max_len\n",
        "\n",
        "  # Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "  print('\\nTokenizing data...')\n",
        "  train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
        "  val_inputs, val_masks = preprocessing_for_bert(X_val)\n",
        "\n",
        "  # Convert other data types to torch.Tensor\n",
        "  train_labels = torch.tensor(y_train)\n",
        "  val_labels = torch.tensor(y_val)\n",
        "\n",
        "  # For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "  batch_size = 8\n",
        "\n",
        "  # Create the DataLoader for our training set\n",
        "  train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Create the DataLoader for our validation set\n",
        "  val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "  val_sampler = SequentialSampler(val_data)\n",
        "  val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "\n",
        "  return train_dataloader, val_dataloader\n",
        "\n",
        "def train_process(train_dataloader, val_dataloader, epoch=4):\n",
        "  set_seed(42)    # Set seed for reproducibility\n",
        "  bert_classifier, optimizer, scheduler, loss_fn = initialize_model(epochs=epoch)\n",
        "\n",
        "  train(bert_classifier, train_dataloader, loss_fn, epochs=4, evaluation=True)\n",
        "\n",
        "  return bert_classifier\n",
        "\n",
        "def testing_process(bert_classifier, X_val, y_val):\n",
        "  #  Run `preprocessing_for_bert` on the test set\n",
        "  print('Tokenizing data...')\n",
        "  test_inputs, test_masks = preprocessing_for_bert(X_val)\n",
        "\n",
        "  # Create the DataLoader for our test set\n",
        "  test_dataset = TensorDataset(test_inputs, test_masks)\n",
        "  test_sampler = SequentialSampler(test_dataset)\n",
        "  test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)\n",
        "\n",
        "  # Compute predicted probabilities on the test set\n",
        "  probs = bert_predict(bert_classifier, test_dataloader)\n",
        "\n",
        "  # Get predictions from the probabilities\n",
        "  threshold = 0.5\n",
        "  preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
        "\n",
        "  # Number of tweets predicted non-negative\n",
        "  print(\"Number of tweets predicted as Rumor: \", preds.sum())\n",
        "\n",
        "  preds = np.argmax(probs, axis = 1)\n",
        "  print(\"\\n\",classification_report(y_val, preds))\n",
        "  print('Accuracy Score:\\t',accuracy_score(y_val, preds))\n",
        "  print('Precision Score:\\t', str(precision_score(y_val,preds)))\n",
        "  print('Recall Score:\\t\\t' + str(recall_score(y_val,preds)))\n",
        "  print('F1 Score:\\t',f1_score(y_val, preds, zero_division=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BertTweetClassifier(nn.Module): # Create the BertClassfier class\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertTweetClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = 768, 50, 2\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
        "\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "\n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def embedding(self, input_ids):\n",
        "        outputs = self.bert(input_ids=input_ids)\n",
        "\n",
        "        # outputs = (sequence_output, pooled_output,) + encoder_outputs[1:]  # add hidden_states and attentions if they are here\n",
        "        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M181HsetpgSU"
      },
      "source": [
        "## Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "v334DoYDfhWs",
        "outputId": "2a3379fd-0900-4d9e-d198-b4a6333d585b"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-faca395afa4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/_PHEME_text.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/_PHEME_target.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/_PHEMEext_text.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 107] Transport endpoint is not connected: './data/_PHEME_text.csv'"
          ]
        }
      ],
      "source": [
        "raw_text = pd.read_csv('./data/_PHEME_text.csv')\n",
        "y = pd.read_csv('./data/_PHEME_target.csv')\n",
        "data = pd.concat([raw_text.text, y], axis=1).reset_index(drop=True)\n",
        "val = pd.read_csv('data/_PHEMEext_text.csv')\n",
        "\n",
        "X_train = data.text.values\n",
        "y_train = data.target.values\n",
        "\n",
        "X_val = val.drop(['Event'],axis=1).text.values\n",
        "y_val = val.target.values\n",
        "\n",
        "rhi_data = pd.read_csv('data/_RHI_text.csv').text.values\n",
        "rhi_y = pd.read_csv('data/_RHI_text.csv').isRumor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwRCb2rGsFQ8",
        "outputId": "1cbd2912-896a-4471-e4ef-3404c95a3919"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5802,)\n",
            "(5802,)\n",
            "(485,)\n",
            "(485,)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZUhevayfhWu",
        "outputId": "4294347a-ac32-4e50-9641-1c9fb768233c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "emoji is not installed, thus not converting emoticons or emojis into text. Please install emoji: pip3 install emoji\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length:  58\n",
            "\n",
            "Tokenizing data...\n"
          ]
        }
      ],
      "source": [
        "device = getDevice()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
        "train_dataloader, val_dataloader = data_process(X_train, y_train, X_val, y_val, batch_size=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-HVi7I2uVey",
        "outputId": "b0f83d49-358d-420f-d664-d4b40a576f6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():       \n",
        "      device = torch.device(\"cuda\")\n",
        "      print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "      print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vtfc0vdYfhWu",
        "outputId": "e3c09e56-86a9-4cc1-808e-b1fd61430657"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.657699   |     -      |     -     |   4.73   \n",
            "   1    |   40    |   0.572159   |     -      |     -     |   4.54   \n",
            "   1    |   60    |   0.599299   |     -      |     -     |   4.65   \n",
            "   1    |   80    |   0.502215   |     -      |     -     |   4.70   \n",
            "   1    |   100   |   0.424222   |     -      |     -     |   4.74   \n",
            "   1    |   120   |   0.471874   |     -      |     -     |   4.66   \n",
            "   1    |   140   |   0.555176   |     -      |     -     |   4.64   \n",
            "   1    |   160   |   0.485429   |     -      |     -     |   4.55   \n",
            "   1    |   180   |   0.442740   |     -      |     -     |   4.51   \n",
            "   1    |   200   |   0.574039   |     -      |     -     |   4.47   \n",
            "   1    |   220   |   0.487704   |     -      |     -     |   4.47   \n",
            "   1    |   240   |   0.527812   |     -      |     -     |   4.45   \n",
            "   1    |   260   |   0.427125   |     -      |     -     |   4.44   \n",
            "   1    |   280   |   0.511334   |     -      |     -     |   4.43   \n",
            "   1    |   300   |   0.385671   |     -      |     -     |   4.40   \n",
            "   1    |   320   |   0.463129   |     -      |     -     |   4.42   \n",
            "   1    |   340   |   0.411448   |     -      |     -     |   4.40   \n",
            "   1    |   360   |   0.424775   |     -      |     -     |   4.43   \n",
            "   1    |   380   |   0.455251   |     -      |     -     |   4.43   \n",
            "   1    |   400   |   0.397658   |     -      |     -     |   4.45   \n",
            "   1    |   420   |   0.486652   |     -      |     -     |   4.47   \n",
            "   1    |   440   |   0.442327   |     -      |     -     |   4.49   \n",
            "   1    |   460   |   0.430557   |     -      |     -     |   4.50   \n",
            "   1    |   480   |   0.434291   |     -      |     -     |   4.51   \n",
            "   1    |   500   |   0.459816   |     -      |     -     |   4.51   \n",
            "   1    |   520   |   0.394905   |     -      |     -     |   4.52   \n",
            "   1    |   540   |   0.542678   |     -      |     -     |   4.60   \n",
            "   1    |   560   |   0.493274   |     -      |     -     |   4.54   \n",
            "   1    |   580   |   0.518410   |     -      |     -     |   4.49   \n",
            "   1    |   600   |   0.466646   |     -      |     -     |   4.48   \n",
            "   1    |   620   |   0.421744   |     -      |     -     |   4.46   \n",
            "   1    |   640   |   0.436818   |     -      |     -     |   4.47   \n",
            "   1    |   660   |   0.485056   |     -      |     -     |   4.46   \n",
            "   1    |   680   |   0.715749   |     -      |     -     |   4.45   \n",
            "   1    |   700   |   0.601952   |     -      |     -     |   4.43   \n",
            "   1    |   720   |   0.416572   |     -      |     -     |   4.43   \n",
            "   1    |   725   |   0.777217   |     -      |     -     |   1.00   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.489019   |  0.858612  |   62.62   |  167.24  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "set_seed(42)    # Set seed for reproducibility\n",
        "bert_classifier, optimizer, scheduler, loss_fn = initialize_model(epochs=4)\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=1, evaluation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqI9LRVbfhWu"
      },
      "outputs": [],
      "source": [
        "torch.save(bert_classifier.state_dict(), './Model/BERTweet_raw_to_fine_tune_ord4.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "1E0J9rlrfhWu",
        "outputId": "94c6ed92-673a-473e-a3c8-1cd4973459bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.4855\n",
            "Accuracy: 62.89%\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Compute predicted probabilities on the test set\n",
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "\n",
        "# Evaluate the Bert classifier\n",
        "evaluate_roc(probs, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxHGJj7BfhWv",
        "outputId": "cf1e94c2-8739-4508-8805-60781b2c3b7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing data...\n",
            "Number of tweets predicted as Rumor:  391\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.18      0.20       116\n",
            "           1       0.76      0.80      0.78       369\n",
            "\n",
            "    accuracy                           0.65       485\n",
            "   macro avg       0.49      0.49      0.49       485\n",
            "weighted avg       0.63      0.65      0.64       485\n",
            "\n",
            "0.6536082474226804\n",
            "0.7789473684210527\n"
          ]
        }
      ],
      "source": [
        "PATH = './Model/BERTweet_raw_to_fine_tune_ord4.pt'\n",
        "bert_classifier.load_state_dict(torch.load(PATH))\n",
        "testing_process(bert_classifier, X_val, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yu1vQFK_fhWv",
        "outputId": "05e4aaa2-74fa-4254-a47b-6218057d8e92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing data...\n",
            "Number of tweets predicted as Rumor:  3823\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.11      0.33      0.17       475\n",
            "           1       0.92      0.74      0.82      4752\n",
            "\n",
            "    accuracy                           0.70      5227\n",
            "   macro avg       0.52      0.54      0.49      5227\n",
            "weighted avg       0.84      0.70      0.76      5227\n",
            "\n",
            "0.7013583317390473\n",
            "0.8179591836734694\n"
          ]
        }
      ],
      "source": [
        "rhi_data = pd.read_csv('data/_RHI_text.csv')\n",
        "rhi_y = pd.read_csv('data/_RHI_text.csv')\n",
        "X_test = rhi_data.text.values\n",
        "y_test = rhi_y.isRumor.values\n",
        "\n",
        "PATH = './Model/BERTweet_raw_to_fine_tune_ord4.pt'\n",
        "bert_classifier.load_state_dict(torch.load(PATH))\n",
        "testing_process(bert_classifier, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F08yfWtRAzty"
      },
      "source": [
        "## Embedding Only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
        "bert_classifier, optimizer, scheduler, loss_fn = initialize_model(epochs=1)\n",
        "PATH = './Model/BERTweet_raw_to_fine_tune_ord4.pt'\n",
        "bert_classifier.load_state_dict(torch.load(PATH))\n",
        "# testing_process(bert_classifier, X_val, y_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocessing_for_bert(data):\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs (빈 리스트 2개 생성)\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in data:\n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=text_preprocessing(sent),  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            pad_to_max_length=True,         # Pad sentence to max length\n",
        "            # return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True      # Return attention mask\n",
        "        )\n",
        "\n",
        "        # Add the outputs to the lists (위의 빈 리스트에 상응하는 값 추가)\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors (리스트들을 텐서화)\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'raw_text' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a3f8a7a2ed4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing_for_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# text.dropna(inplace=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# text = text.sample(frac=0.4, random_state=999)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'raw_text' is not defined"
          ]
        }
      ],
      "source": [
        "text = raw_text.text\n",
        "input_ids, attention_masks = preprocessing_for_bert(text)\n",
        "# text.dropna(inplace=True)\n",
        "# text = text.sample(frac=0.4, random_state=999)\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
        "# bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
        "\n",
        "# sents = [tokenizer.tokenize(tweet) for tweet in text]\n",
        "len(input_ids)\n",
        "# embeddings = []\n",
        "# for sent in input_ids:\n",
        "#     with torch.no_grad():\n",
        "#         features = bert_classifier.embedding(input_ids)\n",
        "#     embeddings.append(features)\n",
        "\n",
        "# result = [torch.mean(features.last_hidden_state[-1], dim=0).tolist() for features in embeddings]\n",
        "\n",
        "last_hidden_states = bert_classifier.embedding(input_ids)\n",
        "features = last_hidden_states[0][:,0,:].numpy() # considering o only the [CLS] for each sentences \n",
        "features.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bertweetEmbed = pd.DataFrame(features)\n",
        "print(bertweetEmbed.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "all_logits = []\n",
        "\n",
        "# For each batch in our test set...\n",
        "for batch in test_dataloader:\n",
        "    # Load batch to GPU\n",
        "    b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "    # Compute logits\n",
        "    with torch.no_grad():\n",
        "        logits = model(b_input_ids, b_attn_mask)\n",
        "    all_logits.append(logits)\n",
        "\n",
        "# Concatenate logits from each batch\n",
        "all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "# Apply softmax to calculate probabilities\n",
        "probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "return probs\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "vtIMlTiMUFzS",
        "fJjwums-kI2t",
        "lqDU7M-oP54i",
        "_HtUKy5HP8X8"
      ],
      "include_colab_link": true,
      "name": "_CLF_BERT.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 2,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0660f1c2cada41d3bdfe2589d7184b56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "0c3cc67272e24e0c807af12b592c3a55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bd1e9fe81934cf3885154196e620c03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f954f9c5e5954c2bb96b5993a9395109",
              "IPY_MODEL_ce478aa49888428c8b784e7389e191f4"
            ],
            "layout": "IPY_MODEL_bd458a99a85e4a13afd1cf36b0a57ef7"
          }
        },
        "930e5993b2fb4a07943e2d0006d54dba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd458a99a85e4a13afd1cf36b0a57ef7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce478aa49888428c8b784e7389e191f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_930e5993b2fb4a07943e2d0006d54dba",
            "placeholder": "​",
            "style": "IPY_MODEL_f228d1ac81f6413a997d164c52f2ce02",
            "value": " 232k/232k [00:00&lt;00:00, 301kB/s]"
          }
        },
        "f228d1ac81f6413a997d164c52f2ce02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f954f9c5e5954c2bb96b5993a9395109": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c3cc67272e24e0c807af12b592c3a55",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0660f1c2cada41d3bdfe2589d7184b56",
            "value": 231508
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}