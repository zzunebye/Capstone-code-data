{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8u0ebtirRci"
      },
      "source": [
        "## Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQSdijCsmtPH",
        "outputId": "c9932f69-53b1-4d31-fb03-784b29bf35f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.0+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FaLWxNckcCp",
        "outputId": "4196fd01-160b-492f-8a1e-2d53cb73aad7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Mounted at ./MyDrive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "\n",
        "# 실행시 등장하는 URL을 클릭하여 허용해주면 인증KEY가 나타난다. 복사하여 URL아래 빈칸에 붙여넣으면 마운트에 성공하게된다.\n",
        "from google.colab import drive\n",
        "drive.mount('./MyDrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTiePp9YlA8R",
        "outputId": "525a66f7-82ae-4025-ac10-8b45c481c34b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/MyDrive/MyDrive/Capstone/code_data\n"
          ]
        }
      ],
      "source": [
        "cd MyDrive/MyDrive/Capstone/code_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAXZNl6bmM7J",
        "outputId": "d2fcf6e1-a3b5-4689-9c5a-dde102b68239"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/MyDrive/My Drive/Capstone/code_data\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ml-UXy1nkI_x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtIMlTiMUFzS"
      },
      "source": [
        "## TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adkKLRH4kI_7",
        "outputId": "b2382fb4-6ff7-4030-842a-d1e15e6bc565"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "# Uncomment to download \"stopwords\"\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def text_preprocessing(s):\n",
        "    \"\"\"\n",
        "    - Lowercase the sentence\n",
        "    - Change \"'t\" to \"not\"\n",
        "    - Remove \"@name\"\n",
        "    - Isolate and remove punctuations except \"?\"\n",
        "    - Remove other special characters\n",
        "    - Remove stop words except \"not\" and \"can\"\n",
        "    - Remove trailing whitespace\n",
        "    \"\"\"\n",
        "    s = s.lower()\n",
        "    # Change 't to 'not'\n",
        "    s = re.sub(r\"\\'t\", \" not\", s)\n",
        "    # Remove @name\n",
        "    s = re.sub(r'(@.*?)[\\s]', ' ', s)\n",
        "    # Isolate and remove punctuations except '?'\n",
        "    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', s)\n",
        "    s = re.sub(r'[^\\w\\s\\?]', ' ', s)\n",
        "    # Remove some special characters\n",
        "    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n",
        "    # Remove stopwords except 'not' and 'can'\n",
        "    s = \" \".join([word for word in s.split()\n",
        "                  if word not in stopwords.words('english')\n",
        "                  or word in ['not', 'can']])\n",
        "    # Remove trailing whitespace\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    \n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YahIdy3XkI_9",
        "outputId": "0e75a5bd-c64c-4394-a1c0-709ae6ef0ccc"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-838a357a3484>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'from sklearn.feature_extraction.text import TfidfVectorizer\\n\\n# Preprocess text\\nX_train_preprocessed = np.array([text_preprocessing(text) for text in X])\\nX_val_preprocessed = np.array([text_preprocessing(text) for text in test_X])\\n\\n# Calculate TF-IDF\\ntf_idf = TfidfVectorizer(ngram_range=(1, 3),\\n                         binary=True,\\n                         smooth_idf=False)\\nX_train_tfidf = tf_idf.fit_transform(X_train_preprocessed)\\nX_val_tfidf = tf_idf.transform(X_val_preprocessed)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-88ed875ee9d6>\u001b[0m in \u001b[0;36mtext_preprocessing\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([\\;\\:\\|•«\\n])'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Remove stopwords except 'not' and 'can'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     s = \" \".join([word for word in s.split()\n\u001b[0m\u001b[1;32m     28\u001b[0m                   \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                   or word in ['not', 'can']])\n",
            "\u001b[0;32m<ipython-input-19-88ed875ee9d6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Remove stopwords except 'not' and 'can'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     s = \" \".join([word for word in s.split()\n\u001b[0;32m---> 28\u001b[0;31m                   \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                   or word in ['not', 'can']])\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Remove trailing whitespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0m\u001b[1;32m     23\u001b[0m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0;32m---> 23\u001b[0;31m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Preprocess text\n",
        "X_train_preprocessed = np.array([text_preprocessing(text) for text in X])\n",
        "X_val_preprocessed = np.array([text_preprocessing(text) for text in test_X])\n",
        "\n",
        "# Calculate TF-IDF\n",
        "tf_idf = TfidfVectorizer(ngram_range=(1, 3),\n",
        "                         binary=True,\n",
        "                         smooth_idf=False)\n",
        "X_train_tfidf = tf_idf.fit_transform(X_train_preprocessed)\n",
        "X_val_tfidf = tf_idf.transform(X_val_preprocessed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86QdYvNrkI_9",
        "outputId": "4884b136-aa77-410d-f6e9-e0e201376b02"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-6ca3cc28359f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_train_tfidf.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLOxl1JDkI_9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "def get_auc_CV(model):\n",
        "    \"\"\"\n",
        "    Return the average AUC score from cross-validation.\n",
        "    \"\"\"\n",
        "    # Set KFold to shuffle data before the split\n",
        "    kf = StratifiedKFold(5, shuffle=True, random_state=1)\n",
        "\n",
        "    # Get AUC scores\n",
        "    auc = cross_val_score(\n",
        "        model, X_train_tfidf, y_train, scoring=\"roc_auc\", cv=kf)\n",
        "\n",
        "    return auc.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "3Bi1JS2IkI_9",
        "outputId": "6c986067-c6ca-4f99-9732-d2302aed588e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best alpha:  1.0\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "res = pd.Series([get_auc_CV(MultinomialNB(i))\n",
        "                 for i in np.arange(1, 10, 0.1)],\n",
        "                index=np.arange(1, 10, 0.1))\n",
        "\n",
        "best_alpha = np.round(res.idxmax(), 2)\n",
        "print('Best alpha: ', best_alpha)\n",
        "\n",
        "plt.plot(res)\n",
        "plt.title('AUC vs. Alpha')\n",
        "plt.xlabel('Alpha')\n",
        "plt.ylabel('AUC')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "PZi8GU6dkI_-",
        "outputId": "eae6e8c0-0ffe-49aa-922c-ab0ab749b5e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.9015\n",
            "Accuracy: 82.10%\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Compute predicted probabilities\n",
        "nb_model = MultinomialNB(alpha=1.8)\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "probs = nb_model.predict_proba(X_val_tfidf)\n",
        "\n",
        "# Evaluate the classifier\n",
        "evaluate_roc(probs, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "MKMBotb3lbfB",
        "outputId": "53155806-9c82-4a9b-b1d3-19917cce2562"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.5743\n",
            "Accuracy: 50.00%\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Compute predicted probabilities\n",
        "\n",
        "# X_data = pd.concat([X_train_tfidf, X_val_tfidf], axis=0)\n",
        "# y_data = pd.concat([X_train_tfidf, X_val_tfidf], axis=0)\n",
        "\n",
        "X_test = test_data.text.values\n",
        "y_test = test_data.isRumor.values\n",
        "\n",
        "X_preprocessed = np.array([text_preprocessing(text) for text in X])\n",
        "X_tfidf = tf_idf.transform(X_preprocessed)\n",
        "X_test_preprocessed = np.array([text_preprocessing(text) for text in X_test])\n",
        "X_test_tfidf = tf_idf.transform(X_test_preprocessed)\n",
        "\n",
        "nb_model = MultinomialNB(alpha=1.8)\n",
        "nb_model.fit(X_tfidf, y)\n",
        "probs = nb_model.predict_proba(X_test_tfidf)\n",
        "\n",
        "# Evaluate the classifier\n",
        "evaluate_roc(probs, test_data.isRumor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCYueNHxlO5C",
        "outputId": "a05dc3c3-72e8-4665-afc6-85b7249591d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      1.00      0.66       189\n",
            "           1       1.00      0.03      0.06       201\n",
            "\n",
            "    accuracy                           0.50       390\n",
            "   macro avg       0.75      0.51      0.36       390\n",
            "weighted avg       0.75      0.50      0.35       390\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "preds = np.argmax(probs, axis = 1)\n",
        "print(classification_report(test_data.isRumor, preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GBWZFMnrpO6"
      },
      "source": [
        "# 공통 Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYT-KkTor3QE"
      },
      "source": [
        "## Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VCz_QkG8ruDj"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "%matplotlib inline\n",
        "from transformers import BertTokenizer\n",
        "from sklearn.metrics import accuracy_score, roc_curve, auc, classification_report, f1_score\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "import random\n",
        "import time\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from sklearn.metrics import accuracy_score, roc_curve, auc, precision_score, recall_score, f1_score\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "from fetchData import fetchdata \n",
        "import __MLP\n",
        "\n",
        "import emoji\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RkOFN0Tpp5s"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UgmOuPxRFhof"
      },
      "outputs": [],
      "source": [
        "def getDevice():\n",
        "  if torch.cuda.is_available():       \n",
        "      device = torch.device(\"cuda\")\n",
        "      print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "      print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "  else:\n",
        "      print('No GPU available, using the CPU instead.')\n",
        "      device = torch.device(\"cpu\")\n",
        "  return device\n",
        "\n",
        "def evaluate_roc(probs, y_true):\n",
        "    \"\"\"\n",
        "    - Print AUC and accuracy on the test set\n",
        "    - Plot ROC\n",
        "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
        "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
        "    \"\"\"\n",
        "    preds = probs[:, 1]\n",
        "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(f'AUC: {roc_auc:.4f}')\n",
        "\n",
        "    # Get accuracy over the test set\n",
        "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "\n",
        "    # Plot ROC AUC\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.plot([0, 1], [0, 1], 'r--')\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()\n",
        "    \n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def preprocessing_for_bert(data): \n",
        "\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs (빈 리스트 2개 생성)\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in data:\n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=text_preprocessing(sent),  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            # max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            pad_to_max_length=True,         # Pad sentence to max length\n",
        "            # return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True,      # Return attention mask\n",
        "\n",
        "            # max_length=True,                  # Max length to truncate/pad\n",
        "            padding='max_length'\n",
        "        )\n",
        "\n",
        "        # Add the outputs to the lists (위의 빈 리스트에 상응하는 값 추가)\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors (리스트들을 텐서화)\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    best_acc = 0\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts += 1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(\n",
        "                t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(\n",
        "                    f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "\n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "\n",
        "        # early_stopping needs the validation loss to check if it has decresed, \n",
        "        # and if it has, it will make a checkpoint of the current model\n",
        "        \n",
        "        if val_accuracy > best_acc:\n",
        "            best_acc = val_accuracy\n",
        "            es = 0\n",
        "            # torch.save(model.state_dict(), \"model_\" + str(epoch_i) + 'weight.pt')\n",
        "            torch.save(model.state_dict(), \"checkpoint.pt\")\n",
        "        else:\n",
        "            es += 1\n",
        "            print(\"Counter {} of 5\".format(es))\n",
        "\n",
        "            if es > 1:\n",
        "                print(\"Early stopping with best_acc: \", best_acc, \"and val_acc for this epoch: \", val_accuracy, \"...\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy\n",
        "\n",
        "def bert_predict(model, test_dataloader):\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        all_logits.append(logits)\n",
        "    \n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return probs\n",
        "\n",
        "def data_process(X_train, y_train, X_val, y_val, batch_size=32):\n",
        "  # Concatenate train data and test data\n",
        "  all_tweets = np.concatenate([X_train, X_val])\n",
        "\n",
        "  # Encode our concatenated data\n",
        "  encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_tweets]\n",
        "\n",
        "  # Find the maximum length\n",
        "  max_len = max([len(sent) for sent in encoded_tweets])\n",
        "  print('Max length: ', max_len)\n",
        "\n",
        "  # Specify `MAX_LEN`\n",
        "  MAX_LEN = max_len\n",
        "\n",
        "  # Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "  print('\\nTokenizing data...')\n",
        "  train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
        "  val_inputs, val_masks = preprocessing_for_bert(X_val)\n",
        "\n",
        "  # Convert other data types to torch.Tensor\n",
        "  train_labels = torch.tensor(y_train)\n",
        "  val_labels = torch.tensor(y_val)\n",
        "\n",
        "  # For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "  batch_size = 8\n",
        "\n",
        "  # Create the DataLoader for our training set\n",
        "  train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Create the DataLoader for our validation set\n",
        "  val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "  val_sampler = SequentialSampler(val_data)\n",
        "  val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "\n",
        "  return train_dataloader, val_dataloader\n",
        "\n",
        "def train_process(train_dataloader, val_dataloader, epoch=4):\n",
        "  set_seed(42)    # Set seed for reproducibility\n",
        "  bert_classifier, optimizer, scheduler, loss_fn = initialize_model(epochs=epoch)\n",
        "\n",
        "  train(bert_classifier, train_dataloader, loss_fn, epochs=4, evaluation=True)\n",
        "\n",
        "  return bert_classifier\n",
        "\n",
        "def testing_process(bert_classifier, X_val, y_val):\n",
        "  #  Run `preprocessing_for_bert` on the test set\n",
        "  print('Tokenizing data...')\n",
        "  test_inputs, test_masks = preprocessing_for_bert(X_val)\n",
        "\n",
        "  # Create the DataLoader for our test set\n",
        "  test_dataset = TensorDataset(test_inputs, test_masks)\n",
        "  test_sampler = SequentialSampler(test_dataset)\n",
        "  test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)\n",
        "\n",
        "  # Compute predicted probabilities on the test set\n",
        "  probs = bert_predict(bert_classifier, test_dataloader)\n",
        "\n",
        "  # Get predictions from the probabilities\n",
        "  threshold = 0.5\n",
        "  preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
        "\n",
        "  # Number of tweets predicted non-negative\n",
        "  print(\"Number of tweets predicted as Rumor: \", preds.sum())\n",
        "\n",
        "  preds = np.argmax(probs, axis = 1)\n",
        "  print(\"\\n\",classification_report(y_val, preds))\n",
        "  print('Accuracy Score:\\t',accuracy_score(y_val, preds))\n",
        "  print('Precision Score:\\t', str(precision_score(y_val,preds)))\n",
        "  print('Recall Score:\\t\\t' + str(recall_score(y_val,preds)))\n",
        "  print('F1 Score:\\t',f1_score(y_val, preds, zero_division=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNAzuZdm-5eY"
      },
      "source": [
        "# BERT 최종\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RkOFN0Tpp5s"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UgmOuPxRFhof"
      },
      "outputs": [],
      "source": [
        "def text_preprocessing(text): # Create a function to tokenize a set of texts\n",
        "\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "    # Remove '@name'\n",
        "    text = text.lower()\n",
        "    # text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
        "    text = re.sub(r\"(#)(\\S+)\", r'\\1 \\2', text) # 해쉬태그 띄우기\n",
        "\n",
        "    # text = re.sub(r\"http\\S+\", \"*\", text)  # http link -> '*'\n",
        "\n",
        "    # text = re.sub(r\"@\\S+\", \"@\", text)   # mention -> '@'\n",
        "    # text = re.sub(r\"@[^\\s]+\", \"@\", text)   # mention -> '@'\n",
        "\n",
        "    # sent = re.sub(r\"(#)(\\S+)\", r'\\1 \\2', sent)\n",
        "    # sent = re.sub(r'([^\\s\\w@#\\*]|_)+', '', sent) # Erasing Special Characters\n",
        "\n",
        "    # Replace '&amp;' with '&'\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "    \n",
        "def initialize_model(epochs=4):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertClassifier(freeze_bert=True)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),\n",
        "                      # lr=5e-5,    # Default learning rate\n",
        "                      lr=2e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0,  # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler, criterion\n",
        "\n",
        "class BertClassifier(nn.Module): # Create the BertClassfier class\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = 768, 50, 2\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "\n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BertClassifier(nn.Module): # Create the BertClassfier class\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = 768, 50, 2\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "\n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk3m9D6kpsOP"
      },
      "source": [
        "## Executions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VAAH77uL_FiS"
      },
      "outputs": [],
      "source": [
        "raw_text = pd.read_csv('./data/_PHEME_text.csv')\n",
        "y = pd.read_csv('./data/_PHEME_target.csv')\n",
        "data = pd.concat([raw_text.text, y], axis=1).reset_index(drop=True)\n",
        "val = pd.read_csv('data/_PHEMEext_text.csv')\n",
        "\n",
        "X_train = data.text.values\n",
        "y_train = data.target.values\n",
        "\n",
        "X_val = val.drop(['Event'],axis=1).text.values\n",
        "y_val = val.target.values\n",
        "\n",
        "rhi_data = pd.read_csv('data/_RHI_text.csv').text.values\n",
        "rhi_y = pd.read_csv('data/_RHI_text.csv').isRumor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>Event</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Micheal Essien denying the Ebola rumours like ...</td>\n      <td>ebola-essien</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>No truth in internet rumours that I have contr...</td>\n      <td>ebola-essien</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Essien and his lawyers are considering to file...</td>\n      <td>ebola-essien</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Good news: The rumours that Michael Essien has...</td>\n      <td>ebola-essien</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Milan have stated that the reports about Essie...</td>\n      <td>ebola-essien</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>480</th>\n      <td>Ex-KGB Yuri Shvets at #Litvinenko inquiry-#Put...</td>\n      <td>putinmissing</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>481</th>\n      <td>Death came for Pratchett, picked up Putin for ...</td>\n      <td>putinmissing</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>482</th>\n      <td>the plot thickens - #putindead http://t.co/Vie...</td>\n      <td>putinmissing</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>483</th>\n      <td>Putin juggling enough instability. He would ma...</td>\n      <td>putinmissing</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>484</th>\n      <td>Vladimir #Putin #reappears on #television amid...</td>\n      <td>putinmissing</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>485 rows × 3 columns</p>\n</div>",
            "text/plain": "                                                  text         Event  target\n0    Micheal Essien denying the Ebola rumours like ...  ebola-essien       1\n1    No truth in internet rumours that I have contr...  ebola-essien       1\n2    Essien and his lawyers are considering to file...  ebola-essien       1\n3    Good news: The rumours that Michael Essien has...  ebola-essien       1\n4    Milan have stated that the reports about Essie...  ebola-essien       1\n..                                                 ...           ...     ...\n480  Ex-KGB Yuri Shvets at #Litvinenko inquiry-#Put...  putinmissing       0\n481  Death came for Pratchett, picked up Putin for ...  putinmissing       0\n482  the plot thickens - #putindead http://t.co/Vie...  putinmissing       0\n483  Putin juggling enough instability. He would ma...  putinmissing       0\n484  Vladimir #Putin #reappears on #television amid...  putinmissing       0\n\n[485 rows x 3 columns]"
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K84HzvLMbP0X",
        "outputId": "9a72777a-1493-4a65-ab56-2b1de267bf21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n",
            "Max length:  64\n",
            "\n",
            "Tokenizing data...\n"
          ]
        }
      ],
      "source": [
        "device = getDevice()\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "# train_dataloader, val_dataloader = data_process(X_train, y_train, X_val, y_val, batch_size=6)\n",
        "train_dataloader, val_dataloader = data_process(X_train[0:100], y_train[0:100], X_val[0:100], y_val[0:100], batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "fFfTQEHZ_Hc6",
        "outputId": "3a8ed61d-1ee3-49b3-8822-7c35745b9c56"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-4c4e6ac02afe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Set seed for reproducibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbert_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# train(bert_classifier, train_dataloader, val_dataloader, epochs=1, evaluation=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-6c03fc2a2a51>\u001b[0m in \u001b[0;36minitialize_model\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;31m# Tell PyTorch to run the model on GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0mbert_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;31m# Create the optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    669\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    670\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
          ]
        }
      ],
      "source": [
        "set_seed(42)    # Set seed for reproducibility\n",
        "bert_classifier, optimizer, scheduler, loss_fn = initialize_model(epochs=3)\n",
        "# train(bert_classifier, train_dataloader, val_dataloader, epochs=1, evaluation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyxq0fPUvbDA"
      },
      "outputs": [],
      "source": [
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=1, evaluation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwDvSUU-GrFW"
      },
      "outputs": [],
      "source": [
        "torch.save(bert_classifier.state_dict(), './Model/BERT_raw_to_fine_tune_ord4.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "16ynZzt_A95T",
        "outputId": "4ca200ac-fb0b-4323-d0aa-31b79eaa888b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.7044\n",
            "Accuracy: 45.36%\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Compute predicted probabilities on the test set\n",
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "\n",
        "# Evaluate the Bert classifier\n",
        "evaluate_roc(probs, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNR9pBOAGz8p",
        "outputId": "97c24e9c-27c5-4917-b4fc-1204c36845b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing data...\n",
            "Number of tweets predicted as Rumor:  122\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.92      0.45       116\n",
            "           1       0.93      0.31      0.46       369\n",
            "\n",
            "    accuracy                           0.45       485\n",
            "   macro avg       0.61      0.61      0.45       485\n",
            "weighted avg       0.78      0.45      0.46       485\n",
            "\n",
            "0.4536082474226804\n",
            "0.4602851323828921\n"
          ]
        }
      ],
      "source": [
        "PATH = './Model/BERT_raw_to_fine_tune_ord4.pt'\n",
        "bert_classifier.load_state_dict(torch.load(PATH))\n",
        "testing_process(bert_classifier, X_val, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eKaw3r2bAwg",
        "outputId": "e84554cf-46e2-4590-a4fe-84043acc2cc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing data...\n",
            "Number of tweets predicted as Rumor:  795\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.10      0.94      0.18       475\n",
            "           1       0.96      0.16      0.28      4752\n",
            "\n",
            "    accuracy                           0.23      5227\n",
            "   macro avg       0.53      0.55      0.23      5227\n",
            "weighted avg       0.89      0.23      0.27      5227\n",
            "\n",
            "0.23225559594413622\n",
            "0.27654588065621055\n"
          ]
        }
      ],
      "source": [
        "rhi_data = pd.read_csv('data/_RHI_text.csv')\n",
        "rhi_y = pd.read_csv('data/_RHI_text.csv')\n",
        "X_test = rhi_data.text.values\n",
        "y_test = rhi_y.isRumor.values\n",
        "\n",
        "PATH = './Model/BERT_raw_to_fine_tune_ord4.pt'\n",
        "bert_classifier.load_state_dict(torch.load(PATH))\n",
        "testing_process(bert_classifier, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSx6mKV2bgB_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2_qMkhDfb-6"
      },
      "source": [
        "# BERTweet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMD4JSaapG-T"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EbsgwY9ZfhWq"
      },
      "outputs": [],
      "source": [
        "def text_preprocessing(text): # Create a function to tokenize a set of texts\n",
        "\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "    # Remove '@name'\n",
        "    text = text.lower()\n",
        "    # text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
        "    # text = re.sub(r\"(#)(\\S+)\", r'\\1 \\2', text)\n",
        "\n",
        "    # text = re.sub(r\"http\\S+\", \"*\", text)  # http link -> '*'\n",
        "\n",
        "    # text = re.sub(r\"@\\S+\", \"@\", text)   # mention -> '@'\n",
        "    # text = re.sub(r\"@[^\\s]+\", \"@\", text)   # mention -> '@'\n",
        "\n",
        "    # sent = re.sub(r\"(#)(\\S+)\", r'\\1 \\2', sent)\n",
        "    # sent = re.sub(r'([^\\s\\w@#\\*]|_)+', '', sent) # Erasing Special Characters\n",
        "\n",
        "    # Replace '&amp;' with '&'\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def initialize_model(epochs=4, freeze_bert=False):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertTweetClassifier(freeze_bert=False)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),\n",
        "                      lr=5e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0,  # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler, criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETnWSkJ7oRqF"
      },
      "source": [
        "### Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5P3_E6mOoRqF"
      },
      "outputs": [],
      "source": [
        "class BertTweetClassifier(nn.Module): # Create the BertClassfier class\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertTweetClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = 768, 50, 2\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
        "\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "\n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def embedding(self, input_ids):\n",
        "        outputs = self.bert(input_ids=input_ids)\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        return last_hidden_state_cls  # sequence_output, pooled_output, (hidden_states), (attentions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M181HsetpgSU"
      },
      "source": [
        "## Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "v334DoYDfhWs"
      },
      "outputs": [],
      "source": [
        "raw_text = pd.read_csv('./data/_PHEME_text.csv')\n",
        "y = pd.read_csv('./data/_PHEME_target.csv')\n",
        "data = pd.concat([raw_text.text, y], axis=1).reset_index(drop=True)\n",
        "val = pd.read_csv('data/_PHEMEext_text.csv')\n",
        "\n",
        "X_train = data.text.values\n",
        "y_train = data.target.values\n",
        "\n",
        "X_val = val.drop(['Event'],axis=1).text.values\n",
        "y_val = val.target.values\n",
        "\n",
        "rhi_data = pd.read_csv('data/_RHI_text.csv').text.values\n",
        "rhi_y = pd.read_csv('data/_RHI_text.csv').isRumor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwRCb2rGsFQ8",
        "outputId": "581cc42b-2d21-4f0d-d0db-5736ad4fa1ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5802,) (5802,)\n",
            "(485,) (485,)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape, y_train.shape)\n",
        "print(X_val.shape, y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258,
          "referenced_widgets": [
            "3a7e702d08a4446ea50b410f2e9843ab",
            "ce795ec9283d46b8927ddc7dc93a2d32",
            "3111d583f32540d3a18bb468741ff888",
            "d202a9f02ebf442e88967fd5d6ddd303",
            "7d0cbaf5fa69439e836c0a49b53b3b42",
            "feb22d4ea5124d13aa49a849eee755f1",
            "72f3f4bbc9d740d4b4c099f8890c84c8",
            "69415ec2eff448f09502f6bb35dd6345",
            "a0e9851bf5b64d13abb59af9b7918082",
            "0ab9fc781a51454ca816230c73d55538",
            "10b4a6adb70440168442d2748c249915",
            "e5a6031cb1d04c4abd7ec69e91eabbe8",
            "b96f5ef0297f4936bd403f02198c50b8",
            "07a9ca0cf13f462f830bba75a4910399",
            "2fa3a91087fb4213b65d8aea738b3c9a",
            "375cc6ce64ce4791835ce34cb3bc2b8c",
            "86ae2530281341bcb7038f6fc347f8b8",
            "772efbe2ccc143f4b182348720311d9e",
            "cf606c73bf674f11b1a6d6db97fc5fff",
            "f1759ca550bf44d0a33c3d86a4347154",
            "af99588ee18e413ba551a3ffd68861de",
            "9c9dce7a03fb4e9bb773a8e49b399fcc",
            "c6ae66940c8d4d6cb1f1a9d1e66c8a2b",
            "2cb51beb13da4c3688008a052ab82ee3"
          ]
        },
        "id": "4ZUhevayfhWu",
        "outputId": "e9b5d796-079f-4433-ac42-31205d4e8de6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No GPU available, using the CPU instead.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length:  58\n",
            "\n",
            "Tokenizing data...\n"
          ]
        }
      ],
      "source": [
        "device = getDevice()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
        "train_dataloader, val_dataloader = data_process(X_train, y_train, X_val, y_val, batch_size=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vtfc0vdYfhWu",
        "outputId": "e3c09e56-86a9-4cc1-808e-b1fd61430657"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.657699   |     -      |     -     |   4.73   \n",
            "   1    |   40    |   0.572159   |     -      |     -     |   4.54   \n",
            "   1    |   60    |   0.599299   |     -      |     -     |   4.65   \n",
            "   1    |   80    |   0.502215   |     -      |     -     |   4.70   \n",
            "   1    |   100   |   0.424222   |     -      |     -     |   4.74   \n",
            "   1    |   120   |   0.471874   |     -      |     -     |   4.66   \n",
            "   1    |   140   |   0.555176   |     -      |     -     |   4.64   \n",
            "   1    |   160   |   0.485429   |     -      |     -     |   4.55   \n",
            "   1    |   180   |   0.442740   |     -      |     -     |   4.51   \n",
            "   1    |   200   |   0.574039   |     -      |     -     |   4.47   \n",
            "   1    |   220   |   0.487704   |     -      |     -     |   4.47   \n",
            "   1    |   240   |   0.527812   |     -      |     -     |   4.45   \n",
            "   1    |   260   |   0.427125   |     -      |     -     |   4.44   \n",
            "   1    |   280   |   0.511334   |     -      |     -     |   4.43   \n",
            "   1    |   300   |   0.385671   |     -      |     -     |   4.40   \n",
            "   1    |   320   |   0.463129   |     -      |     -     |   4.42   \n",
            "   1    |   340   |   0.411448   |     -      |     -     |   4.40   \n",
            "   1    |   360   |   0.424775   |     -      |     -     |   4.43   \n",
            "   1    |   380   |   0.455251   |     -      |     -     |   4.43   \n",
            "   1    |   400   |   0.397658   |     -      |     -     |   4.45   \n",
            "   1    |   420   |   0.486652   |     -      |     -     |   4.47   \n",
            "   1    |   440   |   0.442327   |     -      |     -     |   4.49   \n",
            "   1    |   460   |   0.430557   |     -      |     -     |   4.50   \n",
            "   1    |   480   |   0.434291   |     -      |     -     |   4.51   \n",
            "   1    |   500   |   0.459816   |     -      |     -     |   4.51   \n",
            "   1    |   520   |   0.394905   |     -      |     -     |   4.52   \n",
            "   1    |   540   |   0.542678   |     -      |     -     |   4.60   \n",
            "   1    |   560   |   0.493274   |     -      |     -     |   4.54   \n",
            "   1    |   580   |   0.518410   |     -      |     -     |   4.49   \n",
            "   1    |   600   |   0.466646   |     -      |     -     |   4.48   \n",
            "   1    |   620   |   0.421744   |     -      |     -     |   4.46   \n",
            "   1    |   640   |   0.436818   |     -      |     -     |   4.47   \n",
            "   1    |   660   |   0.485056   |     -      |     -     |   4.46   \n",
            "   1    |   680   |   0.715749   |     -      |     -     |   4.45   \n",
            "   1    |   700   |   0.601952   |     -      |     -     |   4.43   \n",
            "   1    |   720   |   0.416572   |     -      |     -     |   4.43   \n",
            "   1    |   725   |   0.777217   |     -      |     -     |   1.00   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.489019   |  0.858612  |   62.62   |  167.24  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "set_seed(42)    # Set seed for reproducibility\n",
        "bert_classifier, optimizer, scheduler, loss_fn = initialize_model(epochs=4)\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=1, evaluation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqI9LRVbfhWu"
      },
      "outputs": [],
      "source": [
        "torch.save(bert_classifier.state_dict(), './Model/BERTweet_raw_to_fine_tune_ord5.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "1E0J9rlrfhWu",
        "outputId": "94c6ed92-673a-473e-a3c8-1cd4973459bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.4855\n",
            "Accuracy: 62.89%\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Compute predicted probabilities on the test set\n",
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "\n",
        "# Evaluate the Bert classifier\n",
        "evaluate_roc(probs, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxHGJj7BfhWv",
        "outputId": "cf1e94c2-8739-4508-8805-60781b2c3b7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing data...\n",
            "Number of tweets predicted as Rumor:  391\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.18      0.20       116\n",
            "           1       0.76      0.80      0.78       369\n",
            "\n",
            "    accuracy                           0.65       485\n",
            "   macro avg       0.49      0.49      0.49       485\n",
            "weighted avg       0.63      0.65      0.64       485\n",
            "\n",
            "0.6536082474226804\n",
            "0.7789473684210527\n"
          ]
        }
      ],
      "source": [
        "PATH = './Model/BERTweet_raw_to_fine_tune_ord4.pt'\n",
        "bert_classifier.load_state_dict(torch.load(PATH))\n",
        "testing_process(bert_classifier, X_val, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yu1vQFK_fhWv",
        "outputId": "05e4aaa2-74fa-4254-a47b-6218057d8e92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing data...\n",
            "Number of tweets predicted as Rumor:  3823\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.11      0.33      0.17       475\n",
            "           1       0.92      0.74      0.82      4752\n",
            "\n",
            "    accuracy                           0.70      5227\n",
            "   macro avg       0.52      0.54      0.49      5227\n",
            "weighted avg       0.84      0.70      0.76      5227\n",
            "\n",
            "0.7013583317390473\n",
            "0.8179591836734694\n"
          ]
        }
      ],
      "source": [
        "rhi_data = pd.read_csv('data/_RHI_text.csv')\n",
        "rhi_y = pd.read_csv('data/_RHI_text.csv')\n",
        "X_test = rhi_data.text.values\n",
        "y_test = rhi_y.isRumor.values\n",
        "\n",
        "PATH = './Model/BERTweet_raw_to_fine_tune_ord4.pt'\n",
        "bert_classifier.load_state_dict(torch.load(PATH))\n",
        "testing_process(bert_classifier, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F08yfWtRAzty"
      },
      "source": [
        "# BERTweet Embedding Only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### BERT & EMOJI 이모지 다루는 차이 😂😂😂😂😂\n",
        "\n",
        "> Before applying fastBPE to the pre-training corpus of 850M English Tweets, we tokenized these Tweets using TweetTokenizer from the NLTK toolkit and used the emoji package to translate emotion icons into text strings (here, each icon is referred to as a word token). We also normalized the Tweets by converting user mentions and web/url links into special tokens @USER and HTTPURL, respectively. Thus it is recommended to also apply the same pre-processing step for BERTweet-based downstream applications w.r.t. the raw input Tweets. BERTweet provides this pre-processing step by enabling the normalization argument.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Normal text>\n",
            "https://www.naver.com @MichaelEssien that's a shame wanted to Invest INVEST in you😔😂😂 http://www.google.com \n",
            "\n",
            "['https://www.naver.com', '@MichaelEssien', \"that's\", 'a', 'shame', 'wanted', 'to', 'Invest', 'INVEST', 'in', 'you😔😂😂', 'http://www.google.com']\n",
            "\n",
            " [0, 62060, 8798, 3, 36110, 1626, 2237, 6354, 5238, 55508, 471, 10424, 818, 6139, 20, 11, 2536, 588, 9, 22630, 27227, 3969, 16, 3805, 3, 3, 3, 45565, 36110, 9485, 17048, 6354, 2] \n",
            "\n",
            "--------------------------------------------------\n",
            "<Demojized text>\n",
            "https://www.naver.com @MichaelEssien that's a shame wanted to Invest INVEST in you:pensive_face::face_with_tears_of_joy::face_with_tears_of_joy: http://www.google.com \n",
            "\n",
            "['https://www.naver.com', '@MichaelEssien', \"that's\", 'a', 'shame', 'wanted', 'to', 'Invest', 'INVEST', 'in', 'you:pensive_face::face_with_tears_of_joy::face_with_tears_of_joy:', 'http://www.google.com']\n",
            "\n",
            " [0, 62060, 8798, 3, 36110, 1626, 2237, 6354, 5238, 55508, 471, 10424, 818, 6139, 20, 11, 2536, 588, 9, 22630, 27227, 3969, 16, 3805, 3, 524, 9859, 1043, 3, 3, 16517, 1043, 88, 45565, 36110, 9485, 17048, 6354, 2] \n",
            "\n",
            "--------------------------------------------------\n",
            "<Demojized & http text>\n",
            "HTTPURL @MichaelEssien that's a shame wanted to Invest INVEST in you:pensive_face::face_with_tears_of_joy::face_with_tears_of_joy: HTTPURL \n",
            "\n",
            "['HTTPURL', '@MichaelEssien', \"that's\", 'a', 'shame', 'wanted', 'to', 'Invest', 'INVEST', 'in', 'you:pensive_face::face_with_tears_of_joy::face_with_tears_of_joy:', 'HTTPURL']\n",
            "\n",
            " [0, 10, 5238, 55508, 471, 10424, 818, 6139, 20, 11, 2536, 588, 9, 22630, 27227, 3969, 16, 3805, 3, 524, 9859, 1043, 3, 3, 16517, 1043, 88, 10, 2] \n",
            "\n",
            "--------------------------------------------------\n",
            "<Demojized & http & Mention text>\n",
            "HTTPURL @USER that's a shame wanted to Invest INVEST in you:pensive_face::face_with_tears_of_joy::face_with_tears_of_joy: HTTPURL \n",
            "\n",
            "['HTTPURL', '@USER', \"that's\", 'a', 'shame', 'wanted', 'to', 'Invest', 'INVEST', 'in', 'you:pensive_face::face_with_tears_of_joy::face_with_tears_of_joy:', 'HTTPURL']\n",
            "\n",
            " [0, 10, 5, 6139, 20, 11, 2536, 588, 9, 22630, 27227, 3969, 16, 3805, 3, 524, 9859, 1043, 3, 3, 16517, 1043, 88, 10, 2] \n",
            "\n",
            "--------------------------------------------------\n",
            "<Demojized-spaced & http & Mention text>\n",
            "HTTPURL @USER that's a shame wanted to Invest INVEST in you :pensive_face: :face_with_tears_of_joy: :face_with_tears_of_joy: HTTPURL \n",
            "\n",
            "['HTTPURL', '@USER', \"that's\", 'a', 'shame', 'wanted', 'to', 'Invest', 'INVEST', 'in', 'you', ':pensive_face:', ':face_with_tears_of_joy:', ':face_with_tears_of_joy:', 'HTTPURL']\n",
            "\n",
            " [0, 10, 5, 6139, 20, 11, 2536, 588, 9, 22630, 27227, 3969, 16, 14, 908, 88, 88, 10, 2] \n",
            "\n",
            "<FINAL>\n",
            "https://www.naver.com bhang96@gmail.com 12:30 @MichaelEssien that's a shame wanted to Invest INVEST in you😔😂😂 http://www.google.com\n",
            "https://www.naver.com bhang96@gmail.com 12:30 @michaelessien that's a shame wanted to invest invest in you😔😂😂 http://www.google.com\n",
            "\n",
            " [0, 62060, 8798, 3, 36110, 1626, 2237, 6354, 607, 13728, 12937, 9837, 15895, 5238, 13570, 43266, 14480, 818, 6139, 20, 11, 2536, 588, 9, 7761, 7761, 16, 3805, 3, 3, 3, 45565, 36110, 9485, 17048, 6354, 2] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# INPUT TWEET IS ALREADY NORMALIZED!\n",
        "# print(\"<Normal text>\")\n",
        "# line = \"@MichaelEssien that's a shame wanted to Invest in you😔😂😂\"\n",
        "# print(line)\n",
        "# print(tokenizer.encode(line),\"\\n\")\n",
        "# input_ids = torch.tensor([tokenizer.encode(line)])\n",
        "# # print(input_ids)\n",
        "\n",
        "print(\"<Normal text>\")\n",
        "line = \"https://www.naver.com @MichaelEssien that's a shame wanted to Invest INVEST in you😔😂😂 http://www.google.com\"\n",
        "input_ids = torch.tensor([tokenizer.encode(line)])\n",
        "print(line,\"\\n\")\n",
        "print(line.split())\n",
        "print(\"\\n\",tokenizer.encode(line),\"\\n\")\n",
        "print(\"--------------------------------------------------\")\n",
        "\n",
        "print(\"<Demojized text>\")\n",
        "# line = \"HTTPURL @USER that's a shame wanted to Invest INVEST in you:pensive_face::face_with_tears_of_joy::face_with_tears_of_joy: HTTPURL\"\n",
        "line = \"https://www.naver.com @MichaelEssien that's a shame wanted to Invest INVEST in you😔😂😂 http://www.google.com\"\n",
        "line = emoji.demojize(line)\n",
        "print(line,\"\\n\")\n",
        "print(line.split())\n",
        "print(\"\\n\",tokenizer.encode(line),\"\\n\")\n",
        "print(\"--------------------------------------------------\")\n",
        "\n",
        "print(\"<Demojized & http text>\")\n",
        "# line = \"HTTPURL @USER that's a shame wanted to Invest INVEST in you:pensive_face::face_with_tears_of_joy::face_with_tears_of_joy: HTTPURL\"\n",
        "line = \"https://www.naver.com @MichaelEssien that's a shame wanted to Invest INVEST in you😔😂😂 http://www.google.com\"\n",
        "line = emoji.demojize(line)\n",
        "line = re.sub(r\"http\\S+\", \"HTTPURL\", line)  # http link -> '*'\n",
        "print(line,\"\\n\")\n",
        "print(line.split())\n",
        "print(\"\\n\",tokenizer.encode(line),\"\\n\")\n",
        "print(\"--------------------------------------------------\")\n",
        "\n",
        "print(\"<Demojized & http & Mention text>\")\n",
        "# line = \"HTTPURL @USER that's a shame wanted to Invest INVEST in you:pensive_face::face_with_tears_of_joy::face_with_tears_of_joy: HTTPURL\"\n",
        "line = \"https://www.naver.com @MichaelEssien that's a shame wanted to Invest INVEST in you😔😂😂 http://www.google.com\"\n",
        "line = emoji.demojize(line)\n",
        "line = re.sub(r\"http\\S+\", \"HTTPURL\", line)  # http link -> '*'\n",
        "line = re.sub(r\"@\\S+\", \"@USER\", line)   # mention -> '@'\n",
        "print(line,\"\\n\")\n",
        "print(line.split())\n",
        "input_ids = torch.tensor([tokenizer.encode(line)])\n",
        "print(\"\\n\",tokenizer.encode(line),\"\\n\")\n",
        "print(\"--------------------------------------------------\")\n",
        "\n",
        "print(\"<Demojized-spaced & http & Mention text>\")\n",
        "# line = \"HTTPURL @USER that's a shame wanted to Invest INVEST in you:pensive_face::face_with_tears_of_joy::face_with_tears_of_joy: HTTPURL\"\n",
        "line = \"https://www.naver.com @MichaelEssien that's a shame wanted to Invest INVEST in you😔😂😂 http://www.google.com\"\n",
        "line = emoji.demojize(line)\n",
        "line = re.sub(r\"http\\S+\", \"HTTPURL\", line)  # http link -> '*'\n",
        "line = re.sub(r\"@\\S+\", \"@USER\", line)   # mention -> '@'\n",
        "line = re.sub(r':[^:]*:', r' \\g<0>', line)  # http link -> '*'\n",
        "# input_ids = torch.tensor([tokenizer.encode(line)])\n",
        "print(line,\"\\n\")\n",
        "print(line.split())\n",
        "print(\"\\n\",tokenizer.encode(line),\"\\n\")\n",
        "\n",
        "print(\"<FINAL>\")\n",
        "# line = \"HTTPURL @USER that's a shame wanted to Invest INVEST in you:pensive_face::face_with_tears_of_joy::face_with_tears_of_joy: HTTPURL\"\n",
        "line = \"https://www.naver.com bhang96@gmail.com 12:30 @MichaelEssien that's a shame wanted to Invest INVEST in you😔😂😂 http://www.google.com\"\n",
        "print(line)\n",
        "line = text_preprocessing(line)\n",
        "print(line)\n",
        "# input_ids = torch.tensor([tokenizer.encode(line)])\n",
        "# print(line.split())\n",
        "print(\"\\n\",tokenizer.encode(line),\"\\n\")\n",
        "\n",
        "# line = \"HTTPURL @USER that's a shame wanted to Invest INVEST in you pensive_face face_with_tears_of_joy face_with_tears_of_joy :grinning_face_with_big_eyes: HTTPURL  \"\n",
        "# input_ids = torch.tensor([tokenizer.encode(line)])\n",
        "# print(line)\n",
        "# print(tokenizer.encode(line))\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     features = bertweet(input_ids)  # Models outputs are now tuples\n",
        "# print(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'emoji' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-1c511b73e3a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"HTTPURL @MichaelEssien that's a shame wanted to Invest INVEST in you😔😂😂😃 http://www.google.com\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memoji\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memoji_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memoji\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdemojize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(emoji.get_emoji_regexp(),\"\\n\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'emoji' is not defined"
          ]
        }
      ],
      "source": [
        "text = \"HTTPURL @MichaelEssien that's a shame wanted to Invest INVEST in you😔😂😂😃 http://www.google.com\"\n",
        "print(emoji.emoji_count(text),\"\\n\")\n",
        "print(text)\n",
        "text = emoji.demojize(text)\n",
        "\n",
        "# print(emoji.get_emoji_regexp(),\"\\n\")\n",
        "# text=text.strip(':')\n",
        "# text = re.sub(r'(@.*?)[\\s]', '@USER ', text)\n",
        "text = re.sub(r\"@\\S+\", \"@USER\", text)   # mention -> '@'\n",
        "text = re.sub(r\"http\\S+\", \"HTTPURL\", text)  # http link -> '*'\n",
        "emojis = re.findall(r'(::)', text)\n",
        "print(emojis)\n",
        "# print(text,\"\\n\")\n",
        "text = re.sub(r':[^:]*:', r' \\g<0>', text)  # http link -> '*'\n",
        "print(text,\"\\n\")\n",
        "\n",
        "text=text.split()\n",
        "# text= re.sub(r'(:[!_\\-\\w]+:)', '', text)\n",
        "print(text,\"\\n\")\n",
        "# emojis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FUCNTION & DTA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No GPU available, using the CPU instead.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer \n",
        "import emoji\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tweetTokenizer = TweetTokenizer()\n",
        "\n",
        "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
        "\n",
        "device = getDevice()\n",
        "bert_classifier = BertTweetClassifier(freeze_bert=False)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n",
        "\n",
        "PATH = './Model/BERTweet_raw_embedding.pt'\n",
        "# bert_classifier.load_state_dict(torch.load(PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_text = pd.read_csv('./data/_PHEME_text.csv')\n",
        "y = pd.read_csv('./data/_PHEME_target.csv')\n",
        "data = pd.concat([raw_text.text, y], axis=1).reset_index(drop=True)\n",
        "val = pd.read_csv('data/_PHEMEext_text.csv')\n",
        "val_text = pd.read_csv('data/_PHEMEext_textonly.csv')\n",
        "\n",
        "X_train = data.text.values\n",
        "y_train = data.target.values\n",
        "\n",
        "X_val = val.drop(['Event'],axis=1).text.values\n",
        "y_val = val.target.values\n",
        "\n",
        "rhi_data = pd.read_csv('data/_RHI_text.csv').text.values\n",
        "rhi_y = pd.read_csv('data/_RHI_text.csv').isRumor\n",
        "\n",
        "X_test = val.drop(['Event'],axis=1).text.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "    \n",
        "# URL/USER -> TweetTkzr -> Emoji\n",
        "def text_preprocessing_simple(text, lemma=False, twttknzr=True): # Create a function to tokenize a set of texts\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "    text = re.sub(r\"http\\S+\", \"HTTPURL\", text)  # http link -> '*'\n",
        "    # text = re.sub(r\"@\\S+\", \"@USER\", text)   # mention -> '@'\n",
        "    \n",
        "    text = re.sub(r\"@[A-Za-z0-9]+\", \"@USER\", text)   # mention -> '@'\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "    if twttknzr==True:\n",
        "        text = tweetTokenizer.tokenize(text)\n",
        "    else:\n",
        "        text = text.split()\n",
        "    if lemma==True:\n",
        "        text = [lemmatizer.lemmatize(word) for word in text]\n",
        "    text = [emoji.demojize(token) for token in text]\n",
        "    return text\n",
        "    \n",
        "# URL/USER -> TweetTkzr -> Emoji\n",
        "\n",
        "def text_preprocessing_simple2(text, lemma=False, twttknzr=True): # Create a function to tokenize a set of texts\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "    text = re.sub(r\"http\\S+\", \"HTTPURL\", text)  # http link -> '*'\n",
        "    # text = re.sub(r\"@\\S+\", \"@USER\", text)   # mention -> '@'\n",
        "    \n",
        "    text = re.sub(r\"@[A-Za-z0-9]+\", \"@USER\", text)   # mention -> '@'\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "    if twttknzr==True:\n",
        "        text = tweetTokenizer.tokenize(text)\n",
        "    else:\n",
        "        text = text.split()\n",
        "    if lemma==True:\n",
        "        text = [lemmatizer.lemmatize(word) for word in text]\n",
        "    text = [emoji.demojize(token) for token in text]\n",
        "    return text\n",
        "\n",
        "# \n",
        "def text_preprocessing_simple3(text): # Create a function to tokenize a set of texts\n",
        "\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "\n",
        "    # text = re.sub(r\"http\\S+\", \"*\", text)  # http link -> '*'\n",
        "    # sent = re.sub(r'([^\\s\\w@#\\*]|_)+', '', sent) # Erasing Special Characters\n",
        "\n",
        "    text = emoji.demojize(text)\n",
        "    text = re.sub(r':[^:\\s]*:', r' \\g<0>', text)  # http link -> '*'\n",
        "\n",
        "    # text = re.sub(r':[^:\\s]*(?:::[^:\\s]*)*:', r' \\g<0> ', text)  # http link -> '*'\n",
        "\n",
        "    # text = re.sub(r\"\\n\", \" \", text)   # mention -> '@'\n",
        "    text = re.sub(r\"http\\S+\", \"HTTPURL\", text)  # http link -> '*'\n",
        "    # text = re.sub(r\"@\\S+\", \"@USER\", text)   # mention -> '@'\n",
        "    \n",
        "    text = re.sub(r\"@[A-Za-z0-9]+\", \"@USER\", text)   # mention -> '@'\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "    # text = tweetTokenizer.tokenize(text)\n",
        "    # text = [emoji.demojize(token) for token in text]\n",
        "    return text\n",
        "\n",
        "\n",
        "class BertTweetClassifier(nn.Module): # Create the BertClassfier class\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertTweetClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = 768, 50, 2\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
        "\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "\n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def embedding(self, input_ids):\n",
        "        outputs = self.bert(input_ids=input_ids)\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        return last_hidden_state_cls  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
        "\n",
        "\n",
        "def text_preprocessing(text): # Create a function to tokenize a set of texts\n",
        "\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "\n",
        "    # text = re.sub(r\"http\\S+\", \"*\", text)  # http link -> '*'\n",
        "    # sent = re.sub(r'([^\\s\\w@#\\*]|_)+', '', sent) # Erasing Special Characters\n",
        "\n",
        "    text = emoji.demojize(text)\n",
        "    text = re.sub(r':[^:\\s]*:', r' \\g<0>', text)  # http link -> '*'\n",
        "\n",
        "    # text = re.sub(r':[^:\\s]*(?:::[^:\\s]*)*:', r' \\g<0> ', text)  # http link -> '*'\n",
        "\n",
        "    # text = re.sub(r\"\\n\", \" \", text)   # mention -> '@'\n",
        "    text = re.sub(r\"http\\S+\", \"HTTPURL\", text)  # http link -> '*'\n",
        "    # text = re.sub(r\"@\\S+\", \"@USER\", text)   # mention -> '@'\n",
        "\n",
        "    text = re.sub(r\"@[A-Za-z0-9]+\", \"@USER\", text)   # mention -> '@'\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "    # text = tweetTokenizer.tokenize(text)\n",
        "    # text = [emoji.demojize(token) for token in text]\n",
        "\n",
        "    return text\n",
        "\n",
        "def text_preprocessing_brackets(text): # Create a function to tokenize a set of texts\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "\n",
        "    text = emoji.demojize(text)\n",
        "    text = re.sub(r':[^:\\s]*:', r' \\g<0>', text)  # http link -> '*'\n",
        "\n",
        "    # text = re.sub(r':[^:\\s]*(?:::[^:\\s]*)*:', r' \\g<0> ', text)  # http link -> '*'\n",
        "\n",
        "    # text = re.sub(r\"\\n\", \" \", text)   # mention -> '@'\n",
        "    text = re.sub(r\"http\\S+\", \"HTTPURL\", text)  # http link -> '*'\n",
        "    # text = re.sub(r\"@\\S+\", \"@USER\", text)   # mention -> '@'\n",
        "    text = re.sub(r\"(\\w+@\\w+.[\\w+]{2,4})\", \"<email>\", text)   \n",
        "\n",
        "    \n",
        "    text = re.sub(r\"@[A-Za-z0-9]+\", \"@USER\", text)   # mention -> '@'\n",
        "    text = re.sub(r\"\\d+(\\%|\\s\\bpercent\\b)\", \"<percentage>\", text)   # mention -> '@'\n",
        "    text = re.sub(r\"([0-1][0-9]|[2][0-3])[:|.|h]([0-5][0-9])\", \"<time>\", text)   # mention -> '@'\n",
        "    \n",
        "    text = re.sub(r\"([0-2][0-9]|(3)[0-1])(\\/)(((0)[0-9])|((1)[0-2]))(\\/)\\d{4}\", \"<date>\", text)  \n",
        "    text = re.sub(r\"(((0)[0-9])|((1)[0-2]))(\\/)([0-2][0-9]|(3)[0-1])(\\/)\\d{4}\", \"<date>\", text)  \n",
        "    text = re.sub(r\"\\d{4}(\\/)(((0)[0-9])|((1)[0-2]))(\\/)([0-2][0-9]|(3)[0-1])\", \"<date>\", text)  \n",
        "    text = re.sub(r'[0-9]+[.][0-9]+', \"<number>\", text)   # mention -> '@'\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def getEmbeddingBERT(bert_classifier, data):\n",
        "    bert_classifier.eval()\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    result = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for index, sent in enumerate(data):\n",
        "        encoded_sent = tokenizer.encode(\n",
        "            text=text_preprocessing_brackets(sent),  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            return_attention_mask=False,      # Return attention mask\n",
        "\n",
        "            padding='do_not_pad'\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            encoded_sent = torch.tensor([encoded_sent])\n",
        "            # print(encoded_sent)\n",
        "            last_hidden_state = bert_classifier.embedding(encoded_sent)\n",
        "\n",
        "        result.append(last_hidden_state)\n",
        "\n",
        "        if (index % 300 == 0 or index == len(input_ids)): print(\"Current number of processed tweets: {}\".format(index))\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Another Checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:\t\t It's Good news: The rumours that Michael Essien has contracted the Ebola virus are false. 30% @bg http://t.co/5d7hCL46mR http://t.co/VtGuLnjWBD 😅😅 @asdfas\n",
            "_____________________\n",
            "바로 토큰화:\t\t [0, 3, 417, 3, 47, 16368, 25, 1424, 3, 90, 32976, 6, 15930, 4892, 41, 3, 3, 3, 3, 3, 3, 3, 2]\n",
            "\t\t\t <s> <unk> Good <unk> The rumours that Michael <unk> has contracted the Ebola virus are <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
            "\n",
            "정제 한 후 토큰화:\t\t [0, 3, 417, 615, 22, 47, 16368, 25, 1424, 3, 90, 32976, 6, 15930, 4892, 41, 4501, 4, 597, 221, 5, 10, 10, 1411, 1411, 5, 2]\n",
            "\t\t\t <s> <unk> Good news : The rumours that Michael <unk> has contracted the Ebola virus are false . 30 % @USER HTTPURL HTTPURL :grinning_face_with_sweat: :grinning_face_with_sweat: @USER </s> \n",
            "\n",
            "정제만 하기:\t\t [0, 4543, 20, 417, 6308, 11891, 47, 16368, 25, 1424, 471, 10424, 818, 90, 32976, 6, 15930, 4892, 41, 41129, 4, 2774, 221, 5, 10, 10, 1411, 1411, 5, 2]\n",
            "\t\t\t <s> It's Good news: The rumours that Michael Essien has contracted the Ebola virus are false. 30% @USER HTTPURL HTTPURL :grinning_face_with_sweat: :grinning_face_with_sweat: @USER </s> \n",
            "\n",
            "정제만 하기 + split():\t [0, 3, 417, 3, 47, 16368, 25, 1424, 3, 90, 32976, 6, 15930, 4892, 41, 3, 3, 5, 10, 10, 1411, 1411, 5, 2]\n",
            "\t\t\t <s> <unk> Good <unk> The rumours that Michael <unk> has contracted the Ebola virus are <unk> <unk> @USER HTTPURL HTTPURL :grinning_face_with_sweat: :grinning_face_with_sweat: @USER </s> \n",
            "\n",
            "정제만 하기 + TwtTknzr:\t [0, 3, 417, 3, 47, 16368, 25, 1424, 3, 90, 32976, 6, 15930, 4892, 41, 3, 3, 5, 10, 10, 1411, 1411, 5, 2]\n",
            "\t\t\t <s> <unk> Good news : The rumours that Michael <unk> has contracted the Ebola virus are false . 30 % @USER HTTPURL HTTPURL : <unk> : : <unk> : @USER </s> \n",
            "\n",
            "정제만 하기 + Brackets:\t [0, 3, 417, 3, 47, 16368, 25, 1424, 3, 90, 32976, 6, 15930, 4892, 41, 3, 3, 5, 10, 10, 1411, 1411, 5, 2]\n",
            "\t\t\t <s> It's Good news: The rumours that Michael Essien has contracted the Ebola virus are false. <percentage> @USER HTTPURL HTTPURL :grinning_face_with_sweat: :grinning_face_with_sweat: @USER </s> \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# print(tokenizer.encode(\"😅\"),\"\\n\")\n",
        "# print(tokenizer.encode(\"grinning_face_with_sweat\"),\"\\n\")\n",
        "# print(tokenizer.encode(\":grinning_face_with_sweat:\"),\"\\n\\n\")\n",
        "sent = \"It's Good news: The rumours that Michael Essien has contracted the Ebola virus are false. 30% @bg http://t.co/5d7hCL46mR http://t.co/VtGuLnjWBD 😅😅 @asdfas\"\n",
        "print(\"Original:\\t\\t\",sent)\n",
        "print(\"_____________________\")\n",
        "print(\"바로 토큰화:\\t\\t\",tokenizer.encode(sent.split()))\n",
        "ids = tokenizer.encode(sent.split())\n",
        "print(\"\\t\\t\\t\",tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(ids)),\"\\n\")\n",
        "\n",
        "print(\"정제 한 후 토큰화:\\t\\t\",tokenizer.encode(text_preprocessing_simple(sent)))\n",
        "ids = tokenizer.encode(text_preprocessing_simple(sent))\n",
        "print(\"\\t\\t\\t\",tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(ids)),\"\\n\")\n",
        "\n",
        "print(\"정제만 하기:\\t\\t\",tokenizer.encode(text_preprocessing_pos(sent)))\n",
        "ids = tokenizer.encode(text_preprocessing_pos(sent))\n",
        "print(\"\\t\\t\\t\",tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(ids)),\"\\n\")\n",
        "\n",
        "print(\"정제만 하기 + split():\\t\",tokenizer.encode(text_preprocessing_pos(sent).split()))\n",
        "ids = tokenizer.encode(text_preprocessing_pos(sent).split())\n",
        "print(\"\\t\\t\\t\",tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(ids)),\"\\n\")\n",
        "\n",
        "print(\"정제만 하기 + TwtTknzr:\\t\",tokenizer.encode(text_preprocessing_pos(sent).split()))\n",
        "ids = tokenizer.encode(tweetTokenizer.tokenize(text_preprocessing_pos(sent)))\n",
        "print(\"\\t\\t\\t\",tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(ids)),\"\\n\")\n",
        "\n",
        "print(\"정제만 하기 + Brackets:\\t\",tokenizer.encode(text_preprocessing_brackets(sent).split()))\n",
        "ids = tokenizer.encode(text_preprocessing_brackets(sent))\n",
        "print(\"\\t\\t\\t\",tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(ids)),\"\\n\")\n",
        "# print(\"tweetTokenizer:\\t\",tokenizer.encode(result.split()),\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23/03/2019 03/23/2019 2019/03/23 child is jotgatten \n",
            "@JUNE 100% 1% | 12:30 13-40 12-30 12.30 1230 | bhang@adsf.com 100 http://www.naver.com\n",
            "\n",
            "[0, 3, 75, 1126, 3, 75, 1126, 3, 75, 1221, 1052, 17, 3, 5, 550, 221, 167, 221, 216, 15895, 3, 3, 3, 59394, 216, 3, 550, 10, 2]\n",
            "<s> <unk> / 2019 <unk> / 2019 <unk> / 23 child is <unk> @USER 100 % 1 % | 12:30 <unk> <unk> <unk> 1230 | <unk> 100 HTTPURL </s>\n",
            "\n",
            "[0, 3, 75, 1126, 3, 75, 1126, 3, 75, 1221, 1052, 17, 3, 5, 550, 221, 167, 221, 216, 15895, 3, 3, 3, 59394, 216, 3, 550, 10, 2]\n",
            "<s> <unk> / 2019 <unk> / 2019 <unk> / 23 child is <unk> @USER 100 % 1 % | 12:30 <unk> <unk> <unk> 1230 | <unk> 100 HTTPURL </s>\n",
            "\n",
            "[0, 40260, 26329, 1126, 26329, 40260, 1126, 43130, 26329, 1221, 1052, 17, 2998, 27582, 5023, 818, 5, 3919, 221, 1032, 221, 216, 15895, 14842, 971, 8447, 597, 9047, 597, 59394, 216, 607, 13728, 5238, 2479, 2899, 6354, 550, 10, 2]\n",
            "<s> 23/03/2019 03/23/2019 2019/03/23 child is jotgatten @USER 100% 1% | 12:30 13-40 12-30 12.30 1230 | bhang@USER.com 100 HTTPURL </s>\n",
            "\n",
            "[0, 10584, 47244, 241, 10584, 47244, 241, 10584, 47244, 241, 1052, 17, 2998, 27582, 5023, 818, 5, 10584, 47689, 8844, 241, 10584, 47689, 8844, 241, 216, 10584, 9354, 241, 14842, 971, 8447, 597, 10584, 9354, 241, 59394, 216, 10584, 18193, 1370, 241, 550, 10, 2]\n",
            "<date> <date> <date> child is jotgatten @USER <percentage> <percentage> | <time> 13-40 12-30 <time> 1230 | <email> 100 HTTPURL\n",
            "<s> <date> <date> <date> child is jotgatten @USER <percentage> <percentage> | <time> 13-40 12-30 <time> 1230 | <email> 100 HTTPURL </s>\n",
            "\n",
            "[0, 40260, 26329, 1126, 26329, 40260, 1126, 43130, 26329, 1221, 1052, 17, 2998, 27582, 5023, 818, 5, 3919, 221, 1032, 221, 216, 15895, 14842, 971, 8447, 597, 9047, 597, 59394, 216, 607, 13728, 5238, 2479, 2899, 6354, 550, 10, 2]\n",
            "<s> 23/03/2019 03/23/2019 2019/03/23 child is jotgatten @USER 100% 1% | 12:30 13-40 12-30 12.30 1230 | bhang@USER.com 100 HTTPURL </s>\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "첫번째는 \n",
        "'''\n",
        "sent = \"23/03/2019 03/23/2019 2019/03/23 child is jotgatten \\n@JUNE 100% 1% | 12:30 13-40 12-30 12.30 1230 | bhang@adsf.com 100 http://www.naver.com\"\n",
        "\n",
        "encoded_sent = tokenizer.encode(\n",
        "            text=text_preprocessing_simple(sent),  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            return_attention_mask=False,      # Return attention mask\n",
        "            padding='do_not_pad'\n",
        "        )\n",
        "encoded_sent2 = tokenizer.encode(\n",
        "            text=text_preprocessing_simple2(sent),  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            return_attention_mask=False,      # Return attention mask\n",
        "            padding='do_not_pad'\n",
        "        )\n",
        "encoded_sent3 = tokenizer.encode(\n",
        "            text=text_preprocessing_simple3(sent),  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            return_attention_mask=False,      # Return attention mask\n",
        "            padding='do_not_pad'\n",
        "        )\n",
        "encoded_sent4 = tokenizer.encode(\n",
        "            text=text_preprocessing_brackets(sent),  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            return_attention_mask=False,      # Return attention mask\n",
        "            padding='do_not_pad'\n",
        "        )\n",
        "\n",
        "encoded_sent5 = tokenizer.encode(\n",
        "            text=text_preprocessing(sent),  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            return_attention_mask=False,      # Return attention mask\n",
        "            padding='do_not_pad'\n",
        "        )\n",
        "print(sent)\n",
        "print()\n",
        "print(encoded_sent)\n",
        "print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(encoded_sent)))\n",
        "print()\n",
        "print(encoded_sent2)\n",
        "print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(encoded_sent2)))\n",
        "print()\n",
        "print(encoded_sent3)\n",
        "print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(encoded_sent3)))\n",
        "print()\n",
        "print(encoded_sent4)\n",
        "print(text_preprocessing_brackets(sent))\n",
        "print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(encoded_sent4)))\n",
        "print()\n",
        "print(encoded_sent5)\n",
        "print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(encoded_sent5)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['A', 'value', 'iss', 'trying', 'to', 'be', 'set', 'on', 'a', 'copys', 'of', 'an', 'slices', 'from', 'a', 'DataFrame']\n",
            "['A', 'value', 'iss', 'trying', 'to', 'be', 'set', 'on', 'a', 'copys', 'of', 'an', 'slices', 'from', 'a', 'DataFrame']\n",
            "['A', 'value', 'i', 'trying', 'to', 'be', 'set', 'on', 'a', 'copy', 'of', 'an', 'slice', 'from', 'a', 'DataFrame']\n",
            "['A', 'value', 'iss', 'trying', 'to', 'be', 'set', 'on', 'a', 'copys', 'of', 'an', 'slices', 'from', 'a', 'DataFrame']\n"
          ]
        }
      ],
      "source": [
        "sent = \"23/03/2019 03/23/2019 2019/03/23 child is jotgatten \\n@JUNE 100% 1% | 12:30 13-40 12-30 12.30 1230 | bhang@adsf.com 100 http://www.naver.com\"\n",
        "sent = \"A value iss trying to be set on a copys of an slices from a DataFrame\"\n",
        "# sent = raw_text.text[10]\n",
        "print(text_preprocessing_simple(sent))\n",
        "print(text_preprocessing_brackets(sent))\n",
        "print(text_preprocessing_simple(sent, lemma=True))\n",
        "print(text_preprocessing_simple(sent, twttknzr=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-52-a21585fe752d>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  raw_text['Event'][index] = text_preprocessing_simple(sent)\n"
          ]
        }
      ],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "for index, sent in enumerate(raw_text.text):\n",
        "    raw_text['Event'][index] = text_preprocessing_simple(sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokens = pd.DataFrame({})\n",
        "tokens = []\n",
        "# for index, sent in enumerate(raw_text.text):\n",
        "final = [text_preprocessing_simple(sent) for sent in raw_text.text]\n",
        "final = pd.DataFrame(final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BREAKING</td>\n      <td>:</td>\n      <td>Armed</td>\n      <td>man</td>\n      <td>takes</td>\n      <td>hostage</td>\n      <td>in</td>\n      <td>kosher</td>\n      <td>grocery</td>\n      <td>east</td>\n      <td>of</td>\n      <td>Paris</td>\n      <td>HTTPURL</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>#CharlieHebdo</td>\n      <td>killers</td>\n      <td>dead,</td>\n      <td>confirmed</td>\n      <td>by</td>\n      <td>gendarmerie.</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Top</td>\n      <td>French</td>\n      <td>cartoonists</td>\n      <td>Charb,</td>\n      <td>Cabu,</td>\n      <td>Wolinski,</td>\n      <td>Tignous</td>\n      <td>confirmed</td>\n      <td>among</td>\n      <td>dead</td>\n      <td>in</td>\n      <td>#Paris</td>\n      <td>#CharlieHebdo</td>\n      <td>attack.</td>\n      <td>Editor</td>\n      <td>is</td>\n      <td>critically</td>\n      <td>wounded.</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Police</td>\n      <td>have</td>\n      <td>surrounded</td>\n      <td>the</td>\n      <td>area</td>\n      <td>where</td>\n      <td>the</td>\n      <td>#CharlieHebdo</td>\n      <td>attack</td>\n      <td>suspects</td>\n      <td>are</td>\n      <td>believed</td>\n      <td>to</td>\n      <td>be</td>\n      <td>:</td>\n      <td>HTTPURL</td>\n      <td>HTTPURL</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>PHOTO</td>\n      <td>:</td>\n      <td>Armed</td>\n      <td>gunmen</td>\n      <td>face</td>\n      <td>police</td>\n      <td>officers</td>\n      <td>near</td>\n      <td>#CharlieHebdo</td>\n      <td>HQ</td>\n      <td>in</td>\n      <td>Paris</td>\n      <td>HTTPURL</td>\n      <td>HTTPURL</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5797</th>\n      <td>'I'll</td>\n      <td>ride</td>\n      <td>with</td>\n      <td>you'</td>\n      <td>http</td>\n      <td>://t.co/llZnuCAzg5</td>\n      <td>Australia</td>\n      <td>unites</td>\n      <td>during</td>\n      <td>#SydneySiege</td>\n      <td>HTTPURL</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>5798</th>\n      <td>Canada's</td>\n      <td>thoughts</td>\n      <td>and</td>\n      <td>prayers</td>\n      <td>are</td>\n      <td>with</td>\n      <td>our</td>\n      <td>Australian</td>\n      <td>friends.</td>\n      <td>#MartinPlace</td>\n      <td>#SydneySiege</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>5799</th>\n      <td>Every</td>\n      <td>non-muslim</td>\n      <td>in</td>\n      <td>the</td>\n      <td>world</td>\n      <td>must</td>\n      <td>watch</td>\n      <td>this</td>\n      <td>video</td>\n      <td>HTTPURL</td>\n      <td>&amp;</td>\n      <td>show</td>\n      <td>it</td>\n      <td>every</td>\n      <td>other</td>\n      <td>non-muslim!</td>\n      <td>#sydneysiege</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>5800</th>\n      <td>Suspect</td>\n      <td>in</td>\n      <td>Sydney</td>\n      <td>cafe</td>\n      <td>siege</td>\n      <td>identified</td>\n      <td>as</td>\n      <td>Man</td>\n      <td>Haron</td>\n      <td>Monis,</td>\n      <td>an</td>\n      <td>Iranian</td>\n      <td>granted</td>\n      <td>asylum</td>\n      <td>in</td>\n      <td>Australia</td>\n      <td>HTTPURL</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>5801</th>\n      <td>Australians</td>\n      <td>respond</td>\n      <td>to</td>\n      <td>racism</td>\n      <td>by</td>\n      <td>telling</td>\n      <td>#Muslim</td>\n      <td>community</td>\n      <td>#illridewithyou.</td>\n      <td>#sydneysiege</td>\n      <td>#MartinPlace</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n<p>5802 rows × 31 columns</p>\n</div>",
            "text/plain": "                 0           1            2          3      4   \\\n0          BREAKING           :        Armed        man  takes   \n1     #CharlieHebdo     killers        dead,  confirmed     by   \n2               Top      French  cartoonists     Charb,  Cabu,   \n3            Police        have   surrounded        the   area   \n4             PHOTO           :        Armed     gunmen   face   \n...             ...         ...          ...        ...    ...   \n5797          'I'll        ride         with       you'   http   \n5798       Canada's    thoughts          and    prayers    are   \n5799          Every  non-muslim           in        the  world   \n5800        Suspect          in       Sydney       cafe  siege   \n5801    Australians     respond           to     racism     by   \n\n                      5          6              7                 8   \\\n0                hostage         in         kosher           grocery   \n1           gendarmerie.       None           None              None   \n2              Wolinski,    Tignous      confirmed             among   \n3                  where        the  #CharlieHebdo            attack   \n4                 police   officers           near     #CharlieHebdo   \n...                  ...        ...            ...               ...   \n5797  ://t.co/llZnuCAzg5  Australia         unites            during   \n5798                with        our     Australian          friends.   \n5799                must      watch           this             video   \n5800          identified         as            Man             Haron   \n5801             telling    #Muslim      community  #illridewithyou.   \n\n                9             10        11             12       13      14  \\\n0             east            of     Paris        HTTPURL     None    None   \n1             None          None      None           None     None    None   \n2             dead            in    #Paris  #CharlieHebdo  attack.  Editor   \n3         suspects           are  believed             to       be       :   \n4               HQ            in     Paris        HTTPURL  HTTPURL    None   \n...            ...           ...       ...            ...      ...     ...   \n5797  #SydneySiege       HTTPURL      None           None     None    None   \n5798  #MartinPlace  #SydneySiege      None           None     None    None   \n5799       HTTPURL             &      show             it    every   other   \n5800        Monis,            an   Iranian        granted   asylum      in   \n5801  #sydneysiege  #MartinPlace      None           None     None    None   \n\n               15            16        17    18    19    20    21    22    23  \\\n0            None          None      None  None  None  None  None  None  None   \n1            None          None      None  None  None  None  None  None  None   \n2              is    critically  wounded.  None  None  None  None  None  None   \n3         HTTPURL       HTTPURL      None  None  None  None  None  None  None   \n4            None          None      None  None  None  None  None  None  None   \n...           ...           ...       ...   ...   ...   ...   ...   ...   ...   \n5797         None          None      None  None  None  None  None  None  None   \n5798         None          None      None  None  None  None  None  None  None   \n5799  non-muslim!  #sydneysiege      None  None  None  None  None  None  None   \n5800    Australia       HTTPURL      None  None  None  None  None  None  None   \n5801         None          None      None  None  None  None  None  None  None   \n\n        24    25    26    27    28    29    30  \n0     None  None  None  None  None  None  None  \n1     None  None  None  None  None  None  None  \n2     None  None  None  None  None  None  None  \n3     None  None  None  None  None  None  None  \n4     None  None  None  None  None  None  None  \n...    ...   ...   ...   ...   ...   ...   ...  \n5797  None  None  None  None  None  None  None  \n5798  None  None  None  None  None  None  None  \n5799  None  None  None  None  None  None  None  \n5800  None  None  None  None  None  None  None  \n5801  None  None  None  None  None  None  None  \n\n[5802 rows x 31 columns]"
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>",
            "text/plain": "Empty DataFrame\nColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\nIndex: []"
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# result.loc[result.iloc[:,:]=='BREAKING:']\n",
        "final[final.eq(r\"[:)|:-)|8-)|:-|:-))]\").any(1)]\n",
        "final[final.eq(r\"[:-(|:(|:-]+\").any(1)]\n",
        "# print(raw_text.iloc[2944])\n",
        "# raw_text[r\"<percentage>\" in raw_text.Event.values]\n",
        "search = final[final.eq(r\"<number>\").any(1)]\n",
        "# print(search.index.values)\n",
        "search\n",
        "# result.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 457,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>Event</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1024</th>\n      <td>-Muslim shooter - 1.7 Billion Muslims guilty\\n\\n-Black shooter - \"All Blacks are violent\"\\n\\n-White Shooter - \"Lone Wolf\"\\n\\n#CharlieHebdo</td>\n      <td>[-Muslim, shooter, -, &lt;number&gt;, Billion, Muslims, guilty, -Black, shooter, -, \"All, Blacks, are, violent\", -White, Shooter, -, \"Lone, Wolf\", #CharlieHebdo]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3272</th>\n      <td>A @germanwings A322 #4U9525, BCN-DUS,  with at least 143 passengers has crashed today at 9.39 UTC http://t.co/L7Wtodurt7</td>\n      <td>[A, @USER, A322, #4U9525,, BCN-DUS,, with, at, least, 143, passengers, has, crashed, today, at, &lt;number&gt;, UTC, HTTPURL]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3613</th>\n      <td>I don't get it ... The #Germanwings flight descended 31.000 feet in 10 minutes and didn't change course at all, acc. to Flightradar data ...</td>\n      <td>[I, don't, get, it, ..., The, #Germanwings, flight, descended, &lt;number&gt;, feet, in, 10, minutes, and, didn't, change, course, at, all,, acc., to, Flightradar, data, ...]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4187</th>\n      <td>Sad about #OttawaShooting. Worse, shooter might b guy who calls himself Muslim. Pls world dont judge 1.5 billion Muslims by some extremists.</td>\n      <td>[Sad, about, #OttawaShooting., Worse,, shooter, might, b, guy, who, calls, himself, Muslim., Pls, world, dont, judge, &lt;number&gt;, billion, Muslims, by, some, extremists.]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4388</th>\n      <td>In his last three games, Kobe is averaging 26.7 points on 42% shooting, 4.3 rebounds and 3.7 assists in 31.3 minutes. #MathIsFun</td>\n      <td>[In, his, last, three, games,, Kobe, is, averaging, &lt;number&gt;, points, on, &lt;percentage&gt;, shooting,, &lt;number&gt;, rebounds, and, &lt;number&gt;, assists, in, &lt;number&gt;, minutes., #MathIsFun]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5367</th>\n      <td>#NSW police have asked us, and others, not to broadcast demands from the #lindtcafe gunman. We will comply. I will have more to say at 7.30</td>\n      <td>[#NSW, police, have, asked, us,, and, others,, not, to, broadcast, demands, from, the, #lindtcafe, gunman., We, will, comply., I, will, have, more, to, say, at, &lt;number&gt;]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5724</th>\n      <td>The gunman is also holding 1.6 billion Muslims hostage. Islam is free from the crimes he's committing against innocents. #SydneySiege</td>\n      <td>[The, gunman, is, also, holding, &lt;number&gt;, billion, Muslims, hostage., Islam, is, free, from, the, crimes, he's, committing, against, innocents., #SydneySiege]</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "                                                                                                                                              text  \\\n1024    -Muslim shooter - 1.7 Billion Muslims guilty\\n\\n-Black shooter - \"All Blacks are violent\"\\n\\n-White Shooter - \"Lone Wolf\"\\n\\n#CharlieHebdo   \n3272                      A @germanwings A322 #4U9525, BCN-DUS,  with at least 143 passengers has crashed today at 9.39 UTC http://t.co/L7Wtodurt7   \n3613  I don't get it ... The #Germanwings flight descended 31.000 feet in 10 minutes and didn't change course at all, acc. to Flightradar data ...   \n4187  Sad about #OttawaShooting. Worse, shooter might b guy who calls himself Muslim. Pls world dont judge 1.5 billion Muslims by some extremists.   \n4388              In his last three games, Kobe is averaging 26.7 points on 42% shooting, 4.3 rebounds and 3.7 assists in 31.3 minutes. #MathIsFun   \n5367   #NSW police have asked us, and others, not to broadcast demands from the #lindtcafe gunman. We will comply. I will have more to say at 7.30   \n5724         The gunman is also holding 1.6 billion Muslims hostage. Islam is free from the crimes he's committing against innocents. #SydneySiege   \n\n                                                                                                                                                                                   Event  target  \n1024                         [-Muslim, shooter, -, <number>, Billion, Muslims, guilty, -Black, shooter, -, \"All, Blacks, are, violent\", -White, Shooter, -, \"Lone, Wolf\", #CharlieHebdo]       0  \n3272                                                             [A, @USER, A322, #4U9525,, BCN-DUS,, with, at, least, 143, passengers, has, crashed, today, at, <number>, UTC, HTTPURL]       1  \n3613            [I, don't, get, it, ..., The, #Germanwings, flight, descended, <number>, feet, in, 10, minutes, and, didn't, change, course, at, all,, acc., to, Flightradar, data, ...]       0  \n4187            [Sad, about, #OttawaShooting., Worse,, shooter, might, b, guy, who, calls, himself, Muslim., Pls, world, dont, judge, <number>, billion, Muslims, by, some, extremists.]       0  \n4388  [In, his, last, three, games,, Kobe, is, averaging, <number>, points, on, <percentage>, shooting,, <number>, rebounds, and, <number>, assists, in, <number>, minutes., #MathIsFun]       0  \n5367          [#NSW, police, have, asked, us,, and, others,, not, to, broadcast, demands, from, the, #lindtcafe, gunman., We, will, comply., I, will, have, more, to, say, at, <number>]       0  \n5724                     [The, gunman, is, also, holding, <number>, billion, Muslims, hostage., Islam, is, free, from, the, crimes, he's, committing, against, innocents., #SydneySiege]       0  "
          },
          "execution_count": 457,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_text.iloc[search.index.values]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "\"['tokens'] not in index\"",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-d4663c704533>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3028\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3029\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3030\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3032\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1314\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['tokens'] not in index\""
          ]
        }
      ],
      "source": [
        "pd.DataFrame(raw_text[['tokens','text']]).sample(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EMBEDDING PROCESS\n",
        "\n",
        "The proper method I chose is \n",
        "1. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No GPU available, using the CPU instead.\n"
          ]
        }
      ],
      "source": [
        "device = getDevice()\n",
        "bert_classifier = BertTweetClassifier(freeze_bert=False)\n",
        "\n",
        "# PATH = './Model/BERTweet_raw_embedding.pt'\n",
        "# bert_classifier.load_state_dict(torch.load(PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "JGR4vVZKqbhf"
      },
      "outputs": [],
      "source": [
        "def getEmbeddingBERT(bert_classifier, data):\n",
        "  bert_classifier.eval()\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  result = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for index, sent in enumerate(data):\n",
        "      encoded_sent = tokenizer.encode(\n",
        "          text=text_preprocessing(sent),  # Preprocess sentence\n",
        "          add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "          return_attention_mask=False,      # Return attention mask\n",
        "\n",
        "          padding='do_not_pad'\n",
        "      )\n",
        "      with torch.no_grad():\n",
        "          encoded_sent = torch.tensor([encoded_sent])\n",
        "          # print(encoded_sent)\n",
        "          last_hidden_state = bert_classifier.embedding(encoded_sent)\n",
        "\n",
        "      result.append(last_hidden_state)\n",
        "\n",
        "      if (index % 100 == 0 or index == len(input_ids)): print(\"Current number of processed tweets: {}\".format(index))\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of processed tweets: 0\n",
            "Current number of processed tweets: 100\n",
            "Current number of processed tweets: 200\n",
            "Current number of processed tweets: 300\n",
            "Current number of processed tweets: 400\n",
            "Current number of processed tweets: 500\n",
            "Current number of processed tweets: 600\n",
            "Current number of processed tweets: 700\n",
            "Current number of processed tweets: 800\n",
            "Current number of processed tweets: 900\n",
            "Current number of processed tweets: 1000\n",
            "Current number of processed tweets: 1100\n",
            "Current number of processed tweets: 1200\n",
            "Current number of processed tweets: 1300\n",
            "Current number of processed tweets: 1400\n",
            "Current number of processed tweets: 1500\n",
            "Current number of processed tweets: 1600\n",
            "Current number of processed tweets: 1700\n",
            "Current number of processed tweets: 1800\n",
            "Current number of processed tweets: 1900\n",
            "Current number of processed tweets: 2000\n",
            "Current number of processed tweets: 2100\n",
            "Current number of processed tweets: 2200\n",
            "Current number of processed tweets: 2300\n",
            "Current number of processed tweets: 2400\n",
            "Current number of processed tweets: 2500\n",
            "Current number of processed tweets: 2600\n",
            "Current number of processed tweets: 2700\n",
            "Current number of processed tweets: 2800\n",
            "Current number of processed tweets: 2900\n",
            "Current number of processed tweets: 3000\n",
            "Current number of processed tweets: 3100\n",
            "Current number of processed tweets: 3200\n",
            "Current number of processed tweets: 3300\n",
            "Current number of processed tweets: 3400\n",
            "Current number of processed tweets: 3500\n",
            "Current number of processed tweets: 3600\n",
            "Current number of processed tweets: 3700\n",
            "Current number of processed tweets: 3800\n",
            "Current number of processed tweets: 3900\n",
            "Current number of processed tweets: 4000\n",
            "Current number of processed tweets: 4100\n",
            "Current number of processed tweets: 4200\n",
            "Current number of processed tweets: 4300\n",
            "Current number of processed tweets: 4400\n",
            "Current number of processed tweets: 4500\n",
            "Current number of processed tweets: 4600\n",
            "Current number of processed tweets: 4700\n",
            "Current number of processed tweets: 4800\n",
            "Current number of processed tweets: 4900\n",
            "Current number of processed tweets: 5000\n",
            "Current number of processed tweets: 5100\n",
            "Current number of processed tweets: 5200\n",
            "Current number of processed tweets: 5300\n",
            "Current number of processed tweets: 5400\n",
            "Current number of processed tweets: 5500\n",
            "Current number of processed tweets: 5600\n",
            "Current number of processed tweets: 5700\n",
            "Current number of processed tweets: 5800\n"
          ]
        }
      ],
      "source": [
        "result_pheme = getEmbeddingBERT(bert_classifier,X_train)\n",
        "df_result_pheme = pd.DataFrame([sent[0].tolist() for sent in result_pheme]).add_prefix('BERTEmbed_')\n",
        "df_result_pheme.to_csv('./data/_PHEME_Bert_final_simple_nrmzd2.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6GMtJxgE0yX",
        "outputId": "507d3ab0-f445-4468-902a-5bb738b1ac5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of processed tweets: 0\n",
            "Current number of processed tweets: 100\n",
            "Current number of processed tweets: 200\n",
            "Current number of processed tweets: 300\n",
            "Current number of processed tweets: 400\n",
            "Current number of processed tweets: 500\n",
            "Current number of processed tweets: 600\n"
          ]
        }
      ],
      "source": [
        "result_val = getEmbeddingBERT(bert_classifier,X_val)\n",
        "df_result_val = pd.DataFrame([sent[0].tolist() for sent in result_val]).add_prefix('BERTEmbed_')\n",
        "df_result_val.to_csv('./data/_PHEMEext_Bert_final_brackets_nrmzd.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of processed tweets: 0\n",
            "Current number of processed tweets: 300\n",
            "Current number of processed tweets: 600\n",
            "Current number of processed tweets: 900\n",
            "Current number of processed tweets: 1200\n",
            "Current number of processed tweets: 1500\n",
            "Current number of processed tweets: 1800\n",
            "Current number of processed tweets: 2100\n",
            "Current number of processed tweets: 2400\n",
            "Current number of processed tweets: 2700\n",
            "Current number of processed tweets: 3000\n",
            "Current number of processed tweets: 3300\n",
            "Current number of processed tweets: 3600\n",
            "Current number of processed tweets: 3900\n",
            "Current number of processed tweets: 4200\n",
            "Current number of processed tweets: 4500\n",
            "Current number of processed tweets: 4800\n",
            "Current number of processed tweets: 5100\n"
          ]
        }
      ],
      "source": [
        "result_rhi = getEmbeddingBERT(bert_classifier,rhi_data)\n",
        "df_result_rhi = pd.DataFrame([sent[0].tolist() for sent in result_rhi]).add_prefix('BERTEmbed_')\n",
        "df_result_rhi.to_csv('./data/_RHI_Bert_final_brackets_nrmzd.csv', index = False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "lMPu2j75f82A"
      },
      "outputs": [],
      "source": [
        "df_result_pheme.to_csv('./data/_PHEME_Bert_final.csv', index = False)\n",
        "df_result_val.to_csv('./data/_PHEMEext_Bert_final.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "id": "fEsaoa9A-IJK"
      },
      "outputs": [],
      "source": [
        "df_result_rhi.to_csv('./data/_RHI_Bert_final.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 471,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "KVH1rKuAD5i3",
        "outputId": "54f2466c-60e8-433b-a1ca-b1b93e935413"
      },
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BERTEmbed_0</th>\n      <th>BERTEmbed_1</th>\n      <th>BERTEmbed_2</th>\n      <th>BERTEmbed_3</th>\n      <th>BERTEmbed_4</th>\n      <th>BERTEmbed_5</th>\n      <th>BERTEmbed_6</th>\n      <th>BERTEmbed_7</th>\n      <th>BERTEmbed_8</th>\n      <th>BERTEmbed_9</th>\n      <th>BERTEmbed_10</th>\n      <th>BERTEmbed_11</th>\n      <th>BERTEmbed_12</th>\n      <th>BERTEmbed_13</th>\n      <th>BERTEmbed_14</th>\n      <th>BERTEmbed_15</th>\n      <th>BERTEmbed_16</th>\n      <th>BERTEmbed_17</th>\n      <th>BERTEmbed_18</th>\n      <th>BERTEmbed_19</th>\n      <th>BERTEmbed_20</th>\n      <th>BERTEmbed_21</th>\n      <th>BERTEmbed_22</th>\n      <th>BERTEmbed_23</th>\n      <th>BERTEmbed_24</th>\n      <th>BERTEmbed_25</th>\n      <th>BERTEmbed_26</th>\n      <th>BERTEmbed_27</th>\n      <th>BERTEmbed_28</th>\n      <th>BERTEmbed_29</th>\n      <th>BERTEmbed_30</th>\n      <th>BERTEmbed_31</th>\n      <th>BERTEmbed_32</th>\n      <th>BERTEmbed_33</th>\n      <th>BERTEmbed_34</th>\n      <th>BERTEmbed_35</th>\n      <th>BERTEmbed_36</th>\n      <th>BERTEmbed_37</th>\n      <th>BERTEmbed_38</th>\n      <th>BERTEmbed_39</th>\n      <th>BERTEmbed_40</th>\n      <th>BERTEmbed_41</th>\n      <th>BERTEmbed_42</th>\n      <th>BERTEmbed_43</th>\n      <th>BERTEmbed_44</th>\n      <th>BERTEmbed_45</th>\n      <th>BERTEmbed_46</th>\n      <th>BERTEmbed_47</th>\n      <th>BERTEmbed_48</th>\n      <th>BERTEmbed_49</th>\n      <th>BERTEmbed_50</th>\n      <th>BERTEmbed_51</th>\n      <th>BERTEmbed_52</th>\n      <th>BERTEmbed_53</th>\n      <th>BERTEmbed_54</th>\n      <th>BERTEmbed_55</th>\n      <th>BERTEmbed_56</th>\n      <th>BERTEmbed_57</th>\n      <th>BERTEmbed_58</th>\n      <th>BERTEmbed_59</th>\n      <th>BERTEmbed_60</th>\n      <th>BERTEmbed_61</th>\n      <th>BERTEmbed_62</th>\n      <th>BERTEmbed_63</th>\n      <th>BERTEmbed_64</th>\n      <th>BERTEmbed_65</th>\n      <th>BERTEmbed_66</th>\n      <th>BERTEmbed_67</th>\n      <th>BERTEmbed_68</th>\n      <th>BERTEmbed_69</th>\n      <th>BERTEmbed_70</th>\n      <th>BERTEmbed_71</th>\n      <th>BERTEmbed_72</th>\n      <th>BERTEmbed_73</th>\n      <th>BERTEmbed_74</th>\n      <th>BERTEmbed_75</th>\n      <th>BERTEmbed_76</th>\n      <th>BERTEmbed_77</th>\n      <th>BERTEmbed_78</th>\n      <th>BERTEmbed_79</th>\n      <th>BERTEmbed_80</th>\n      <th>BERTEmbed_81</th>\n      <th>BERTEmbed_82</th>\n      <th>BERTEmbed_83</th>\n      <th>BERTEmbed_84</th>\n      <th>BERTEmbed_85</th>\n      <th>BERTEmbed_86</th>\n      <th>BERTEmbed_87</th>\n      <th>BERTEmbed_88</th>\n      <th>BERTEmbed_89</th>\n      <th>BERTEmbed_90</th>\n      <th>BERTEmbed_91</th>\n      <th>BERTEmbed_92</th>\n      <th>BERTEmbed_93</th>\n      <th>BERTEmbed_94</th>\n      <th>BERTEmbed_95</th>\n      <th>BERTEmbed_96</th>\n      <th>BERTEmbed_97</th>\n      <th>BERTEmbed_98</th>\n      <th>BERTEmbed_99</th>\n      <th>BERTEmbed_100</th>\n      <th>BERTEmbed_101</th>\n      <th>BERTEmbed_102</th>\n      <th>BERTEmbed_103</th>\n      <th>BERTEmbed_104</th>\n      <th>BERTEmbed_105</th>\n      <th>BERTEmbed_106</th>\n      <th>BERTEmbed_107</th>\n      <th>BERTEmbed_108</th>\n      <th>BERTEmbed_109</th>\n      <th>BERTEmbed_110</th>\n      <th>BERTEmbed_111</th>\n      <th>BERTEmbed_112</th>\n      <th>BERTEmbed_113</th>\n      <th>BERTEmbed_114</th>\n      <th>BERTEmbed_115</th>\n      <th>BERTEmbed_116</th>\n      <th>BERTEmbed_117</th>\n      <th>BERTEmbed_118</th>\n      <th>BERTEmbed_119</th>\n      <th>BERTEmbed_120</th>\n      <th>BERTEmbed_121</th>\n      <th>BERTEmbed_122</th>\n      <th>BERTEmbed_123</th>\n      <th>BERTEmbed_124</th>\n      <th>BERTEmbed_125</th>\n      <th>BERTEmbed_126</th>\n      <th>BERTEmbed_127</th>\n      <th>BERTEmbed_128</th>\n      <th>BERTEmbed_129</th>\n      <th>BERTEmbed_130</th>\n      <th>BERTEmbed_131</th>\n      <th>BERTEmbed_132</th>\n      <th>BERTEmbed_133</th>\n      <th>BERTEmbed_134</th>\n      <th>BERTEmbed_135</th>\n      <th>BERTEmbed_136</th>\n      <th>BERTEmbed_137</th>\n      <th>BERTEmbed_138</th>\n      <th>BERTEmbed_139</th>\n      <th>BERTEmbed_140</th>\n      <th>BERTEmbed_141</th>\n      <th>BERTEmbed_142</th>\n      <th>BERTEmbed_143</th>\n      <th>BERTEmbed_144</th>\n      <th>BERTEmbed_145</th>\n      <th>BERTEmbed_146</th>\n      <th>BERTEmbed_147</th>\n      <th>BERTEmbed_148</th>\n      <th>BERTEmbed_149</th>\n      <th>BERTEmbed_150</th>\n      <th>BERTEmbed_151</th>\n      <th>BERTEmbed_152</th>\n      <th>BERTEmbed_153</th>\n      <th>BERTEmbed_154</th>\n      <th>BERTEmbed_155</th>\n      <th>BERTEmbed_156</th>\n      <th>BERTEmbed_157</th>\n      <th>BERTEmbed_158</th>\n      <th>BERTEmbed_159</th>\n      <th>BERTEmbed_160</th>\n      <th>BERTEmbed_161</th>\n      <th>BERTEmbed_162</th>\n      <th>BERTEmbed_163</th>\n      <th>BERTEmbed_164</th>\n      <th>BERTEmbed_165</th>\n      <th>BERTEmbed_166</th>\n      <th>BERTEmbed_167</th>\n      <th>BERTEmbed_168</th>\n      <th>BERTEmbed_169</th>\n      <th>BERTEmbed_170</th>\n      <th>BERTEmbed_171</th>\n      <th>BERTEmbed_172</th>\n      <th>BERTEmbed_173</th>\n      <th>BERTEmbed_174</th>\n      <th>BERTEmbed_175</th>\n      <th>BERTEmbed_176</th>\n      <th>BERTEmbed_177</th>\n      <th>BERTEmbed_178</th>\n      <th>BERTEmbed_179</th>\n      <th>BERTEmbed_180</th>\n      <th>BERTEmbed_181</th>\n      <th>BERTEmbed_182</th>\n      <th>BERTEmbed_183</th>\n      <th>BERTEmbed_184</th>\n      <th>BERTEmbed_185</th>\n      <th>BERTEmbed_186</th>\n      <th>BERTEmbed_187</th>\n      <th>BERTEmbed_188</th>\n      <th>BERTEmbed_189</th>\n      <th>BERTEmbed_190</th>\n      <th>BERTEmbed_191</th>\n      <th>BERTEmbed_192</th>\n      <th>BERTEmbed_193</th>\n      <th>BERTEmbed_194</th>\n      <th>BERTEmbed_195</th>\n      <th>BERTEmbed_196</th>\n      <th>BERTEmbed_197</th>\n      <th>BERTEmbed_198</th>\n      <th>BERTEmbed_199</th>\n      <th>BERTEmbed_200</th>\n      <th>BERTEmbed_201</th>\n      <th>BERTEmbed_202</th>\n      <th>BERTEmbed_203</th>\n      <th>BERTEmbed_204</th>\n      <th>BERTEmbed_205</th>\n      <th>BERTEmbed_206</th>\n      <th>BERTEmbed_207</th>\n      <th>BERTEmbed_208</th>\n      <th>BERTEmbed_209</th>\n      <th>BERTEmbed_210</th>\n      <th>BERTEmbed_211</th>\n      <th>BERTEmbed_212</th>\n      <th>BERTEmbed_213</th>\n      <th>BERTEmbed_214</th>\n      <th>BERTEmbed_215</th>\n      <th>BERTEmbed_216</th>\n      <th>BERTEmbed_217</th>\n      <th>BERTEmbed_218</th>\n      <th>BERTEmbed_219</th>\n      <th>BERTEmbed_220</th>\n      <th>BERTEmbed_221</th>\n      <th>BERTEmbed_222</th>\n      <th>BERTEmbed_223</th>\n      <th>BERTEmbed_224</th>\n      <th>BERTEmbed_225</th>\n      <th>BERTEmbed_226</th>\n      <th>BERTEmbed_227</th>\n      <th>BERTEmbed_228</th>\n      <th>BERTEmbed_229</th>\n      <th>BERTEmbed_230</th>\n      <th>BERTEmbed_231</th>\n      <th>BERTEmbed_232</th>\n      <th>BERTEmbed_233</th>\n      <th>BERTEmbed_234</th>\n      <th>BERTEmbed_235</th>\n      <th>BERTEmbed_236</th>\n      <th>BERTEmbed_237</th>\n      <th>BERTEmbed_238</th>\n      <th>BERTEmbed_239</th>\n      <th>BERTEmbed_240</th>\n      <th>BERTEmbed_241</th>\n      <th>BERTEmbed_242</th>\n      <th>BERTEmbed_243</th>\n      <th>BERTEmbed_244</th>\n      <th>BERTEmbed_245</th>\n      <th>BERTEmbed_246</th>\n      <th>BERTEmbed_247</th>\n      <th>BERTEmbed_248</th>\n      <th>BERTEmbed_249</th>\n      <th>BERTEmbed_250</th>\n      <th>BERTEmbed_251</th>\n      <th>BERTEmbed_252</th>\n      <th>BERTEmbed_253</th>\n      <th>BERTEmbed_254</th>\n      <th>BERTEmbed_255</th>\n      <th>BERTEmbed_256</th>\n      <th>BERTEmbed_257</th>\n      <th>BERTEmbed_258</th>\n      <th>BERTEmbed_259</th>\n      <th>BERTEmbed_260</th>\n      <th>BERTEmbed_261</th>\n      <th>BERTEmbed_262</th>\n      <th>BERTEmbed_263</th>\n      <th>BERTEmbed_264</th>\n      <th>BERTEmbed_265</th>\n      <th>BERTEmbed_266</th>\n      <th>BERTEmbed_267</th>\n      <th>BERTEmbed_268</th>\n      <th>BERTEmbed_269</th>\n      <th>BERTEmbed_270</th>\n      <th>BERTEmbed_271</th>\n      <th>BERTEmbed_272</th>\n      <th>BERTEmbed_273</th>\n      <th>BERTEmbed_274</th>\n      <th>BERTEmbed_275</th>\n      <th>BERTEmbed_276</th>\n      <th>BERTEmbed_277</th>\n      <th>BERTEmbed_278</th>\n      <th>BERTEmbed_279</th>\n      <th>BERTEmbed_280</th>\n      <th>BERTEmbed_281</th>\n      <th>BERTEmbed_282</th>\n      <th>BERTEmbed_283</th>\n      <th>BERTEmbed_284</th>\n      <th>BERTEmbed_285</th>\n      <th>BERTEmbed_286</th>\n      <th>BERTEmbed_287</th>\n      <th>BERTEmbed_288</th>\n      <th>BERTEmbed_289</th>\n      <th>BERTEmbed_290</th>\n      <th>BERTEmbed_291</th>\n      <th>BERTEmbed_292</th>\n      <th>BERTEmbed_293</th>\n      <th>BERTEmbed_294</th>\n      <th>BERTEmbed_295</th>\n      <th>BERTEmbed_296</th>\n      <th>BERTEmbed_297</th>\n      <th>BERTEmbed_298</th>\n      <th>BERTEmbed_299</th>\n      <th>BERTEmbed_300</th>\n      <th>BERTEmbed_301</th>\n      <th>BERTEmbed_302</th>\n      <th>BERTEmbed_303</th>\n      <th>BERTEmbed_304</th>\n      <th>BERTEmbed_305</th>\n      <th>BERTEmbed_306</th>\n      <th>BERTEmbed_307</th>\n      <th>BERTEmbed_308</th>\n      <th>BERTEmbed_309</th>\n      <th>BERTEmbed_310</th>\n      <th>BERTEmbed_311</th>\n      <th>BERTEmbed_312</th>\n      <th>BERTEmbed_313</th>\n      <th>BERTEmbed_314</th>\n      <th>BERTEmbed_315</th>\n      <th>BERTEmbed_316</th>\n      <th>BERTEmbed_317</th>\n      <th>BERTEmbed_318</th>\n      <th>BERTEmbed_319</th>\n      <th>BERTEmbed_320</th>\n      <th>BERTEmbed_321</th>\n      <th>BERTEmbed_322</th>\n      <th>BERTEmbed_323</th>\n      <th>BERTEmbed_324</th>\n      <th>BERTEmbed_325</th>\n      <th>BERTEmbed_326</th>\n      <th>BERTEmbed_327</th>\n      <th>BERTEmbed_328</th>\n      <th>BERTEmbed_329</th>\n      <th>BERTEmbed_330</th>\n      <th>BERTEmbed_331</th>\n      <th>BERTEmbed_332</th>\n      <th>BERTEmbed_333</th>\n      <th>BERTEmbed_334</th>\n      <th>BERTEmbed_335</th>\n      <th>BERTEmbed_336</th>\n      <th>BERTEmbed_337</th>\n      <th>BERTEmbed_338</th>\n      <th>BERTEmbed_339</th>\n      <th>BERTEmbed_340</th>\n      <th>BERTEmbed_341</th>\n      <th>BERTEmbed_342</th>\n      <th>BERTEmbed_343</th>\n      <th>BERTEmbed_344</th>\n      <th>BERTEmbed_345</th>\n      <th>BERTEmbed_346</th>\n      <th>BERTEmbed_347</th>\n      <th>BERTEmbed_348</th>\n      <th>BERTEmbed_349</th>\n      <th>BERTEmbed_350</th>\n      <th>BERTEmbed_351</th>\n      <th>BERTEmbed_352</th>\n      <th>BERTEmbed_353</th>\n      <th>BERTEmbed_354</th>\n      <th>BERTEmbed_355</th>\n      <th>BERTEmbed_356</th>\n      <th>BERTEmbed_357</th>\n      <th>BERTEmbed_358</th>\n      <th>BERTEmbed_359</th>\n      <th>BERTEmbed_360</th>\n      <th>BERTEmbed_361</th>\n      <th>BERTEmbed_362</th>\n      <th>BERTEmbed_363</th>\n      <th>BERTEmbed_364</th>\n      <th>BERTEmbed_365</th>\n      <th>BERTEmbed_366</th>\n      <th>BERTEmbed_367</th>\n      <th>BERTEmbed_368</th>\n      <th>BERTEmbed_369</th>\n      <th>BERTEmbed_370</th>\n      <th>BERTEmbed_371</th>\n      <th>BERTEmbed_372</th>\n      <th>BERTEmbed_373</th>\n      <th>BERTEmbed_374</th>\n      <th>BERTEmbed_375</th>\n      <th>BERTEmbed_376</th>\n      <th>BERTEmbed_377</th>\n      <th>BERTEmbed_378</th>\n      <th>BERTEmbed_379</th>\n      <th>BERTEmbed_380</th>\n      <th>BERTEmbed_381</th>\n      <th>BERTEmbed_382</th>\n      <th>BERTEmbed_383</th>\n      <th>BERTEmbed_384</th>\n      <th>BERTEmbed_385</th>\n      <th>BERTEmbed_386</th>\n      <th>BERTEmbed_387</th>\n      <th>BERTEmbed_388</th>\n      <th>BERTEmbed_389</th>\n      <th>BERTEmbed_390</th>\n      <th>BERTEmbed_391</th>\n      <th>BERTEmbed_392</th>\n      <th>BERTEmbed_393</th>\n      <th>BERTEmbed_394</th>\n      <th>BERTEmbed_395</th>\n      <th>BERTEmbed_396</th>\n      <th>BERTEmbed_397</th>\n      <th>BERTEmbed_398</th>\n      <th>BERTEmbed_399</th>\n      <th>BERTEmbed_400</th>\n      <th>BERTEmbed_401</th>\n      <th>BERTEmbed_402</th>\n      <th>BERTEmbed_403</th>\n      <th>BERTEmbed_404</th>\n      <th>BERTEmbed_405</th>\n      <th>BERTEmbed_406</th>\n      <th>BERTEmbed_407</th>\n      <th>BERTEmbed_408</th>\n      <th>BERTEmbed_409</th>\n      <th>BERTEmbed_410</th>\n      <th>BERTEmbed_411</th>\n      <th>BERTEmbed_412</th>\n      <th>BERTEmbed_413</th>\n      <th>BERTEmbed_414</th>\n      <th>BERTEmbed_415</th>\n      <th>BERTEmbed_416</th>\n      <th>BERTEmbed_417</th>\n      <th>BERTEmbed_418</th>\n      <th>BERTEmbed_419</th>\n      <th>BERTEmbed_420</th>\n      <th>BERTEmbed_421</th>\n      <th>BERTEmbed_422</th>\n      <th>BERTEmbed_423</th>\n      <th>BERTEmbed_424</th>\n      <th>BERTEmbed_425</th>\n      <th>BERTEmbed_426</th>\n      <th>BERTEmbed_427</th>\n      <th>BERTEmbed_428</th>\n      <th>BERTEmbed_429</th>\n      <th>BERTEmbed_430</th>\n      <th>BERTEmbed_431</th>\n      <th>BERTEmbed_432</th>\n      <th>BERTEmbed_433</th>\n      <th>BERTEmbed_434</th>\n      <th>BERTEmbed_435</th>\n      <th>BERTEmbed_436</th>\n      <th>BERTEmbed_437</th>\n      <th>BERTEmbed_438</th>\n      <th>BERTEmbed_439</th>\n      <th>BERTEmbed_440</th>\n      <th>BERTEmbed_441</th>\n      <th>BERTEmbed_442</th>\n      <th>BERTEmbed_443</th>\n      <th>BERTEmbed_444</th>\n      <th>BERTEmbed_445</th>\n      <th>BERTEmbed_446</th>\n      <th>BERTEmbed_447</th>\n      <th>BERTEmbed_448</th>\n      <th>BERTEmbed_449</th>\n      <th>BERTEmbed_450</th>\n      <th>BERTEmbed_451</th>\n      <th>BERTEmbed_452</th>\n      <th>BERTEmbed_453</th>\n      <th>BERTEmbed_454</th>\n      <th>BERTEmbed_455</th>\n      <th>BERTEmbed_456</th>\n      <th>BERTEmbed_457</th>\n      <th>BERTEmbed_458</th>\n      <th>BERTEmbed_459</th>\n      <th>BERTEmbed_460</th>\n      <th>BERTEmbed_461</th>\n      <th>BERTEmbed_462</th>\n      <th>BERTEmbed_463</th>\n      <th>BERTEmbed_464</th>\n      <th>BERTEmbed_465</th>\n      <th>BERTEmbed_466</th>\n      <th>BERTEmbed_467</th>\n      <th>BERTEmbed_468</th>\n      <th>BERTEmbed_469</th>\n      <th>BERTEmbed_470</th>\n      <th>BERTEmbed_471</th>\n      <th>BERTEmbed_472</th>\n      <th>BERTEmbed_473</th>\n      <th>BERTEmbed_474</th>\n      <th>BERTEmbed_475</th>\n      <th>BERTEmbed_476</th>\n      <th>BERTEmbed_477</th>\n      <th>BERTEmbed_478</th>\n      <th>BERTEmbed_479</th>\n      <th>BERTEmbed_480</th>\n      <th>BERTEmbed_481</th>\n      <th>BERTEmbed_482</th>\n      <th>BERTEmbed_483</th>\n      <th>BERTEmbed_484</th>\n      <th>BERTEmbed_485</th>\n      <th>BERTEmbed_486</th>\n      <th>BERTEmbed_487</th>\n      <th>BERTEmbed_488</th>\n      <th>BERTEmbed_489</th>\n      <th>BERTEmbed_490</th>\n      <th>BERTEmbed_491</th>\n      <th>BERTEmbed_492</th>\n      <th>BERTEmbed_493</th>\n      <th>BERTEmbed_494</th>\n      <th>BERTEmbed_495</th>\n      <th>BERTEmbed_496</th>\n      <th>BERTEmbed_497</th>\n      <th>BERTEmbed_498</th>\n      <th>BERTEmbed_499</th>\n      <th>BERTEmbed_500</th>\n      <th>BERTEmbed_501</th>\n      <th>BERTEmbed_502</th>\n      <th>BERTEmbed_503</th>\n      <th>BERTEmbed_504</th>\n      <th>BERTEmbed_505</th>\n      <th>BERTEmbed_506</th>\n      <th>BERTEmbed_507</th>\n      <th>BERTEmbed_508</th>\n      <th>BERTEmbed_509</th>\n      <th>BERTEmbed_510</th>\n      <th>BERTEmbed_511</th>\n      <th>BERTEmbed_512</th>\n      <th>BERTEmbed_513</th>\n      <th>BERTEmbed_514</th>\n      <th>BERTEmbed_515</th>\n      <th>BERTEmbed_516</th>\n      <th>BERTEmbed_517</th>\n      <th>BERTEmbed_518</th>\n      <th>BERTEmbed_519</th>\n      <th>BERTEmbed_520</th>\n      <th>BERTEmbed_521</th>\n      <th>BERTEmbed_522</th>\n      <th>BERTEmbed_523</th>\n      <th>BERTEmbed_524</th>\n      <th>BERTEmbed_525</th>\n      <th>BERTEmbed_526</th>\n      <th>BERTEmbed_527</th>\n      <th>BERTEmbed_528</th>\n      <th>BERTEmbed_529</th>\n      <th>BERTEmbed_530</th>\n      <th>BERTEmbed_531</th>\n      <th>BERTEmbed_532</th>\n      <th>BERTEmbed_533</th>\n      <th>BERTEmbed_534</th>\n      <th>BERTEmbed_535</th>\n      <th>BERTEmbed_536</th>\n      <th>BERTEmbed_537</th>\n      <th>BERTEmbed_538</th>\n      <th>BERTEmbed_539</th>\n      <th>BERTEmbed_540</th>\n      <th>BERTEmbed_541</th>\n      <th>BERTEmbed_542</th>\n      <th>BERTEmbed_543</th>\n      <th>BERTEmbed_544</th>\n      <th>BERTEmbed_545</th>\n      <th>BERTEmbed_546</th>\n      <th>BERTEmbed_547</th>\n      <th>BERTEmbed_548</th>\n      <th>BERTEmbed_549</th>\n      <th>BERTEmbed_550</th>\n      <th>BERTEmbed_551</th>\n      <th>BERTEmbed_552</th>\n      <th>BERTEmbed_553</th>\n      <th>BERTEmbed_554</th>\n      <th>BERTEmbed_555</th>\n      <th>BERTEmbed_556</th>\n      <th>BERTEmbed_557</th>\n      <th>BERTEmbed_558</th>\n      <th>BERTEmbed_559</th>\n      <th>BERTEmbed_560</th>\n      <th>BERTEmbed_561</th>\n      <th>BERTEmbed_562</th>\n      <th>BERTEmbed_563</th>\n      <th>BERTEmbed_564</th>\n      <th>BERTEmbed_565</th>\n      <th>BERTEmbed_566</th>\n      <th>BERTEmbed_567</th>\n      <th>BERTEmbed_568</th>\n      <th>BERTEmbed_569</th>\n      <th>BERTEmbed_570</th>\n      <th>BERTEmbed_571</th>\n      <th>BERTEmbed_572</th>\n      <th>BERTEmbed_573</th>\n      <th>BERTEmbed_574</th>\n      <th>BERTEmbed_575</th>\n      <th>BERTEmbed_576</th>\n      <th>BERTEmbed_577</th>\n      <th>BERTEmbed_578</th>\n      <th>BERTEmbed_579</th>\n      <th>BERTEmbed_580</th>\n      <th>BERTEmbed_581</th>\n      <th>BERTEmbed_582</th>\n      <th>BERTEmbed_583</th>\n      <th>BERTEmbed_584</th>\n      <th>BERTEmbed_585</th>\n      <th>BERTEmbed_586</th>\n      <th>BERTEmbed_587</th>\n      <th>BERTEmbed_588</th>\n      <th>BERTEmbed_589</th>\n      <th>BERTEmbed_590</th>\n      <th>BERTEmbed_591</th>\n      <th>BERTEmbed_592</th>\n      <th>BERTEmbed_593</th>\n      <th>BERTEmbed_594</th>\n      <th>BERTEmbed_595</th>\n      <th>BERTEmbed_596</th>\n      <th>BERTEmbed_597</th>\n      <th>BERTEmbed_598</th>\n      <th>BERTEmbed_599</th>\n      <th>BERTEmbed_600</th>\n      <th>BERTEmbed_601</th>\n      <th>BERTEmbed_602</th>\n      <th>BERTEmbed_603</th>\n      <th>BERTEmbed_604</th>\n      <th>BERTEmbed_605</th>\n      <th>BERTEmbed_606</th>\n      <th>BERTEmbed_607</th>\n      <th>BERTEmbed_608</th>\n      <th>BERTEmbed_609</th>\n      <th>BERTEmbed_610</th>\n      <th>BERTEmbed_611</th>\n      <th>BERTEmbed_612</th>\n      <th>BERTEmbed_613</th>\n      <th>BERTEmbed_614</th>\n      <th>BERTEmbed_615</th>\n      <th>BERTEmbed_616</th>\n      <th>BERTEmbed_617</th>\n      <th>BERTEmbed_618</th>\n      <th>BERTEmbed_619</th>\n      <th>BERTEmbed_620</th>\n      <th>BERTEmbed_621</th>\n      <th>BERTEmbed_622</th>\n      <th>BERTEmbed_623</th>\n      <th>BERTEmbed_624</th>\n      <th>BERTEmbed_625</th>\n      <th>BERTEmbed_626</th>\n      <th>BERTEmbed_627</th>\n      <th>BERTEmbed_628</th>\n      <th>BERTEmbed_629</th>\n      <th>BERTEmbed_630</th>\n      <th>BERTEmbed_631</th>\n      <th>BERTEmbed_632</th>\n      <th>BERTEmbed_633</th>\n      <th>BERTEmbed_634</th>\n      <th>BERTEmbed_635</th>\n      <th>BERTEmbed_636</th>\n      <th>BERTEmbed_637</th>\n      <th>BERTEmbed_638</th>\n      <th>BERTEmbed_639</th>\n      <th>BERTEmbed_640</th>\n      <th>BERTEmbed_641</th>\n      <th>BERTEmbed_642</th>\n      <th>BERTEmbed_643</th>\n      <th>BERTEmbed_644</th>\n      <th>BERTEmbed_645</th>\n      <th>BERTEmbed_646</th>\n      <th>BERTEmbed_647</th>\n      <th>BERTEmbed_648</th>\n      <th>BERTEmbed_649</th>\n      <th>BERTEmbed_650</th>\n      <th>BERTEmbed_651</th>\n      <th>BERTEmbed_652</th>\n      <th>BERTEmbed_653</th>\n      <th>BERTEmbed_654</th>\n      <th>BERTEmbed_655</th>\n      <th>BERTEmbed_656</th>\n      <th>BERTEmbed_657</th>\n      <th>BERTEmbed_658</th>\n      <th>BERTEmbed_659</th>\n      <th>BERTEmbed_660</th>\n      <th>BERTEmbed_661</th>\n      <th>BERTEmbed_662</th>\n      <th>BERTEmbed_663</th>\n      <th>BERTEmbed_664</th>\n      <th>BERTEmbed_665</th>\n      <th>BERTEmbed_666</th>\n      <th>BERTEmbed_667</th>\n      <th>BERTEmbed_668</th>\n      <th>BERTEmbed_669</th>\n      <th>BERTEmbed_670</th>\n      <th>BERTEmbed_671</th>\n      <th>BERTEmbed_672</th>\n      <th>BERTEmbed_673</th>\n      <th>BERTEmbed_674</th>\n      <th>BERTEmbed_675</th>\n      <th>BERTEmbed_676</th>\n      <th>BERTEmbed_677</th>\n      <th>BERTEmbed_678</th>\n      <th>BERTEmbed_679</th>\n      <th>BERTEmbed_680</th>\n      <th>BERTEmbed_681</th>\n      <th>BERTEmbed_682</th>\n      <th>BERTEmbed_683</th>\n      <th>BERTEmbed_684</th>\n      <th>BERTEmbed_685</th>\n      <th>BERTEmbed_686</th>\n      <th>BERTEmbed_687</th>\n      <th>BERTEmbed_688</th>\n      <th>BERTEmbed_689</th>\n      <th>BERTEmbed_690</th>\n      <th>BERTEmbed_691</th>\n      <th>BERTEmbed_692</th>\n      <th>BERTEmbed_693</th>\n      <th>BERTEmbed_694</th>\n      <th>BERTEmbed_695</th>\n      <th>BERTEmbed_696</th>\n      <th>BERTEmbed_697</th>\n      <th>BERTEmbed_698</th>\n      <th>BERTEmbed_699</th>\n      <th>BERTEmbed_700</th>\n      <th>BERTEmbed_701</th>\n      <th>BERTEmbed_702</th>\n      <th>BERTEmbed_703</th>\n      <th>BERTEmbed_704</th>\n      <th>BERTEmbed_705</th>\n      <th>BERTEmbed_706</th>\n      <th>BERTEmbed_707</th>\n      <th>BERTEmbed_708</th>\n      <th>BERTEmbed_709</th>\n      <th>BERTEmbed_710</th>\n      <th>BERTEmbed_711</th>\n      <th>BERTEmbed_712</th>\n      <th>BERTEmbed_713</th>\n      <th>BERTEmbed_714</th>\n      <th>BERTEmbed_715</th>\n      <th>BERTEmbed_716</th>\n      <th>BERTEmbed_717</th>\n      <th>BERTEmbed_718</th>\n      <th>BERTEmbed_719</th>\n      <th>BERTEmbed_720</th>\n      <th>BERTEmbed_721</th>\n      <th>BERTEmbed_722</th>\n      <th>BERTEmbed_723</th>\n      <th>BERTEmbed_724</th>\n      <th>BERTEmbed_725</th>\n      <th>BERTEmbed_726</th>\n      <th>BERTEmbed_727</th>\n      <th>BERTEmbed_728</th>\n      <th>BERTEmbed_729</th>\n      <th>BERTEmbed_730</th>\n      <th>BERTEmbed_731</th>\n      <th>BERTEmbed_732</th>\n      <th>BERTEmbed_733</th>\n      <th>BERTEmbed_734</th>\n      <th>BERTEmbed_735</th>\n      <th>BERTEmbed_736</th>\n      <th>BERTEmbed_737</th>\n      <th>BERTEmbed_738</th>\n      <th>BERTEmbed_739</th>\n      <th>BERTEmbed_740</th>\n      <th>BERTEmbed_741</th>\n      <th>BERTEmbed_742</th>\n      <th>BERTEmbed_743</th>\n      <th>BERTEmbed_744</th>\n      <th>BERTEmbed_745</th>\n      <th>BERTEmbed_746</th>\n      <th>BERTEmbed_747</th>\n      <th>BERTEmbed_748</th>\n      <th>BERTEmbed_749</th>\n      <th>BERTEmbed_750</th>\n      <th>BERTEmbed_751</th>\n      <th>BERTEmbed_752</th>\n      <th>BERTEmbed_753</th>\n      <th>BERTEmbed_754</th>\n      <th>BERTEmbed_755</th>\n      <th>BERTEmbed_756</th>\n      <th>BERTEmbed_757</th>\n      <th>BERTEmbed_758</th>\n      <th>BERTEmbed_759</th>\n      <th>BERTEmbed_760</th>\n      <th>BERTEmbed_761</th>\n      <th>BERTEmbed_762</th>\n      <th>BERTEmbed_763</th>\n      <th>BERTEmbed_764</th>\n      <th>BERTEmbed_765</th>\n      <th>BERTEmbed_766</th>\n      <th>BERTEmbed_767</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.112559</td>\n      <td>0.256139</td>\n      <td>0.171227</td>\n      <td>-0.076553</td>\n      <td>0.143061</td>\n      <td>-0.062029</td>\n      <td>-0.072677</td>\n      <td>-0.112627</td>\n      <td>0.171462</td>\n      <td>-0.207341</td>\n      <td>-0.046663</td>\n      <td>0.078731</td>\n      <td>-0.039187</td>\n      <td>-0.128192</td>\n      <td>-0.095857</td>\n      <td>-0.287233</td>\n      <td>0.018149</td>\n      <td>0.122441</td>\n      <td>0.475947</td>\n      <td>0.299201</td>\n      <td>0.067449</td>\n      <td>-0.953918</td>\n      <td>-0.133265</td>\n      <td>0.001833</td>\n      <td>0.146900</td>\n      <td>-0.076164</td>\n      <td>-0.179670</td>\n      <td>-0.067646</td>\n      <td>-0.104437</td>\n      <td>0.086202</td>\n      <td>0.046614</td>\n      <td>0.256747</td>\n      <td>-0.470094</td>\n      <td>-0.067134</td>\n      <td>0.270478</td>\n      <td>-0.049564</td>\n      <td>-0.049378</td>\n      <td>0.247585</td>\n      <td>0.324011</td>\n      <td>0.035134</td>\n      <td>0.143400</td>\n      <td>-0.311486</td>\n      <td>0.167787</td>\n      <td>-0.224317</td>\n      <td>0.246488</td>\n      <td>-0.031889</td>\n      <td>0.338930</td>\n      <td>-0.076163</td>\n      <td>0.295636</td>\n      <td>0.067720</td>\n      <td>0.084182</td>\n      <td>0.333475</td>\n      <td>-0.072839</td>\n      <td>0.119350</td>\n      <td>-0.139098</td>\n      <td>-0.602282</td>\n      <td>-0.142682</td>\n      <td>0.468993</td>\n      <td>0.478908</td>\n      <td>0.356846</td>\n      <td>-0.125217</td>\n      <td>-0.036384</td>\n      <td>0.266413</td>\n      <td>0.157529</td>\n      <td>0.592066</td>\n      <td>0.249830</td>\n      <td>0.041442</td>\n      <td>-0.208175</td>\n      <td>0.113400</td>\n      <td>0.235857</td>\n      <td>-0.432324</td>\n      <td>-0.441868</td>\n      <td>0.027432</td>\n      <td>0.165315</td>\n      <td>-0.404585</td>\n      <td>-0.080296</td>\n      <td>0.083703</td>\n      <td>0.230129</td>\n      <td>-0.349844</td>\n      <td>-0.016986</td>\n      <td>-0.008322</td>\n      <td>-0.123397</td>\n      <td>0.015158</td>\n      <td>0.079697</td>\n      <td>0.058765</td>\n      <td>-0.072907</td>\n      <td>0.109945</td>\n      <td>0.173136</td>\n      <td>0.130883</td>\n      <td>-0.015202</td>\n      <td>0.271100</td>\n      <td>0.027043</td>\n      <td>0.000853</td>\n      <td>-0.153687</td>\n      <td>-0.041729</td>\n      <td>0.067801</td>\n      <td>-0.140518</td>\n      <td>-0.080658</td>\n      <td>-0.368284</td>\n      <td>0.411745</td>\n      <td>-0.362667</td>\n      <td>-0.006279</td>\n      <td>0.119001</td>\n      <td>-0.051532</td>\n      <td>0.359874</td>\n      <td>0.312525</td>\n      <td>0.052078</td>\n      <td>-0.395427</td>\n      <td>-0.029553</td>\n      <td>0.250159</td>\n      <td>0.161739</td>\n      <td>-0.116295</td>\n      <td>-0.077457</td>\n      <td>-0.084327</td>\n      <td>-0.371191</td>\n      <td>-0.204650</td>\n      <td>0.112661</td>\n      <td>-0.032450</td>\n      <td>-0.145746</td>\n      <td>-0.211910</td>\n      <td>0.266981</td>\n      <td>0.385478</td>\n      <td>-0.635658</td>\n      <td>-0.138266</td>\n      <td>0.017824</td>\n      <td>0.350206</td>\n      <td>0.125419</td>\n      <td>-0.173613</td>\n      <td>0.482098</td>\n      <td>0.189541</td>\n      <td>0.077151</td>\n      <td>-0.044387</td>\n      <td>-0.052943</td>\n      <td>0.079629</td>\n      <td>0.282330</td>\n      <td>0.310168</td>\n      <td>0.130220</td>\n      <td>0.012737</td>\n      <td>0.356406</td>\n      <td>0.149958</td>\n      <td>0.050123</td>\n      <td>0.237315</td>\n      <td>0.143462</td>\n      <td>0.065749</td>\n      <td>0.179177</td>\n      <td>0.086108</td>\n      <td>-0.263236</td>\n      <td>0.208715</td>\n      <td>0.119488</td>\n      <td>0.227063</td>\n      <td>-0.044106</td>\n      <td>0.342906</td>\n      <td>-0.047339</td>\n      <td>-0.154376</td>\n      <td>0.074996</td>\n      <td>0.053576</td>\n      <td>0.007444</td>\n      <td>0.149203</td>\n      <td>0.099868</td>\n      <td>-0.453321</td>\n      <td>-0.324889</td>\n      <td>0.010741</td>\n      <td>0.231252</td>\n      <td>0.304312</td>\n      <td>-0.060922</td>\n      <td>-0.211887</td>\n      <td>0.165883</td>\n      <td>0.083221</td>\n      <td>0.243373</td>\n      <td>-0.534628</td>\n      <td>0.218726</td>\n      <td>0.307381</td>\n      <td>-0.167603</td>\n      <td>-0.243763</td>\n      <td>-0.379733</td>\n      <td>0.369471</td>\n      <td>0.016668</td>\n      <td>0.011161</td>\n      <td>0.330671</td>\n      <td>0.328514</td>\n      <td>0.251347</td>\n      <td>-0.139805</td>\n      <td>0.192017</td>\n      <td>-0.420310</td>\n      <td>0.164875</td>\n      <td>0.340967</td>\n      <td>-0.726799</td>\n      <td>0.125515</td>\n      <td>0.131599</td>\n      <td>-0.399806</td>\n      <td>-0.489963</td>\n      <td>-0.196015</td>\n      <td>0.326039</td>\n      <td>0.152051</td>\n      <td>0.537931</td>\n      <td>0.053144</td>\n      <td>-0.263809</td>\n      <td>-0.035566</td>\n      <td>-0.064936</td>\n      <td>-0.107309</td>\n      <td>0.074838</td>\n      <td>0.072762</td>\n      <td>-0.117619</td>\n      <td>0.090758</td>\n      <td>-0.100880</td>\n      <td>0.191830</td>\n      <td>0.187349</td>\n      <td>0.127041</td>\n      <td>-0.025108</td>\n      <td>-0.165990</td>\n      <td>0.223326</td>\n      <td>-0.130356</td>\n      <td>0.075869</td>\n      <td>-0.054922</td>\n      <td>-0.162718</td>\n      <td>-3.498117</td>\n      <td>-0.236880</td>\n      <td>0.016385</td>\n      <td>0.025485</td>\n      <td>0.077016</td>\n      <td>-0.214546</td>\n      <td>0.164464</td>\n      <td>0.182428</td>\n      <td>0.002008</td>\n      <td>0.169447</td>\n      <td>-0.125676</td>\n      <td>0.184625</td>\n      <td>0.154596</td>\n      <td>-0.319359</td>\n      <td>0.153597</td>\n      <td>0.396978</td>\n      <td>0.454981</td>\n      <td>-0.156553</td>\n      <td>-0.268734</td>\n      <td>0.202336</td>\n      <td>0.122704</td>\n      <td>-0.194059</td>\n      <td>0.254637</td>\n      <td>-0.227367</td>\n      <td>-0.311855</td>\n      <td>0.225219</td>\n      <td>0.087716</td>\n      <td>0.408750</td>\n      <td>0.078682</td>\n      <td>0.047307</td>\n      <td>0.024218</td>\n      <td>0.320653</td>\n      <td>0.557417</td>\n      <td>0.158551</td>\n      <td>0.123917</td>\n      <td>0.298512</td>\n      <td>-0.149285</td>\n      <td>0.058466</td>\n      <td>0.249471</td>\n      <td>0.407494</td>\n      <td>0.263274</td>\n      <td>0.071685</td>\n      <td>0.093606</td>\n      <td>0.303074</td>\n      <td>0.052460</td>\n      <td>0.111555</td>\n      <td>0.245380</td>\n      <td>-0.197684</td>\n      <td>0.150766</td>\n      <td>0.052638</td>\n      <td>-0.015154</td>\n      <td>0.287149</td>\n      <td>-0.014091</td>\n      <td>0.009641</td>\n      <td>-0.053864</td>\n      <td>0.274082</td>\n      <td>-0.217135</td>\n      <td>0.038326</td>\n      <td>-0.187159</td>\n      <td>0.279335</td>\n      <td>0.247314</td>\n      <td>-0.180083</td>\n      <td>0.350674</td>\n      <td>0.034805</td>\n      <td>-0.170363</td>\n      <td>0.663317</td>\n      <td>-0.411698</td>\n      <td>0.074372</td>\n      <td>-0.062803</td>\n      <td>0.274713</td>\n      <td>-0.132838</td>\n      <td>-0.179576</td>\n      <td>0.369982</td>\n      <td>-0.222017</td>\n      <td>0.629291</td>\n      <td>0.372114</td>\n      <td>-0.166405</td>\n      <td>-0.408148</td>\n      <td>0.276812</td>\n      <td>-0.208603</td>\n      <td>0.209347</td>\n      <td>-0.003584</td>\n      <td>0.006407</td>\n      <td>0.016974</td>\n      <td>-0.254865</td>\n      <td>0.420173</td>\n      <td>0.110496</td>\n      <td>-0.285859</td>\n      <td>0.049026</td>\n      <td>-0.090400</td>\n      <td>0.143198</td>\n      <td>0.065386</td>\n      <td>0.201401</td>\n      <td>-0.053070</td>\n      <td>-0.328640</td>\n      <td>-0.187568</td>\n      <td>-0.095185</td>\n      <td>0.801337</td>\n      <td>-0.006089</td>\n      <td>-0.006455</td>\n      <td>0.335097</td>\n      <td>0.019561</td>\n      <td>0.125312</td>\n      <td>0.014889</td>\n      <td>-0.014618</td>\n      <td>0.266433</td>\n      <td>0.083708</td>\n      <td>0.067822</td>\n      <td>-0.258764</td>\n      <td>-0.036717</td>\n      <td>-0.049954</td>\n      <td>0.134303</td>\n      <td>0.594728</td>\n      <td>-0.038062</td>\n      <td>0.054395</td>\n      <td>0.002550</td>\n      <td>-0.049797</td>\n      <td>0.119587</td>\n      <td>-0.018863</td>\n      <td>-0.317392</td>\n      <td>0.237164</td>\n      <td>0.136618</td>\n      <td>0.213643</td>\n      <td>-0.247811</td>\n      <td>-0.104963</td>\n      <td>0.063190</td>\n      <td>0.234255</td>\n      <td>0.232615</td>\n      <td>-0.984170</td>\n      <td>0.438266</td>\n      <td>0.066568</td>\n      <td>-0.334039</td>\n      <td>0.634895</td>\n      <td>0.169662</td>\n      <td>-0.223718</td>\n      <td>0.348286</td>\n      <td>-0.298877</td>\n      <td>0.417768</td>\n      <td>-0.019382</td>\n      <td>0.141550</td>\n      <td>0.456167</td>\n      <td>-0.005118</td>\n      <td>0.146783</td>\n      <td>-0.174523</td>\n      <td>0.248328</td>\n      <td>-0.864120</td>\n      <td>0.078204</td>\n      <td>0.194633</td>\n      <td>0.350986</td>\n      <td>-0.079671</td>\n      <td>-0.049141</td>\n      <td>0.081007</td>\n      <td>-0.053641</td>\n      <td>-0.010466</td>\n      <td>-0.311530</td>\n      <td>-0.232820</td>\n      <td>0.022677</td>\n      <td>-0.070380</td>\n      <td>0.228312</td>\n      <td>0.247126</td>\n      <td>-0.047978</td>\n      <td>-0.732083</td>\n      <td>-1.484927</td>\n      <td>0.258183</td>\n      <td>0.359196</td>\n      <td>0.495749</td>\n      <td>0.134124</td>\n      <td>0.201615</td>\n      <td>-0.158650</td>\n      <td>-0.109172</td>\n      <td>0.130224</td>\n      <td>0.076583</td>\n      <td>0.148866</td>\n      <td>0.102369</td>\n      <td>0.497895</td>\n      <td>-0.051056</td>\n      <td>0.102688</td>\n      <td>-0.061415</td>\n      <td>-0.181574</td>\n      <td>0.147057</td>\n      <td>-0.318179</td>\n      <td>0.168422</td>\n      <td>0.179673</td>\n      <td>0.135512</td>\n      <td>0.137194</td>\n      <td>-0.105513</td>\n      <td>0.090339</td>\n      <td>0.332361</td>\n      <td>-0.429137</td>\n      <td>0.738065</td>\n      <td>-0.029155</td>\n      <td>0.107776</td>\n      <td>0.450315</td>\n      <td>0.228201</td>\n      <td>-0.107785</td>\n      <td>0.047060</td>\n      <td>-0.067315</td>\n      <td>-0.356229</td>\n      <td>0.025094</td>\n      <td>-0.003247</td>\n      <td>0.239771</td>\n      <td>0.146900</td>\n      <td>0.051395</td>\n      <td>0.281231</td>\n      <td>0.123686</td>\n      <td>0.131963</td>\n      <td>-0.114395</td>\n      <td>-0.121030</td>\n      <td>0.183903</td>\n      <td>-0.183826</td>\n      <td>0.012366</td>\n      <td>0.325674</td>\n      <td>-0.062778</td>\n      <td>0.320075</td>\n      <td>-0.104069</td>\n      <td>-0.287190</td>\n      <td>0.000626</td>\n      <td>0.144869</td>\n      <td>0.005394</td>\n      <td>0.408010</td>\n      <td>-0.078483</td>\n      <td>0.338656</td>\n      <td>0.199368</td>\n      <td>0.276234</td>\n      <td>0.403440</td>\n      <td>0.105201</td>\n      <td>0.111741</td>\n      <td>0.130294</td>\n      <td>0.073811</td>\n      <td>-0.160810</td>\n      <td>4.747331</td>\n      <td>0.072952</td>\n      <td>0.055920</td>\n      <td>0.006768</td>\n      <td>0.349222</td>\n      <td>-0.130795</td>\n      <td>0.379010</td>\n      <td>-0.010385</td>\n      <td>0.090603</td>\n      <td>0.393199</td>\n      <td>-0.103349</td>\n      <td>0.147315</td>\n      <td>-0.291891</td>\n      <td>-0.066541</td>\n      <td>-0.279493</td>\n      <td>0.305302</td>\n      <td>0.201402</td>\n      <td>0.041830</td>\n      <td>0.200939</td>\n      <td>0.185602</td>\n      <td>-0.265424</td>\n      <td>0.502218</td>\n      <td>-0.028360</td>\n      <td>-0.168451</td>\n      <td>-0.105645</td>\n      <td>-0.079321</td>\n      <td>-0.047105</td>\n      <td>0.021278</td>\n      <td>-0.349123</td>\n      <td>0.029367</td>\n      <td>0.190332</td>\n      <td>-0.095118</td>\n      <td>0.012601</td>\n      <td>-0.382088</td>\n      <td>0.352415</td>\n      <td>-0.074742</td>\n      <td>-0.113953</td>\n      <td>-0.032978</td>\n      <td>0.584896</td>\n      <td>0.161829</td>\n      <td>-0.054128</td>\n      <td>0.538985</td>\n      <td>0.276367</td>\n      <td>0.136710</td>\n      <td>-0.199390</td>\n      <td>0.451597</td>\n      <td>0.319561</td>\n      <td>0.349828</td>\n      <td>-0.516844</td>\n      <td>0.000485</td>\n      <td>-0.179471</td>\n      <td>0.159981</td>\n      <td>-0.318736</td>\n      <td>0.053733</td>\n      <td>0.168461</td>\n      <td>-0.083542</td>\n      <td>-0.128236</td>\n      <td>0.099722</td>\n      <td>0.067470</td>\n      <td>0.119408</td>\n      <td>0.163848</td>\n      <td>-0.216542</td>\n      <td>0.249147</td>\n      <td>-0.213316</td>\n      <td>-0.413827</td>\n      <td>-0.094823</td>\n      <td>0.005856</td>\n      <td>-0.065209</td>\n      <td>0.421940</td>\n      <td>0.087920</td>\n      <td>-0.088703</td>\n      <td>0.061222</td>\n      <td>-0.012150</td>\n      <td>0.514841</td>\n      <td>-0.047247</td>\n      <td>-0.166690</td>\n      <td>-0.115741</td>\n      <td>-0.286324</td>\n      <td>-0.092178</td>\n      <td>-0.678961</td>\n      <td>0.259359</td>\n      <td>-0.159371</td>\n      <td>0.254227</td>\n      <td>0.149190</td>\n      <td>0.167796</td>\n      <td>0.210738</td>\n      <td>-0.151450</td>\n      <td>-0.210756</td>\n      <td>1.435943</td>\n      <td>-0.078241</td>\n      <td>-0.114692</td>\n      <td>0.008090</td>\n      <td>0.467595</td>\n      <td>0.145489</td>\n      <td>-0.087178</td>\n      <td>0.481435</td>\n      <td>-0.047705</td>\n      <td>0.118655</td>\n      <td>0.455190</td>\n      <td>-0.040371</td>\n      <td>0.091085</td>\n      <td>0.016216</td>\n      <td>0.157984</td>\n      <td>0.027836</td>\n      <td>0.257078</td>\n      <td>-0.279486</td>\n      <td>-0.004686</td>\n      <td>-0.395863</td>\n      <td>-0.020518</td>\n      <td>0.082706</td>\n      <td>-0.445489</td>\n      <td>0.042649</td>\n      <td>0.338311</td>\n      <td>-0.381562</td>\n      <td>-0.063263</td>\n      <td>-0.059470</td>\n      <td>-0.059927</td>\n      <td>0.297672</td>\n      <td>-0.069628</td>\n      <td>-0.407640</td>\n      <td>-0.082503</td>\n      <td>0.289098</td>\n      <td>0.337436</td>\n      <td>-0.082769</td>\n      <td>0.357320</td>\n      <td>0.176288</td>\n      <td>0.214432</td>\n      <td>-0.368707</td>\n      <td>-0.006962</td>\n      <td>0.433743</td>\n      <td>0.236393</td>\n      <td>-0.245224</td>\n      <td>0.358831</td>\n      <td>0.040904</td>\n      <td>-0.437277</td>\n      <td>0.061364</td>\n      <td>-0.360650</td>\n      <td>0.111130</td>\n      <td>0.309104</td>\n      <td>-0.351730</td>\n      <td>-0.155896</td>\n      <td>0.184967</td>\n      <td>-0.120664</td>\n      <td>0.107070</td>\n      <td>-0.267976</td>\n      <td>0.340889</td>\n      <td>0.591779</td>\n      <td>0.673692</td>\n      <td>0.400805</td>\n      <td>0.413043</td>\n      <td>-0.057027</td>\n      <td>0.038008</td>\n      <td>0.179726</td>\n      <td>0.380631</td>\n      <td>-0.099256</td>\n      <td>-0.326425</td>\n      <td>-0.098433</td>\n      <td>-0.249388</td>\n      <td>0.335000</td>\n      <td>0.161901</td>\n      <td>0.151454</td>\n      <td>-0.124199</td>\n      <td>-0.039004</td>\n      <td>0.027825</td>\n      <td>0.268244</td>\n      <td>-0.021239</td>\n      <td>-0.209983</td>\n      <td>-0.060947</td>\n      <td>0.383124</td>\n      <td>-0.181260</td>\n      <td>0.178984</td>\n      <td>0.190001</td>\n      <td>-0.159826</td>\n      <td>0.107045</td>\n      <td>-0.238369</td>\n      <td>0.317041</td>\n      <td>-0.307818</td>\n      <td>0.065264</td>\n      <td>0.075966</td>\n      <td>-0.183574</td>\n      <td>0.121416</td>\n      <td>0.280752</td>\n      <td>-0.047025</td>\n      <td>-0.072381</td>\n      <td>-0.069134</td>\n      <td>0.102866</td>\n      <td>0.240492</td>\n      <td>-0.387272</td>\n      <td>0.182156</td>\n      <td>0.334470</td>\n      <td>-0.329235</td>\n      <td>0.112086</td>\n      <td>-0.065010</td>\n      <td>0.175637</td>\n      <td>0.071752</td>\n      <td>0.064201</td>\n      <td>0.160761</td>\n      <td>0.134399</td>\n      <td>0.268696</td>\n      <td>-0.029933</td>\n      <td>-0.601953</td>\n      <td>-0.097186</td>\n      <td>0.085533</td>\n      <td>0.322962</td>\n      <td>-0.206732</td>\n      <td>0.430094</td>\n      <td>0.407679</td>\n      <td>-0.209427</td>\n      <td>-0.170119</td>\n      <td>-0.335401</td>\n      <td>0.116572</td>\n      <td>0.210395</td>\n      <td>-0.055914</td>\n      <td>-0.043767</td>\n      <td>-0.127778</td>\n      <td>0.783382</td>\n      <td>0.140414</td>\n      <td>-0.094320</td>\n      <td>0.007408</td>\n      <td>0.014933</td>\n      <td>0.313902</td>\n      <td>0.029507</td>\n      <td>-0.076028</td>\n      <td>0.322393</td>\n      <td>-0.310212</td>\n      <td>-0.617736</td>\n      <td>0.272213</td>\n      <td>0.149387</td>\n      <td>-0.275052</td>\n      <td>-0.143151</td>\n      <td>-0.174631</td>\n      <td>0.247282</td>\n      <td>-0.062002</td>\n      <td>0.250701</td>\n      <td>0.099866</td>\n      <td>-0.041563</td>\n      <td>-0.085862</td>\n      <td>0.078959</td>\n      <td>-0.413587</td>\n      <td>0.169517</td>\n      <td>0.537831</td>\n      <td>0.206173</td>\n      <td>-0.105326</td>\n      <td>0.083155</td>\n      <td>-0.082530</td>\n      <td>0.049865</td>\n      <td>0.218028</td>\n      <td>-0.094545</td>\n      <td>-0.081364</td>\n      <td>0.122886</td>\n      <td>-0.019323</td>\n      <td>-0.050918</td>\n      <td>0.046577</td>\n      <td>0.033605</td>\n      <td>-0.457211</td>\n      <td>-0.032003</td>\n      <td>0.265508</td>\n      <td>-0.114494</td>\n      <td>0.387637</td>\n      <td>0.075453</td>\n      <td>0.091503</td>\n      <td>-0.005730</td>\n      <td>0.246317</td>\n      <td>-0.082467</td>\n      <td>-0.414326</td>\n      <td>-0.102766</td>\n      <td>0.559930</td>\n      <td>-0.024724</td>\n      <td>0.069124</td>\n      <td>0.234209</td>\n      <td>-0.104877</td>\n      <td>-0.267793</td>\n      <td>0.224398</td>\n      <td>0.196256</td>\n      <td>-0.106226</td>\n      <td>0.374463</td>\n      <td>-0.290358</td>\n      <td>-0.203963</td>\n      <td>-0.403375</td>\n      <td>-0.059965</td>\n      <td>0.462064</td>\n      <td>0.067607</td>\n      <td>0.144920</td>\n      <td>0.016390</td>\n      <td>-0.258372</td>\n      <td>-0.100162</td>\n      <td>0.007905</td>\n      <td>-0.272773</td>\n      <td>-0.068892</td>\n      <td>-0.164473</td>\n      <td>-0.241873</td>\n      <td>0.136297</td>\n      <td>0.032911</td>\n      <td>-0.019837</td>\n      <td>-0.354281</td>\n      <td>0.222190</td>\n      <td>-0.311203</td>\n      <td>0.001657</td>\n      <td>-0.009683</td>\n      <td>-0.200231</td>\n      <td>0.637772</td>\n      <td>0.152364</td>\n      <td>-0.076765</td>\n      <td>-0.076144</td>\n      <td>-0.025430</td>\n      <td>0.400906</td>\n      <td>-0.004340</td>\n      <td>0.103831</td>\n      <td>-0.071845</td>\n      <td>-0.126130</td>\n      <td>0.241650</td>\n      <td>0.188085</td>\n      <td>0.346959</td>\n      <td>0.119001</td>\n      <td>0.176695</td>\n      <td>-0.039616</td>\n      <td>0.095833</td>\n      <td>0.033078</td>\n      <td>-0.131427</td>\n      <td>0.048515</td>\n      <td>-0.047153</td>\n      <td>-0.144548</td>\n      <td>-0.027911</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.227629</td>\n      <td>0.225224</td>\n      <td>0.231292</td>\n      <td>-0.106223</td>\n      <td>0.059433</td>\n      <td>0.013205</td>\n      <td>0.105225</td>\n      <td>0.030600</td>\n      <td>0.051235</td>\n      <td>-0.101789</td>\n      <td>0.027778</td>\n      <td>0.269350</td>\n      <td>0.082311</td>\n      <td>-0.017754</td>\n      <td>-0.038839</td>\n      <td>-0.310158</td>\n      <td>0.218842</td>\n      <td>0.006542</td>\n      <td>0.533929</td>\n      <td>0.198319</td>\n      <td>0.105475</td>\n      <td>-0.803174</td>\n      <td>-0.240670</td>\n      <td>-0.106467</td>\n      <td>0.300168</td>\n      <td>0.006877</td>\n      <td>-0.116769</td>\n      <td>0.036999</td>\n      <td>-0.235160</td>\n      <td>0.041112</td>\n      <td>0.045297</td>\n      <td>0.244439</td>\n      <td>-0.385171</td>\n      <td>-0.182888</td>\n      <td>0.287584</td>\n      <td>-0.120918</td>\n      <td>-0.088403</td>\n      <td>0.316307</td>\n      <td>0.234557</td>\n      <td>-0.032739</td>\n      <td>0.043801</td>\n      <td>-0.103561</td>\n      <td>0.373260</td>\n      <td>-0.138574</td>\n      <td>0.237544</td>\n      <td>0.025560</td>\n      <td>0.207588</td>\n      <td>-0.057392</td>\n      <td>0.226009</td>\n      <td>0.149087</td>\n      <td>0.075969</td>\n      <td>0.183343</td>\n      <td>0.123620</td>\n      <td>0.108489</td>\n      <td>0.001905</td>\n      <td>-0.773536</td>\n      <td>-0.104454</td>\n      <td>0.408551</td>\n      <td>0.461901</td>\n      <td>0.381368</td>\n      <td>-0.058764</td>\n      <td>-0.114650</td>\n      <td>0.245143</td>\n      <td>0.194444</td>\n      <td>0.523532</td>\n      <td>-0.004141</td>\n      <td>0.124094</td>\n      <td>-0.287005</td>\n      <td>-0.035957</td>\n      <td>0.119354</td>\n      <td>-0.257367</td>\n      <td>-0.511525</td>\n      <td>-0.004055</td>\n      <td>0.192340</td>\n      <td>-0.387030</td>\n      <td>0.074694</td>\n      <td>0.045565</td>\n      <td>0.200065</td>\n      <td>-0.144557</td>\n      <td>-0.087852</td>\n      <td>0.124347</td>\n      <td>-0.084571</td>\n      <td>-0.070033</td>\n      <td>0.252908</td>\n      <td>0.001638</td>\n      <td>-0.131021</td>\n      <td>0.060998</td>\n      <td>0.108455</td>\n      <td>0.096317</td>\n      <td>0.075333</td>\n      <td>0.134659</td>\n      <td>-0.002534</td>\n      <td>0.026052</td>\n      <td>-0.319822</td>\n      <td>-0.039080</td>\n      <td>0.028208</td>\n      <td>-0.157551</td>\n      <td>-0.070561</td>\n      <td>-0.397704</td>\n      <td>0.472786</td>\n      <td>-0.071336</td>\n      <td>-0.084122</td>\n      <td>0.159971</td>\n      <td>0.045540</td>\n      <td>0.337496</td>\n      <td>0.325120</td>\n      <td>0.133904</td>\n      <td>-0.362409</td>\n      <td>-0.161822</td>\n      <td>0.270715</td>\n      <td>0.266550</td>\n      <td>-0.105878</td>\n      <td>-0.077163</td>\n      <td>-0.087837</td>\n      <td>-0.374546</td>\n      <td>-0.217475</td>\n      <td>0.131968</td>\n      <td>0.229010</td>\n      <td>0.124254</td>\n      <td>-0.218547</td>\n      <td>0.082128</td>\n      <td>0.147969</td>\n      <td>-0.632372</td>\n      <td>-0.033969</td>\n      <td>-0.046950</td>\n      <td>0.357366</td>\n      <td>0.074304</td>\n      <td>-0.125998</td>\n      <td>0.567422</td>\n      <td>0.219362</td>\n      <td>0.108973</td>\n      <td>0.239894</td>\n      <td>-0.025410</td>\n      <td>0.151556</td>\n      <td>0.220999</td>\n      <td>0.407328</td>\n      <td>0.286155</td>\n      <td>0.086555</td>\n      <td>0.204056</td>\n      <td>0.105037</td>\n      <td>-0.066177</td>\n      <td>0.173624</td>\n      <td>0.239872</td>\n      <td>0.090851</td>\n      <td>0.013560</td>\n      <td>0.163928</td>\n      <td>-0.280196</td>\n      <td>0.192369</td>\n      <td>0.296814</td>\n      <td>0.162351</td>\n      <td>0.100971</td>\n      <td>0.432011</td>\n      <td>-0.194543</td>\n      <td>-0.153222</td>\n      <td>-0.076628</td>\n      <td>0.247797</td>\n      <td>0.037855</td>\n      <td>0.075523</td>\n      <td>0.089071</td>\n      <td>-0.395528</td>\n      <td>-0.251435</td>\n      <td>0.154281</td>\n      <td>0.092694</td>\n      <td>0.149225</td>\n      <td>-0.101007</td>\n      <td>-0.345696</td>\n      <td>0.077113</td>\n      <td>0.009493</td>\n      <td>0.267335</td>\n      <td>-0.500945</td>\n      <td>0.414689</td>\n      <td>0.301062</td>\n      <td>-0.302927</td>\n      <td>-0.241013</td>\n      <td>-0.382438</td>\n      <td>0.475339</td>\n      <td>0.009753</td>\n      <td>-0.019691</td>\n      <td>0.203157</td>\n      <td>0.356048</td>\n      <td>0.186692</td>\n      <td>-0.138737</td>\n      <td>0.336944</td>\n      <td>-0.460266</td>\n      <td>0.046250</td>\n      <td>0.350409</td>\n      <td>-0.630069</td>\n      <td>0.156080</td>\n      <td>0.093638</td>\n      <td>-0.188227</td>\n      <td>-0.351190</td>\n      <td>-0.058942</td>\n      <td>0.358646</td>\n      <td>0.254728</td>\n      <td>0.385072</td>\n      <td>0.154378</td>\n      <td>-0.249765</td>\n      <td>0.113245</td>\n      <td>0.037746</td>\n      <td>-0.056376</td>\n      <td>0.122183</td>\n      <td>0.105027</td>\n      <td>-0.049097</td>\n      <td>0.102195</td>\n      <td>-0.179841</td>\n      <td>0.251561</td>\n      <td>0.363708</td>\n      <td>0.198006</td>\n      <td>-0.045219</td>\n      <td>-0.263609</td>\n      <td>0.145885</td>\n      <td>-0.045822</td>\n      <td>-0.032397</td>\n      <td>0.192920</td>\n      <td>-0.116281</td>\n      <td>-3.179759</td>\n      <td>-0.196065</td>\n      <td>-0.007169</td>\n      <td>0.043181</td>\n      <td>0.042290</td>\n      <td>-0.384116</td>\n      <td>0.285349</td>\n      <td>0.199268</td>\n      <td>-0.112486</td>\n      <td>0.034734</td>\n      <td>-0.243222</td>\n      <td>-0.184304</td>\n      <td>0.074179</td>\n      <td>-0.336608</td>\n      <td>0.201832</td>\n      <td>0.180492</td>\n      <td>0.480714</td>\n      <td>-0.087649</td>\n      <td>-0.243396</td>\n      <td>0.104218</td>\n      <td>0.341130</td>\n      <td>-0.145712</td>\n      <td>0.262657</td>\n      <td>-0.090222</td>\n      <td>-0.241125</td>\n      <td>0.172139</td>\n      <td>0.004944</td>\n      <td>0.539119</td>\n      <td>0.135715</td>\n      <td>0.008567</td>\n      <td>0.017065</td>\n      <td>0.324559</td>\n      <td>0.766401</td>\n      <td>0.223282</td>\n      <td>0.007300</td>\n      <td>0.365641</td>\n      <td>-0.033623</td>\n      <td>0.054330</td>\n      <td>0.080136</td>\n      <td>0.418952</td>\n      <td>0.272747</td>\n      <td>0.151535</td>\n      <td>0.051640</td>\n      <td>0.225289</td>\n      <td>-0.061753</td>\n      <td>0.270189</td>\n      <td>0.452422</td>\n      <td>-0.208400</td>\n      <td>0.118558</td>\n      <td>0.099702</td>\n      <td>-0.004625</td>\n      <td>0.411649</td>\n      <td>0.015993</td>\n      <td>0.129711</td>\n      <td>-0.074533</td>\n      <td>0.225088</td>\n      <td>-0.396319</td>\n      <td>0.044865</td>\n      <td>-0.144067</td>\n      <td>0.378246</td>\n      <td>0.368826</td>\n      <td>-0.463148</td>\n      <td>0.354378</td>\n      <td>0.150233</td>\n      <td>-0.379255</td>\n      <td>0.475093</td>\n      <td>-0.321968</td>\n      <td>-0.057343</td>\n      <td>0.008990</td>\n      <td>0.142343</td>\n      <td>-0.191032</td>\n      <td>-0.295985</td>\n      <td>0.246414</td>\n      <td>-0.108251</td>\n      <td>0.501328</td>\n      <td>0.327485</td>\n      <td>-0.212088</td>\n      <td>-0.448749</td>\n      <td>0.320620</td>\n      <td>-0.167318</td>\n      <td>0.372286</td>\n      <td>0.009420</td>\n      <td>0.120864</td>\n      <td>0.027195</td>\n      <td>-0.243592</td>\n      <td>0.236433</td>\n      <td>0.165549</td>\n      <td>-0.180362</td>\n      <td>0.109129</td>\n      <td>-0.239819</td>\n      <td>-0.089468</td>\n      <td>0.113333</td>\n      <td>0.423506</td>\n      <td>0.066341</td>\n      <td>-0.163666</td>\n      <td>-0.182584</td>\n      <td>-0.184029</td>\n      <td>0.710894</td>\n      <td>0.001453</td>\n      <td>-0.044658</td>\n      <td>0.246526</td>\n      <td>0.135383</td>\n      <td>0.114494</td>\n      <td>-0.045414</td>\n      <td>0.106293</td>\n      <td>0.202201</td>\n      <td>-0.032879</td>\n      <td>0.054366</td>\n      <td>-0.325524</td>\n      <td>-0.023439</td>\n      <td>-0.206997</td>\n      <td>0.230366</td>\n      <td>0.580575</td>\n      <td>-0.136127</td>\n      <td>-0.152403</td>\n      <td>-0.303149</td>\n      <td>-0.112432</td>\n      <td>0.164106</td>\n      <td>-0.058271</td>\n      <td>-0.334430</td>\n      <td>0.247042</td>\n      <td>0.271443</td>\n      <td>0.034018</td>\n      <td>-0.484950</td>\n      <td>-0.168721</td>\n      <td>0.282506</td>\n      <td>0.232015</td>\n      <td>0.287673</td>\n      <td>-1.112222</td>\n      <td>0.519301</td>\n      <td>0.322298</td>\n      <td>-0.415515</td>\n      <td>0.569293</td>\n      <td>0.148346</td>\n      <td>-0.190594</td>\n      <td>0.320587</td>\n      <td>-0.360830</td>\n      <td>0.643503</td>\n      <td>0.153274</td>\n      <td>0.095032</td>\n      <td>0.468780</td>\n      <td>-0.071511</td>\n      <td>0.089628</td>\n      <td>-0.156166</td>\n      <td>0.212629</td>\n      <td>-0.797346</td>\n      <td>0.087646</td>\n      <td>0.162812</td>\n      <td>0.334125</td>\n      <td>-0.002642</td>\n      <td>-0.156543</td>\n      <td>0.145238</td>\n      <td>-0.261697</td>\n      <td>0.051554</td>\n      <td>-0.294390</td>\n      <td>-0.102898</td>\n      <td>-0.025798</td>\n      <td>0.094722</td>\n      <td>0.215585</td>\n      <td>0.035600</td>\n      <td>0.065640</td>\n      <td>-0.693600</td>\n      <td>-1.572459</td>\n      <td>0.251762</td>\n      <td>0.318642</td>\n      <td>0.574410</td>\n      <td>0.121396</td>\n      <td>0.218621</td>\n      <td>-0.145323</td>\n      <td>-0.043381</td>\n      <td>0.151153</td>\n      <td>0.075572</td>\n      <td>-0.094515</td>\n      <td>-0.013944</td>\n      <td>0.373636</td>\n      <td>-0.027739</td>\n      <td>0.039750</td>\n      <td>-0.118055</td>\n      <td>-0.249140</td>\n      <td>0.197492</td>\n      <td>-0.402778</td>\n      <td>0.195925</td>\n      <td>0.229122</td>\n      <td>0.205297</td>\n      <td>0.186908</td>\n      <td>-0.126872</td>\n      <td>0.075641</td>\n      <td>0.112252</td>\n      <td>-0.250059</td>\n      <td>0.615707</td>\n      <td>0.055545</td>\n      <td>0.119774</td>\n      <td>0.347842</td>\n      <td>0.190807</td>\n      <td>-0.093280</td>\n      <td>0.112409</td>\n      <td>-0.100943</td>\n      <td>-0.259254</td>\n      <td>0.037487</td>\n      <td>-0.083349</td>\n      <td>0.245945</td>\n      <td>0.271640</td>\n      <td>0.169596</td>\n      <td>0.229249</td>\n      <td>0.082409</td>\n      <td>0.038823</td>\n      <td>0.000280</td>\n      <td>-0.066066</td>\n      <td>0.107624</td>\n      <td>-0.265498</td>\n      <td>-0.134188</td>\n      <td>0.246832</td>\n      <td>-0.045114</td>\n      <td>-0.118571</td>\n      <td>-0.147457</td>\n      <td>-0.151012</td>\n      <td>-0.022420</td>\n      <td>-0.024002</td>\n      <td>0.108842</td>\n      <td>0.335836</td>\n      <td>-0.066827</td>\n      <td>0.171031</td>\n      <td>0.328363</td>\n      <td>0.267881</td>\n      <td>0.358730</td>\n      <td>0.233092</td>\n      <td>0.064519</td>\n      <td>0.063946</td>\n      <td>-0.036409</td>\n      <td>-0.088903</td>\n      <td>4.735420</td>\n      <td>0.224308</td>\n      <td>0.247671</td>\n      <td>0.120584</td>\n      <td>0.165223</td>\n      <td>-0.192003</td>\n      <td>0.529650</td>\n      <td>0.013423</td>\n      <td>-0.024935</td>\n      <td>0.277555</td>\n      <td>-0.126389</td>\n      <td>0.087202</td>\n      <td>-0.320496</td>\n      <td>0.010809</td>\n      <td>-0.227202</td>\n      <td>0.341529</td>\n      <td>0.150292</td>\n      <td>0.071865</td>\n      <td>0.166022</td>\n      <td>0.127493</td>\n      <td>-0.122384</td>\n      <td>0.515608</td>\n      <td>0.017776</td>\n      <td>-0.269383</td>\n      <td>-0.076361</td>\n      <td>-0.196708</td>\n      <td>-0.073824</td>\n      <td>0.114449</td>\n      <td>-0.230122</td>\n      <td>0.152294</td>\n      <td>0.139875</td>\n      <td>0.101156</td>\n      <td>0.020042</td>\n      <td>-0.119836</td>\n      <td>0.389177</td>\n      <td>-0.135395</td>\n      <td>-0.320353</td>\n      <td>0.068988</td>\n      <td>0.461596</td>\n      <td>0.101328</td>\n      <td>-0.042104</td>\n      <td>0.443206</td>\n      <td>0.194410</td>\n      <td>0.066657</td>\n      <td>-0.003343</td>\n      <td>0.383663</td>\n      <td>0.328565</td>\n      <td>0.650958</td>\n      <td>-0.660451</td>\n      <td>-0.044131</td>\n      <td>-0.091020</td>\n      <td>0.058883</td>\n      <td>-0.268196</td>\n      <td>0.027388</td>\n      <td>0.293735</td>\n      <td>0.008397</td>\n      <td>-0.088674</td>\n      <td>0.139552</td>\n      <td>0.075708</td>\n      <td>0.099574</td>\n      <td>0.231444</td>\n      <td>-0.298517</td>\n      <td>0.207255</td>\n      <td>-0.166472</td>\n      <td>-0.382062</td>\n      <td>-0.107599</td>\n      <td>-0.138740</td>\n      <td>-0.054414</td>\n      <td>0.417538</td>\n      <td>-0.191485</td>\n      <td>-0.202802</td>\n      <td>0.042192</td>\n      <td>0.023265</td>\n      <td>0.540775</td>\n      <td>-0.000280</td>\n      <td>-0.092398</td>\n      <td>-0.095384</td>\n      <td>-0.215992</td>\n      <td>-0.106038</td>\n      <td>-0.676636</td>\n      <td>0.164287</td>\n      <td>-0.148349</td>\n      <td>0.110805</td>\n      <td>0.116059</td>\n      <td>0.173325</td>\n      <td>0.277992</td>\n      <td>-0.185510</td>\n      <td>-0.183296</td>\n      <td>1.302565</td>\n      <td>0.012073</td>\n      <td>-0.008259</td>\n      <td>0.021889</td>\n      <td>0.438934</td>\n      <td>0.177178</td>\n      <td>-0.159774</td>\n      <td>0.605235</td>\n      <td>-0.185618</td>\n      <td>0.083717</td>\n      <td>0.556896</td>\n      <td>-0.012802</td>\n      <td>0.076579</td>\n      <td>-0.019178</td>\n      <td>0.087928</td>\n      <td>-0.013053</td>\n      <td>0.340684</td>\n      <td>-0.164111</td>\n      <td>0.038193</td>\n      <td>-0.436777</td>\n      <td>-0.062536</td>\n      <td>0.094718</td>\n      <td>-0.480012</td>\n      <td>0.064211</td>\n      <td>0.595901</td>\n      <td>-0.420840</td>\n      <td>-0.078573</td>\n      <td>-0.044344</td>\n      <td>-0.094345</td>\n      <td>0.448299</td>\n      <td>-0.141775</td>\n      <td>-0.321848</td>\n      <td>-0.101493</td>\n      <td>0.042241</td>\n      <td>0.328914</td>\n      <td>-0.192891</td>\n      <td>0.483030</td>\n      <td>0.038726</td>\n      <td>0.311949</td>\n      <td>-0.353370</td>\n      <td>-0.042744</td>\n      <td>0.437307</td>\n      <td>0.138747</td>\n      <td>-0.262991</td>\n      <td>0.233440</td>\n      <td>-0.178823</td>\n      <td>-0.372884</td>\n      <td>-0.049909</td>\n      <td>-0.436951</td>\n      <td>0.107153</td>\n      <td>0.160562</td>\n      <td>-0.262083</td>\n      <td>-0.193632</td>\n      <td>0.173114</td>\n      <td>-0.368331</td>\n      <td>0.188000</td>\n      <td>-0.444024</td>\n      <td>0.301579</td>\n      <td>0.668362</td>\n      <td>0.620575</td>\n      <td>0.437763</td>\n      <td>0.291784</td>\n      <td>-0.092909</td>\n      <td>-0.018777</td>\n      <td>0.270073</td>\n      <td>0.285350</td>\n      <td>-0.054565</td>\n      <td>-0.312149</td>\n      <td>0.000933</td>\n      <td>-0.081424</td>\n      <td>0.215595</td>\n      <td>0.180109</td>\n      <td>0.059430</td>\n      <td>-0.199825</td>\n      <td>0.016596</td>\n      <td>0.037411</td>\n      <td>0.251007</td>\n      <td>-0.100918</td>\n      <td>-0.117772</td>\n      <td>-0.010459</td>\n      <td>0.056066</td>\n      <td>0.021274</td>\n      <td>0.211695</td>\n      <td>0.142366</td>\n      <td>-0.200547</td>\n      <td>-0.080780</td>\n      <td>-0.200967</td>\n      <td>0.223285</td>\n      <td>-0.195979</td>\n      <td>-0.031363</td>\n      <td>-0.003380</td>\n      <td>-0.369168</td>\n      <td>0.245116</td>\n      <td>0.191714</td>\n      <td>-0.096014</td>\n      <td>-0.040704</td>\n      <td>-0.031535</td>\n      <td>0.134452</td>\n      <td>0.165713</td>\n      <td>-0.288285</td>\n      <td>0.108456</td>\n      <td>0.287496</td>\n      <td>-0.222331</td>\n      <td>0.122112</td>\n      <td>-0.065364</td>\n      <td>0.172508</td>\n      <td>0.088675</td>\n      <td>0.086672</td>\n      <td>0.104378</td>\n      <td>0.189148</td>\n      <td>0.154208</td>\n      <td>-0.040203</td>\n      <td>-0.602993</td>\n      <td>-0.122736</td>\n      <td>0.062129</td>\n      <td>0.015915</td>\n      <td>-0.141395</td>\n      <td>0.319665</td>\n      <td>0.389355</td>\n      <td>-0.153609</td>\n      <td>-0.262987</td>\n      <td>-0.375414</td>\n      <td>0.186913</td>\n      <td>0.225910</td>\n      <td>-0.110922</td>\n      <td>-0.125461</td>\n      <td>-0.077291</td>\n      <td>0.735426</td>\n      <td>0.099012</td>\n      <td>-0.132591</td>\n      <td>0.154569</td>\n      <td>-0.037251</td>\n      <td>0.234324</td>\n      <td>-0.017863</td>\n      <td>-0.182162</td>\n      <td>0.221957</td>\n      <td>-0.308723</td>\n      <td>-0.368446</td>\n      <td>0.291514</td>\n      <td>0.145127</td>\n      <td>-0.231798</td>\n      <td>-0.232929</td>\n      <td>-0.082045</td>\n      <td>0.282466</td>\n      <td>-0.147976</td>\n      <td>0.168857</td>\n      <td>0.217745</td>\n      <td>0.022542</td>\n      <td>-0.129517</td>\n      <td>-0.009451</td>\n      <td>-0.323145</td>\n      <td>0.094201</td>\n      <td>0.472913</td>\n      <td>0.221711</td>\n      <td>-0.037085</td>\n      <td>0.097374</td>\n      <td>-0.044385</td>\n      <td>0.273708</td>\n      <td>0.228361</td>\n      <td>0.045839</td>\n      <td>-0.050821</td>\n      <td>0.183618</td>\n      <td>0.056735</td>\n      <td>0.095231</td>\n      <td>0.019847</td>\n      <td>-0.037542</td>\n      <td>-0.378556</td>\n      <td>-0.019202</td>\n      <td>0.266377</td>\n      <td>-0.038626</td>\n      <td>0.557122</td>\n      <td>-0.158079</td>\n      <td>0.054925</td>\n      <td>0.012482</td>\n      <td>0.249959</td>\n      <td>-0.021767</td>\n      <td>-0.337370</td>\n      <td>-0.160150</td>\n      <td>0.533608</td>\n      <td>0.011663</td>\n      <td>0.311485</td>\n      <td>0.421899</td>\n      <td>0.063289</td>\n      <td>-0.120987</td>\n      <td>0.239355</td>\n      <td>0.316279</td>\n      <td>-0.104481</td>\n      <td>0.417814</td>\n      <td>-0.309106</td>\n      <td>-0.254129</td>\n      <td>-0.275178</td>\n      <td>0.121118</td>\n      <td>0.447513</td>\n      <td>0.079621</td>\n      <td>0.206817</td>\n      <td>-0.173267</td>\n      <td>-0.189410</td>\n      <td>-0.181768</td>\n      <td>0.021070</td>\n      <td>-0.309448</td>\n      <td>0.005398</td>\n      <td>-0.120456</td>\n      <td>-0.362979</td>\n      <td>0.030736</td>\n      <td>0.076042</td>\n      <td>-0.013623</td>\n      <td>-0.415654</td>\n      <td>0.190638</td>\n      <td>-0.297017</td>\n      <td>-0.152379</td>\n      <td>-0.072528</td>\n      <td>-0.054599</td>\n      <td>0.602690</td>\n      <td>0.295859</td>\n      <td>-0.077529</td>\n      <td>-0.229374</td>\n      <td>0.067749</td>\n      <td>0.321622</td>\n      <td>-0.181074</td>\n      <td>-0.047573</td>\n      <td>-0.211155</td>\n      <td>-0.117754</td>\n      <td>0.300636</td>\n      <td>0.058140</td>\n      <td>0.292613</td>\n      <td>0.120544</td>\n      <td>0.155199</td>\n      <td>-0.049755</td>\n      <td>0.152939</td>\n      <td>-0.055066</td>\n      <td>-0.050144</td>\n      <td>0.104712</td>\n      <td>-0.058793</td>\n      <td>0.008540</td>\n      <td>0.052572</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.156663</td>\n      <td>0.144015</td>\n      <td>0.121652</td>\n      <td>-0.039961</td>\n      <td>0.181759</td>\n      <td>0.034058</td>\n      <td>0.070990</td>\n      <td>0.120514</td>\n      <td>-0.091443</td>\n      <td>-0.222109</td>\n      <td>0.044104</td>\n      <td>0.158231</td>\n      <td>0.242143</td>\n      <td>-0.003043</td>\n      <td>-0.049446</td>\n      <td>-0.339215</td>\n      <td>0.057154</td>\n      <td>0.166102</td>\n      <td>0.481451</td>\n      <td>0.215605</td>\n      <td>0.102058</td>\n      <td>-1.040277</td>\n      <td>-0.183183</td>\n      <td>-0.149181</td>\n      <td>0.233562</td>\n      <td>0.003653</td>\n      <td>-0.096026</td>\n      <td>-0.003947</td>\n      <td>-0.140939</td>\n      <td>0.012934</td>\n      <td>0.069196</td>\n      <td>0.152730</td>\n      <td>-0.470859</td>\n      <td>-0.074503</td>\n      <td>0.285940</td>\n      <td>-0.079989</td>\n      <td>-0.087406</td>\n      <td>0.257870</td>\n      <td>0.337321</td>\n      <td>0.060606</td>\n      <td>-0.057308</td>\n      <td>-0.366120</td>\n      <td>0.376492</td>\n      <td>-0.161677</td>\n      <td>0.403069</td>\n      <td>0.024838</td>\n      <td>0.303680</td>\n      <td>-0.177946</td>\n      <td>0.350208</td>\n      <td>-0.023298</td>\n      <td>-0.020037</td>\n      <td>0.455113</td>\n      <td>0.105763</td>\n      <td>-0.098347</td>\n      <td>0.033148</td>\n      <td>-0.702089</td>\n      <td>-0.204126</td>\n      <td>0.427760</td>\n      <td>0.327849</td>\n      <td>0.341120</td>\n      <td>-0.067064</td>\n      <td>-0.025585</td>\n      <td>0.207255</td>\n      <td>0.169362</td>\n      <td>0.548283</td>\n      <td>0.250421</td>\n      <td>0.054299</td>\n      <td>-0.192055</td>\n      <td>-0.066694</td>\n      <td>0.160709</td>\n      <td>-0.473928</td>\n      <td>-0.297893</td>\n      <td>0.178422</td>\n      <td>0.161338</td>\n      <td>-0.412606</td>\n      <td>-0.054201</td>\n      <td>-0.099775</td>\n      <td>0.260790</td>\n      <td>-0.173795</td>\n      <td>-0.159312</td>\n      <td>0.236255</td>\n      <td>-0.102328</td>\n      <td>-0.276479</td>\n      <td>0.002062</td>\n      <td>0.111890</td>\n      <td>0.016183</td>\n      <td>0.030645</td>\n      <td>0.151044</td>\n      <td>0.075064</td>\n      <td>-0.014816</td>\n      <td>0.120915</td>\n      <td>0.087660</td>\n      <td>-0.029492</td>\n      <td>-0.183027</td>\n      <td>-0.018089</td>\n      <td>0.001679</td>\n      <td>-0.109386</td>\n      <td>-0.070902</td>\n      <td>-0.334765</td>\n      <td>0.428085</td>\n      <td>-0.251212</td>\n      <td>-0.124699</td>\n      <td>0.176491</td>\n      <td>0.002355</td>\n      <td>0.218339</td>\n      <td>0.207843</td>\n      <td>0.072184</td>\n      <td>-0.304653</td>\n      <td>-0.122569</td>\n      <td>0.143048</td>\n      <td>0.014649</td>\n      <td>-0.061174</td>\n      <td>-0.057008</td>\n      <td>0.014338</td>\n      <td>-0.415426</td>\n      <td>-0.159387</td>\n      <td>0.002112</td>\n      <td>0.006859</td>\n      <td>0.068110</td>\n      <td>-0.125654</td>\n      <td>0.121386</td>\n      <td>0.133449</td>\n      <td>-0.567735</td>\n      <td>-0.065501</td>\n      <td>-0.011225</td>\n      <td>0.417597</td>\n      <td>-0.112115</td>\n      <td>-0.274241</td>\n      <td>0.552531</td>\n      <td>0.313544</td>\n      <td>0.258108</td>\n      <td>0.292497</td>\n      <td>0.033401</td>\n      <td>0.018047</td>\n      <td>0.098912</td>\n      <td>0.312311</td>\n      <td>0.050923</td>\n      <td>0.078871</td>\n      <td>0.180917</td>\n      <td>-0.246239</td>\n      <td>-0.111141</td>\n      <td>0.242654</td>\n      <td>0.195084</td>\n      <td>0.006037</td>\n      <td>0.115205</td>\n      <td>0.169492</td>\n      <td>-0.150973</td>\n      <td>0.028631</td>\n      <td>0.385599</td>\n      <td>0.343959</td>\n      <td>-0.086351</td>\n      <td>0.354821</td>\n      <td>-0.143474</td>\n      <td>-0.157999</td>\n      <td>-0.027486</td>\n      <td>0.161247</td>\n      <td>-0.018787</td>\n      <td>0.096103</td>\n      <td>-0.039661</td>\n      <td>-0.425403</td>\n      <td>-0.282121</td>\n      <td>-0.000790</td>\n      <td>0.115437</td>\n      <td>0.229407</td>\n      <td>-0.009209</td>\n      <td>-0.304521</td>\n      <td>0.132736</td>\n      <td>0.100025</td>\n      <td>0.295487</td>\n      <td>-0.548718</td>\n      <td>0.508793</td>\n      <td>0.212659</td>\n      <td>-0.285575</td>\n      <td>-0.119210</td>\n      <td>-0.188746</td>\n      <td>0.652089</td>\n      <td>0.013219</td>\n      <td>0.054829</td>\n      <td>0.290093</td>\n      <td>0.320279</td>\n      <td>0.168257</td>\n      <td>-0.209247</td>\n      <td>0.242835</td>\n      <td>-0.303448</td>\n      <td>-0.030037</td>\n      <td>0.227640</td>\n      <td>-0.641570</td>\n      <td>0.121716</td>\n      <td>0.166924</td>\n      <td>-0.242704</td>\n      <td>-0.477683</td>\n      <td>-0.121446</td>\n      <td>0.464237</td>\n      <td>0.142659</td>\n      <td>0.219073</td>\n      <td>0.183695</td>\n      <td>-0.331932</td>\n      <td>0.028306</td>\n      <td>-0.181870</td>\n      <td>0.004978</td>\n      <td>0.176789</td>\n      <td>0.015282</td>\n      <td>-0.115615</td>\n      <td>0.152025</td>\n      <td>-0.113486</td>\n      <td>0.178382</td>\n      <td>0.223526</td>\n      <td>0.186151</td>\n      <td>-0.084463</td>\n      <td>-0.341933</td>\n      <td>0.160031</td>\n      <td>-0.098737</td>\n      <td>-0.005475</td>\n      <td>0.068233</td>\n      <td>-0.022395</td>\n      <td>-3.204847</td>\n      <td>-0.141447</td>\n      <td>-0.026438</td>\n      <td>0.171242</td>\n      <td>-0.068086</td>\n      <td>-0.314257</td>\n      <td>0.341201</td>\n      <td>0.311529</td>\n      <td>-0.195146</td>\n      <td>0.094182</td>\n      <td>-0.178546</td>\n      <td>-0.113799</td>\n      <td>-0.017555</td>\n      <td>-0.305044</td>\n      <td>0.193052</td>\n      <td>0.287055</td>\n      <td>0.415349</td>\n      <td>-0.151645</td>\n      <td>-0.271396</td>\n      <td>0.035253</td>\n      <td>0.146891</td>\n      <td>-0.048210</td>\n      <td>0.420014</td>\n      <td>-0.091909</td>\n      <td>-0.163848</td>\n      <td>0.237720</td>\n      <td>-0.000511</td>\n      <td>0.588322</td>\n      <td>0.084210</td>\n      <td>0.129060</td>\n      <td>-0.001547</td>\n      <td>0.440588</td>\n      <td>0.693371</td>\n      <td>0.121086</td>\n      <td>-0.012625</td>\n      <td>0.305039</td>\n      <td>-0.031379</td>\n      <td>-0.034435</td>\n      <td>0.006126</td>\n      <td>0.299577</td>\n      <td>0.345784</td>\n      <td>0.080592</td>\n      <td>0.082613</td>\n      <td>0.135674</td>\n      <td>-0.111408</td>\n      <td>0.445957</td>\n      <td>0.456607</td>\n      <td>-0.178324</td>\n      <td>-0.007740</td>\n      <td>0.113862</td>\n      <td>0.074956</td>\n      <td>0.481392</td>\n      <td>0.005457</td>\n      <td>0.233580</td>\n      <td>-0.058320</td>\n      <td>0.294973</td>\n      <td>-0.343200</td>\n      <td>-0.086051</td>\n      <td>-0.307408</td>\n      <td>0.387312</td>\n      <td>0.354753</td>\n      <td>-0.427438</td>\n      <td>0.479795</td>\n      <td>0.200130</td>\n      <td>-0.314773</td>\n      <td>0.569507</td>\n      <td>-0.258279</td>\n      <td>-0.194728</td>\n      <td>-0.053379</td>\n      <td>0.056034</td>\n      <td>-0.015196</td>\n      <td>-0.131081</td>\n      <td>0.261108</td>\n      <td>-0.151185</td>\n      <td>0.532220</td>\n      <td>0.313282</td>\n      <td>-0.280039</td>\n      <td>-0.324210</td>\n      <td>0.152156</td>\n      <td>-0.337306</td>\n      <td>0.321261</td>\n      <td>0.027640</td>\n      <td>0.028485</td>\n      <td>-0.042114</td>\n      <td>-0.197813</td>\n      <td>0.497736</td>\n      <td>0.227063</td>\n      <td>-0.147483</td>\n      <td>0.030116</td>\n      <td>-0.139211</td>\n      <td>0.011582</td>\n      <td>-0.028006</td>\n      <td>0.388773</td>\n      <td>-0.013760</td>\n      <td>-0.092919</td>\n      <td>-0.225695</td>\n      <td>-0.080953</td>\n      <td>0.767048</td>\n      <td>0.120815</td>\n      <td>0.037046</td>\n      <td>0.443890</td>\n      <td>-0.038200</td>\n      <td>0.139776</td>\n      <td>-0.007890</td>\n      <td>0.042617</td>\n      <td>0.137429</td>\n      <td>-0.126990</td>\n      <td>0.256451</td>\n      <td>-0.212291</td>\n      <td>0.013423</td>\n      <td>-0.107143</td>\n      <td>0.051471</td>\n      <td>0.459323</td>\n      <td>-0.053593</td>\n      <td>-0.208996</td>\n      <td>-0.112290</td>\n      <td>-0.077355</td>\n      <td>0.105256</td>\n      <td>-0.180351</td>\n      <td>-0.364925</td>\n      <td>0.073399</td>\n      <td>0.243976</td>\n      <td>0.074563</td>\n      <td>-0.365365</td>\n      <td>-0.207265</td>\n      <td>0.258874</td>\n      <td>0.132119</td>\n      <td>0.181021</td>\n      <td>-1.040165</td>\n      <td>0.479087</td>\n      <td>0.109802</td>\n      <td>-0.398881</td>\n      <td>0.446057</td>\n      <td>0.233638</td>\n      <td>-0.221418</td>\n      <td>0.420982</td>\n      <td>-0.365871</td>\n      <td>0.286442</td>\n      <td>0.104808</td>\n      <td>0.106781</td>\n      <td>0.328672</td>\n      <td>-0.008369</td>\n      <td>-0.015763</td>\n      <td>-0.186026</td>\n      <td>0.322232</td>\n      <td>-0.774753</td>\n      <td>0.172023</td>\n      <td>0.086287</td>\n      <td>0.247828</td>\n      <td>0.124542</td>\n      <td>0.007692</td>\n      <td>0.133141</td>\n      <td>-0.148554</td>\n      <td>0.214363</td>\n      <td>-0.158922</td>\n      <td>-0.103673</td>\n      <td>-0.085503</td>\n      <td>0.199857</td>\n      <td>0.305532</td>\n      <td>0.222068</td>\n      <td>-0.073283</td>\n      <td>-0.889415</td>\n      <td>-1.420962</td>\n      <td>0.218933</td>\n      <td>0.352416</td>\n      <td>0.599981</td>\n      <td>0.098127</td>\n      <td>0.273358</td>\n      <td>-0.213906</td>\n      <td>0.023297</td>\n      <td>0.159789</td>\n      <td>0.117926</td>\n      <td>0.043708</td>\n      <td>-0.046526</td>\n      <td>0.415024</td>\n      <td>-0.190328</td>\n      <td>0.064324</td>\n      <td>-0.088316</td>\n      <td>-0.122251</td>\n      <td>0.275452</td>\n      <td>-0.467009</td>\n      <td>0.127770</td>\n      <td>0.199551</td>\n      <td>0.243181</td>\n      <td>0.146670</td>\n      <td>0.001147</td>\n      <td>-0.009912</td>\n      <td>0.333561</td>\n      <td>-0.365534</td>\n      <td>0.593835</td>\n      <td>0.239373</td>\n      <td>0.084423</td>\n      <td>0.394833</td>\n      <td>0.160417</td>\n      <td>-0.139481</td>\n      <td>0.061734</td>\n      <td>-0.153677</td>\n      <td>-0.277519</td>\n      <td>0.095881</td>\n      <td>-0.063011</td>\n      <td>0.218702</td>\n      <td>0.315401</td>\n      <td>0.182546</td>\n      <td>0.184343</td>\n      <td>0.117218</td>\n      <td>0.072643</td>\n      <td>-0.155542</td>\n      <td>0.177091</td>\n      <td>0.139776</td>\n      <td>-0.155007</td>\n      <td>-0.156836</td>\n      <td>0.245269</td>\n      <td>0.033423</td>\n      <td>-0.042583</td>\n      <td>-0.166833</td>\n      <td>-0.126384</td>\n      <td>0.015218</td>\n      <td>0.185972</td>\n      <td>0.146797</td>\n      <td>0.399611</td>\n      <td>-0.221173</td>\n      <td>0.145568</td>\n      <td>0.157571</td>\n      <td>0.297717</td>\n      <td>0.424670</td>\n      <td>0.238460</td>\n      <td>0.090002</td>\n      <td>0.161389</td>\n      <td>0.013415</td>\n      <td>-0.171204</td>\n      <td>4.659614</td>\n      <td>0.319820</td>\n      <td>0.207910</td>\n      <td>0.019596</td>\n      <td>0.113093</td>\n      <td>-0.021126</td>\n      <td>0.397520</td>\n      <td>0.028851</td>\n      <td>0.038145</td>\n      <td>0.269388</td>\n      <td>-0.071700</td>\n      <td>0.085228</td>\n      <td>-0.310905</td>\n      <td>-0.075097</td>\n      <td>-0.396189</td>\n      <td>0.218256</td>\n      <td>0.225526</td>\n      <td>0.089454</td>\n      <td>0.199574</td>\n      <td>0.320235</td>\n      <td>-0.121477</td>\n      <td>0.634486</td>\n      <td>0.001248</td>\n      <td>-0.108045</td>\n      <td>-0.134644</td>\n      <td>-0.147585</td>\n      <td>-0.144309</td>\n      <td>0.155484</td>\n      <td>-0.133366</td>\n      <td>0.137158</td>\n      <td>0.125155</td>\n      <td>-0.200577</td>\n      <td>-0.099577</td>\n      <td>-0.337281</td>\n      <td>0.489089</td>\n      <td>-0.184713</td>\n      <td>-0.219138</td>\n      <td>-0.055203</td>\n      <td>0.415248</td>\n      <td>0.018134</td>\n      <td>0.163483</td>\n      <td>0.425787</td>\n      <td>0.293932</td>\n      <td>-0.061277</td>\n      <td>-0.036749</td>\n      <td>0.349015</td>\n      <td>0.257537</td>\n      <td>0.506231</td>\n      <td>-0.484376</td>\n      <td>-0.086010</td>\n      <td>-0.232735</td>\n      <td>0.130123</td>\n      <td>-0.299626</td>\n      <td>0.178947</td>\n      <td>0.333248</td>\n      <td>0.087834</td>\n      <td>0.071150</td>\n      <td>0.285669</td>\n      <td>-0.066506</td>\n      <td>0.115129</td>\n      <td>0.172095</td>\n      <td>-0.244898</td>\n      <td>0.172538</td>\n      <td>-0.266423</td>\n      <td>-0.298037</td>\n      <td>-0.065096</td>\n      <td>-0.147692</td>\n      <td>-0.164367</td>\n      <td>0.165593</td>\n      <td>-0.122191</td>\n      <td>-0.029296</td>\n      <td>0.136410</td>\n      <td>0.007887</td>\n      <td>0.600133</td>\n      <td>0.049115</td>\n      <td>-0.159277</td>\n      <td>-0.125493</td>\n      <td>-0.228019</td>\n      <td>-0.131946</td>\n      <td>-0.667638</td>\n      <td>0.245047</td>\n      <td>-0.215789</td>\n      <td>0.237266</td>\n      <td>0.253201</td>\n      <td>0.245690</td>\n      <td>0.207296</td>\n      <td>-0.170030</td>\n      <td>-0.176699</td>\n      <td>1.260766</td>\n      <td>0.068533</td>\n      <td>-0.155212</td>\n      <td>0.188573</td>\n      <td>0.553603</td>\n      <td>0.021000</td>\n      <td>-0.220785</td>\n      <td>0.480360</td>\n      <td>-0.047067</td>\n      <td>0.076357</td>\n      <td>0.432700</td>\n      <td>0.008528</td>\n      <td>0.086129</td>\n      <td>-0.015649</td>\n      <td>0.200788</td>\n      <td>0.041094</td>\n      <td>0.397157</td>\n      <td>-0.256051</td>\n      <td>-0.006302</td>\n      <td>-0.502058</td>\n      <td>0.003792</td>\n      <td>-0.046427</td>\n      <td>-0.432073</td>\n      <td>0.089857</td>\n      <td>0.469271</td>\n      <td>-0.342020</td>\n      <td>-0.066909</td>\n      <td>-0.195322</td>\n      <td>0.057461</td>\n      <td>0.470980</td>\n      <td>-0.084404</td>\n      <td>-0.246583</td>\n      <td>0.020685</td>\n      <td>0.184780</td>\n      <td>0.290825</td>\n      <td>-0.261427</td>\n      <td>0.292839</td>\n      <td>0.070038</td>\n      <td>0.272943</td>\n      <td>-0.430346</td>\n      <td>0.071460</td>\n      <td>0.577150</td>\n      <td>0.060315</td>\n      <td>-0.162282</td>\n      <td>0.127919</td>\n      <td>-0.041031</td>\n      <td>-0.262100</td>\n      <td>0.197708</td>\n      <td>-0.351543</td>\n      <td>0.309030</td>\n      <td>0.255049</td>\n      <td>-0.477788</td>\n      <td>-0.163560</td>\n      <td>0.198803</td>\n      <td>-0.487982</td>\n      <td>0.182566</td>\n      <td>-0.458819</td>\n      <td>0.327886</td>\n      <td>0.614070</td>\n      <td>0.611886</td>\n      <td>0.524146</td>\n      <td>0.424707</td>\n      <td>-0.005190</td>\n      <td>-0.081319</td>\n      <td>0.278993</td>\n      <td>0.338263</td>\n      <td>-0.040779</td>\n      <td>-0.155405</td>\n      <td>-0.010572</td>\n      <td>-0.022277</td>\n      <td>0.479041</td>\n      <td>-0.011212</td>\n      <td>-0.034441</td>\n      <td>-0.177459</td>\n      <td>0.024584</td>\n      <td>-0.016893</td>\n      <td>0.140829</td>\n      <td>0.028988</td>\n      <td>-0.137176</td>\n      <td>-0.265032</td>\n      <td>0.289644</td>\n      <td>-0.232542</td>\n      <td>0.121185</td>\n      <td>0.140518</td>\n      <td>-0.132441</td>\n      <td>0.078564</td>\n      <td>-0.243050</td>\n      <td>0.237742</td>\n      <td>-0.199846</td>\n      <td>-0.065695</td>\n      <td>-0.014940</td>\n      <td>-0.214697</td>\n      <td>0.142232</td>\n      <td>0.047358</td>\n      <td>-0.214465</td>\n      <td>0.066636</td>\n      <td>-0.093653</td>\n      <td>0.171615</td>\n      <td>0.161688</td>\n      <td>-0.287282</td>\n      <td>0.121923</td>\n      <td>0.463174</td>\n      <td>-0.316325</td>\n      <td>0.098219</td>\n      <td>-0.018035</td>\n      <td>0.223211</td>\n      <td>0.098358</td>\n      <td>0.174359</td>\n      <td>0.250949</td>\n      <td>0.217919</td>\n      <td>0.166736</td>\n      <td>0.087708</td>\n      <td>-0.568713</td>\n      <td>-0.020482</td>\n      <td>0.159302</td>\n      <td>0.187071</td>\n      <td>-0.040462</td>\n      <td>0.267023</td>\n      <td>0.275973</td>\n      <td>-0.030010</td>\n      <td>-0.413160</td>\n      <td>-0.333432</td>\n      <td>0.055706</td>\n      <td>0.272102</td>\n      <td>0.045399</td>\n      <td>-0.124057</td>\n      <td>-0.183621</td>\n      <td>0.661158</td>\n      <td>0.075592</td>\n      <td>0.154313</td>\n      <td>0.112476</td>\n      <td>-0.082439</td>\n      <td>0.266568</td>\n      <td>0.154344</td>\n      <td>-0.168504</td>\n      <td>0.196354</td>\n      <td>-0.250716</td>\n      <td>-0.597610</td>\n      <td>0.413501</td>\n      <td>0.179719</td>\n      <td>-0.251343</td>\n      <td>-0.246134</td>\n      <td>-0.185203</td>\n      <td>0.371252</td>\n      <td>0.209646</td>\n      <td>-0.019791</td>\n      <td>0.210080</td>\n      <td>0.084468</td>\n      <td>-0.055473</td>\n      <td>0.027057</td>\n      <td>-0.461324</td>\n      <td>0.029137</td>\n      <td>0.354453</td>\n      <td>0.143857</td>\n      <td>0.041494</td>\n      <td>0.263278</td>\n      <td>0.087657</td>\n      <td>0.282488</td>\n      <td>0.182306</td>\n      <td>0.018609</td>\n      <td>0.048870</td>\n      <td>0.034924</td>\n      <td>0.117454</td>\n      <td>0.048706</td>\n      <td>0.018731</td>\n      <td>-0.084871</td>\n      <td>-0.438666</td>\n      <td>-0.068246</td>\n      <td>0.267072</td>\n      <td>0.090743</td>\n      <td>0.533925</td>\n      <td>0.108869</td>\n      <td>-0.075757</td>\n      <td>-0.152011</td>\n      <td>0.196671</td>\n      <td>-0.027047</td>\n      <td>-0.203506</td>\n      <td>-0.059581</td>\n      <td>0.464162</td>\n      <td>-0.085020</td>\n      <td>0.263243</td>\n      <td>0.351539</td>\n      <td>-0.040017</td>\n      <td>-0.324080</td>\n      <td>0.172602</td>\n      <td>0.202878</td>\n      <td>-0.053347</td>\n      <td>0.405290</td>\n      <td>-0.090488</td>\n      <td>-0.154337</td>\n      <td>-0.334477</td>\n      <td>0.019493</td>\n      <td>0.443522</td>\n      <td>0.072783</td>\n      <td>0.020109</td>\n      <td>-0.160150</td>\n      <td>-0.340063</td>\n      <td>-0.162504</td>\n      <td>-0.049432</td>\n      <td>-0.215553</td>\n      <td>-0.109270</td>\n      <td>0.021254</td>\n      <td>-0.339571</td>\n      <td>0.028898</td>\n      <td>0.145408</td>\n      <td>-0.137000</td>\n      <td>-0.432440</td>\n      <td>0.056058</td>\n      <td>-0.514949</td>\n      <td>-0.074441</td>\n      <td>-0.031295</td>\n      <td>0.010599</td>\n      <td>0.570636</td>\n      <td>0.187400</td>\n      <td>-0.214750</td>\n      <td>-0.200343</td>\n      <td>-0.051809</td>\n      <td>0.349155</td>\n      <td>-0.062885</td>\n      <td>-0.122001</td>\n      <td>-0.177467</td>\n      <td>-0.062024</td>\n      <td>0.178993</td>\n      <td>0.101019</td>\n      <td>0.186958</td>\n      <td>0.157977</td>\n      <td>0.114238</td>\n      <td>0.074561</td>\n      <td>0.136963</td>\n      <td>0.093136</td>\n      <td>-0.036314</td>\n      <td>0.200261</td>\n      <td>-0.120719</td>\n      <td>-0.027865</td>\n      <td>-0.068774</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.035580</td>\n      <td>0.188858</td>\n      <td>-0.003260</td>\n      <td>-0.130486</td>\n      <td>0.160150</td>\n      <td>-0.174751</td>\n      <td>0.095568</td>\n      <td>-0.080505</td>\n      <td>0.254445</td>\n      <td>-0.096919</td>\n      <td>-0.005112</td>\n      <td>0.146347</td>\n      <td>-0.020105</td>\n      <td>-0.144764</td>\n      <td>0.070111</td>\n      <td>-0.461313</td>\n      <td>0.020317</td>\n      <td>0.153474</td>\n      <td>0.449021</td>\n      <td>0.240020</td>\n      <td>-0.029829</td>\n      <td>-0.746964</td>\n      <td>-0.110814</td>\n      <td>-0.152175</td>\n      <td>0.118142</td>\n      <td>0.026853</td>\n      <td>-0.047350</td>\n      <td>-0.153123</td>\n      <td>-0.079949</td>\n      <td>0.071040</td>\n      <td>0.236494</td>\n      <td>0.258795</td>\n      <td>-0.412821</td>\n      <td>-0.089466</td>\n      <td>0.274340</td>\n      <td>-0.049335</td>\n      <td>0.163575</td>\n      <td>0.108345</td>\n      <td>0.280589</td>\n      <td>0.041556</td>\n      <td>0.083502</td>\n      <td>-0.447287</td>\n      <td>0.161984</td>\n      <td>-0.230275</td>\n      <td>0.050764</td>\n      <td>-0.176455</td>\n      <td>0.295260</td>\n      <td>-0.071810</td>\n      <td>0.204033</td>\n      <td>0.194393</td>\n      <td>0.072104</td>\n      <td>0.069230</td>\n      <td>0.057365</td>\n      <td>0.250115</td>\n      <td>-0.226674</td>\n      <td>-0.409882</td>\n      <td>-0.083209</td>\n      <td>0.409443</td>\n      <td>0.586730</td>\n      <td>0.294902</td>\n      <td>-0.009036</td>\n      <td>-0.066134</td>\n      <td>0.378873</td>\n      <td>0.139024</td>\n      <td>0.616953</td>\n      <td>0.147327</td>\n      <td>0.152984</td>\n      <td>-0.220352</td>\n      <td>-0.060052</td>\n      <td>0.294716</td>\n      <td>-0.353490</td>\n      <td>-0.347696</td>\n      <td>0.019526</td>\n      <td>0.150951</td>\n      <td>-0.523688</td>\n      <td>-0.021887</td>\n      <td>0.151107</td>\n      <td>-0.031520</td>\n      <td>-0.080580</td>\n      <td>-0.226491</td>\n      <td>0.072965</td>\n      <td>0.024485</td>\n      <td>0.090849</td>\n      <td>0.061660</td>\n      <td>0.160308</td>\n      <td>-0.000249</td>\n      <td>0.145427</td>\n      <td>0.036488</td>\n      <td>0.143511</td>\n      <td>0.066186</td>\n      <td>0.185096</td>\n      <td>0.064714</td>\n      <td>0.043788</td>\n      <td>-0.253447</td>\n      <td>-0.085037</td>\n      <td>0.131762</td>\n      <td>-0.241207</td>\n      <td>-0.214116</td>\n      <td>-0.262134</td>\n      <td>0.428314</td>\n      <td>-0.205756</td>\n      <td>0.020433</td>\n      <td>0.053164</td>\n      <td>-0.046708</td>\n      <td>0.336227</td>\n      <td>0.250764</td>\n      <td>0.184627</td>\n      <td>-0.307911</td>\n      <td>-0.054481</td>\n      <td>0.251481</td>\n      <td>0.152357</td>\n      <td>-0.139081</td>\n      <td>-0.123923</td>\n      <td>-0.140809</td>\n      <td>-0.395432</td>\n      <td>-0.232222</td>\n      <td>-0.039668</td>\n      <td>0.076020</td>\n      <td>0.047020</td>\n      <td>-0.178981</td>\n      <td>0.452181</td>\n      <td>0.195260</td>\n      <td>-0.642997</td>\n      <td>-0.067788</td>\n      <td>-0.035879</td>\n      <td>0.492625</td>\n      <td>0.137494</td>\n      <td>-0.251071</td>\n      <td>0.506569</td>\n      <td>0.142970</td>\n      <td>0.057660</td>\n      <td>-0.130678</td>\n      <td>0.112102</td>\n      <td>0.009028</td>\n      <td>0.369862</td>\n      <td>0.354283</td>\n      <td>0.145550</td>\n      <td>-0.029876</td>\n      <td>0.331898</td>\n      <td>0.214437</td>\n      <td>-0.045436</td>\n      <td>0.242849</td>\n      <td>0.076237</td>\n      <td>0.150197</td>\n      <td>0.122341</td>\n      <td>0.043499</td>\n      <td>-0.207265</td>\n      <td>0.181092</td>\n      <td>0.297803</td>\n      <td>0.302810</td>\n      <td>-0.187244</td>\n      <td>0.270493</td>\n      <td>-0.115787</td>\n      <td>-0.157989</td>\n      <td>0.224122</td>\n      <td>0.015160</td>\n      <td>0.036422</td>\n      <td>-0.014339</td>\n      <td>-0.100939</td>\n      <td>-0.487877</td>\n      <td>-0.207670</td>\n      <td>-0.023456</td>\n      <td>0.131760</td>\n      <td>0.445729</td>\n      <td>0.140985</td>\n      <td>-0.129708</td>\n      <td>-0.006917</td>\n      <td>0.203316</td>\n      <td>0.214676</td>\n      <td>-0.414249</td>\n      <td>0.181311</td>\n      <td>0.535992</td>\n      <td>-0.058810</td>\n      <td>-0.194038</td>\n      <td>-0.236499</td>\n      <td>0.344177</td>\n      <td>-0.082826</td>\n      <td>-0.009086</td>\n      <td>0.273671</td>\n      <td>0.361212</td>\n      <td>0.255325</td>\n      <td>-0.137906</td>\n      <td>0.186877</td>\n      <td>-0.354912</td>\n      <td>0.346547</td>\n      <td>0.348800</td>\n      <td>-0.881826</td>\n      <td>0.206155</td>\n      <td>0.178205</td>\n      <td>-0.443061</td>\n      <td>-0.465032</td>\n      <td>-0.269095</td>\n      <td>0.219621</td>\n      <td>0.228248</td>\n      <td>0.381883</td>\n      <td>0.068821</td>\n      <td>-0.174993</td>\n      <td>-0.000809</td>\n      <td>-0.238583</td>\n      <td>-0.228395</td>\n      <td>-0.013334</td>\n      <td>-0.045344</td>\n      <td>-0.015050</td>\n      <td>-0.040867</td>\n      <td>-0.057101</td>\n      <td>0.171305</td>\n      <td>0.090578</td>\n      <td>0.036180</td>\n      <td>-0.059382</td>\n      <td>-0.220203</td>\n      <td>0.202565</td>\n      <td>-0.089261</td>\n      <td>0.183520</td>\n      <td>-0.137313</td>\n      <td>0.092158</td>\n      <td>-3.610868</td>\n      <td>-0.231617</td>\n      <td>-0.027206</td>\n      <td>-0.038752</td>\n      <td>0.094744</td>\n      <td>-0.233482</td>\n      <td>0.177227</td>\n      <td>-0.005728</td>\n      <td>-0.142980</td>\n      <td>0.283989</td>\n      <td>-0.158183</td>\n      <td>0.286960</td>\n      <td>0.194096</td>\n      <td>-0.282940</td>\n      <td>0.111715</td>\n      <td>0.509651</td>\n      <td>0.349996</td>\n      <td>-0.174520</td>\n      <td>-0.365142</td>\n      <td>0.111448</td>\n      <td>0.017188</td>\n      <td>-0.256177</td>\n      <td>0.171137</td>\n      <td>-0.208715</td>\n      <td>-0.213327</td>\n      <td>0.279918</td>\n      <td>0.042586</td>\n      <td>0.564589</td>\n      <td>0.143606</td>\n      <td>0.077246</td>\n      <td>0.032110</td>\n      <td>0.226598</td>\n      <td>0.521446</td>\n      <td>0.112921</td>\n      <td>0.444375</td>\n      <td>0.311787</td>\n      <td>-0.054853</td>\n      <td>0.053673</td>\n      <td>0.390642</td>\n      <td>0.550242</td>\n      <td>0.317521</td>\n      <td>0.223242</td>\n      <td>0.245071</td>\n      <td>0.180030</td>\n      <td>-0.008855</td>\n      <td>0.081058</td>\n      <td>0.367180</td>\n      <td>0.080807</td>\n      <td>0.082388</td>\n      <td>0.198337</td>\n      <td>0.020685</td>\n      <td>0.313999</td>\n      <td>-0.066791</td>\n      <td>0.049743</td>\n      <td>0.031478</td>\n      <td>0.226368</td>\n      <td>-0.217628</td>\n      <td>-0.007321</td>\n      <td>-0.399018</td>\n      <td>0.453673</td>\n      <td>0.266275</td>\n      <td>-0.347746</td>\n      <td>0.373964</td>\n      <td>-0.146171</td>\n      <td>-0.203050</td>\n      <td>0.733007</td>\n      <td>-0.283803</td>\n      <td>-0.012803</td>\n      <td>-0.071472</td>\n      <td>0.137610</td>\n      <td>-0.062926</td>\n      <td>-0.041954</td>\n      <td>0.443351</td>\n      <td>-0.242382</td>\n      <td>0.575240</td>\n      <td>0.311491</td>\n      <td>-0.151576</td>\n      <td>-0.489626</td>\n      <td>0.052303</td>\n      <td>-0.212201</td>\n      <td>0.344724</td>\n      <td>-0.020761</td>\n      <td>0.020689</td>\n      <td>0.072646</td>\n      <td>-0.221001</td>\n      <td>0.239724</td>\n      <td>0.219805</td>\n      <td>-0.254889</td>\n      <td>0.050592</td>\n      <td>-0.101746</td>\n      <td>0.253341</td>\n      <td>0.045858</td>\n      <td>0.134396</td>\n      <td>-0.133380</td>\n      <td>-0.457550</td>\n      <td>-0.120918</td>\n      <td>-0.186633</td>\n      <td>0.683703</td>\n      <td>0.010012</td>\n      <td>0.216744</td>\n      <td>0.246107</td>\n      <td>0.141247</td>\n      <td>0.027905</td>\n      <td>0.031845</td>\n      <td>-0.282164</td>\n      <td>0.259298</td>\n      <td>0.131470</td>\n      <td>0.107494</td>\n      <td>-0.260831</td>\n      <td>-0.060890</td>\n      <td>-0.054568</td>\n      <td>0.088920</td>\n      <td>0.531682</td>\n      <td>-0.120472</td>\n      <td>0.120701</td>\n      <td>0.159719</td>\n      <td>0.001423</td>\n      <td>0.018898</td>\n      <td>0.067823</td>\n      <td>-0.213029</td>\n      <td>0.186586</td>\n      <td>0.154574</td>\n      <td>-0.103128</td>\n      <td>-0.341050</td>\n      <td>-0.078466</td>\n      <td>0.346418</td>\n      <td>0.189367</td>\n      <td>0.200054</td>\n      <td>-0.882663</td>\n      <td>0.150782</td>\n      <td>0.204444</td>\n      <td>-0.301041</td>\n      <td>0.509706</td>\n      <td>0.192786</td>\n      <td>-0.238708</td>\n      <td>0.273323</td>\n      <td>-0.439996</td>\n      <td>0.365666</td>\n      <td>-0.015508</td>\n      <td>0.151713</td>\n      <td>0.622219</td>\n      <td>0.017667</td>\n      <td>0.071008</td>\n      <td>-0.209177</td>\n      <td>0.249654</td>\n      <td>-0.633336</td>\n      <td>0.072947</td>\n      <td>0.141322</td>\n      <td>0.333345</td>\n      <td>-0.165535</td>\n      <td>0.035011</td>\n      <td>0.097019</td>\n      <td>-0.043437</td>\n      <td>0.211305</td>\n      <td>-0.400283</td>\n      <td>-0.257370</td>\n      <td>0.095195</td>\n      <td>-0.101903</td>\n      <td>0.287008</td>\n      <td>0.087937</td>\n      <td>-0.028010</td>\n      <td>-0.725253</td>\n      <td>-1.573878</td>\n      <td>0.223328</td>\n      <td>0.463709</td>\n      <td>0.323709</td>\n      <td>0.137103</td>\n      <td>0.198513</td>\n      <td>-0.173492</td>\n      <td>-0.067455</td>\n      <td>0.183557</td>\n      <td>0.096656</td>\n      <td>0.082774</td>\n      <td>-0.074274</td>\n      <td>0.505964</td>\n      <td>-0.171689</td>\n      <td>0.059159</td>\n      <td>-0.064126</td>\n      <td>-0.191974</td>\n      <td>0.280933</td>\n      <td>-0.286824</td>\n      <td>0.055742</td>\n      <td>0.164370</td>\n      <td>0.233401</td>\n      <td>0.110808</td>\n      <td>-0.109585</td>\n      <td>0.057272</td>\n      <td>0.326034</td>\n      <td>-0.300844</td>\n      <td>0.570521</td>\n      <td>0.077208</td>\n      <td>-0.011745</td>\n      <td>0.308210</td>\n      <td>0.215719</td>\n      <td>-0.132078</td>\n      <td>0.108649</td>\n      <td>-0.037369</td>\n      <td>-0.193766</td>\n      <td>0.028883</td>\n      <td>0.018203</td>\n      <td>0.161684</td>\n      <td>0.160041</td>\n      <td>0.021596</td>\n      <td>0.327838</td>\n      <td>0.142262</td>\n      <td>0.234510</td>\n      <td>-0.289800</td>\n      <td>0.097334</td>\n      <td>0.142094</td>\n      <td>-0.135536</td>\n      <td>-0.053005</td>\n      <td>0.185052</td>\n      <td>-0.084795</td>\n      <td>0.112072</td>\n      <td>-0.115877</td>\n      <td>-0.070810</td>\n      <td>-0.003475</td>\n      <td>0.091526</td>\n      <td>0.089123</td>\n      <td>0.296380</td>\n      <td>-0.057218</td>\n      <td>0.282161</td>\n      <td>0.147720</td>\n      <td>0.231796</td>\n      <td>0.231546</td>\n      <td>0.109525</td>\n      <td>0.208487</td>\n      <td>0.143153</td>\n      <td>0.096822</td>\n      <td>-0.038644</td>\n      <td>4.589263</td>\n      <td>0.073934</td>\n      <td>0.274289</td>\n      <td>0.075938</td>\n      <td>0.610581</td>\n      <td>-0.103872</td>\n      <td>0.474489</td>\n      <td>0.106104</td>\n      <td>-0.147438</td>\n      <td>0.399179</td>\n      <td>-0.095195</td>\n      <td>0.144800</td>\n      <td>-0.423337</td>\n      <td>-0.159502</td>\n      <td>-0.319680</td>\n      <td>0.184307</td>\n      <td>0.261812</td>\n      <td>0.030999</td>\n      <td>0.249410</td>\n      <td>0.222796</td>\n      <td>-0.170890</td>\n      <td>0.424784</td>\n      <td>-0.217573</td>\n      <td>-0.104660</td>\n      <td>-0.101065</td>\n      <td>-0.100737</td>\n      <td>-0.017755</td>\n      <td>-0.058625</td>\n      <td>-0.294678</td>\n      <td>-0.031522</td>\n      <td>0.184311</td>\n      <td>-0.138472</td>\n      <td>-0.015762</td>\n      <td>-0.423487</td>\n      <td>0.316276</td>\n      <td>-0.105167</td>\n      <td>-0.209806</td>\n      <td>0.103387</td>\n      <td>0.440179</td>\n      <td>0.305836</td>\n      <td>-0.031058</td>\n      <td>0.429843</td>\n      <td>0.437899</td>\n      <td>0.098855</td>\n      <td>-0.366449</td>\n      <td>0.394186</td>\n      <td>0.430007</td>\n      <td>0.444887</td>\n      <td>-0.669890</td>\n      <td>-0.163727</td>\n      <td>-0.139011</td>\n      <td>0.039955</td>\n      <td>-0.354806</td>\n      <td>0.004578</td>\n      <td>0.112291</td>\n      <td>-0.081875</td>\n      <td>-0.075048</td>\n      <td>0.251497</td>\n      <td>0.137177</td>\n      <td>0.042746</td>\n      <td>0.233811</td>\n      <td>0.112302</td>\n      <td>0.166262</td>\n      <td>-0.189081</td>\n      <td>-0.584402</td>\n      <td>-0.099310</td>\n      <td>-0.153482</td>\n      <td>-0.085784</td>\n      <td>0.657140</td>\n      <td>0.058212</td>\n      <td>-0.069985</td>\n      <td>0.172562</td>\n      <td>-0.122488</td>\n      <td>0.499766</td>\n      <td>0.023834</td>\n      <td>-0.063930</td>\n      <td>-0.054424</td>\n      <td>-0.394738</td>\n      <td>0.023029</td>\n      <td>-0.566956</td>\n      <td>0.322194</td>\n      <td>-0.174802</td>\n      <td>0.359565</td>\n      <td>0.191352</td>\n      <td>0.214928</td>\n      <td>0.081044</td>\n      <td>0.014249</td>\n      <td>-0.248099</td>\n      <td>1.306929</td>\n      <td>0.010150</td>\n      <td>-0.154674</td>\n      <td>-0.049906</td>\n      <td>0.441014</td>\n      <td>0.218925</td>\n      <td>-0.153914</td>\n      <td>0.488047</td>\n      <td>-0.005701</td>\n      <td>0.032783</td>\n      <td>0.438099</td>\n      <td>-0.177114</td>\n      <td>0.264958</td>\n      <td>-0.012098</td>\n      <td>0.172682</td>\n      <td>0.080806</td>\n      <td>0.314867</td>\n      <td>0.015568</td>\n      <td>0.052388</td>\n      <td>-0.425482</td>\n      <td>-0.033898</td>\n      <td>0.044292</td>\n      <td>-0.423019</td>\n      <td>0.104922</td>\n      <td>0.387087</td>\n      <td>-0.313701</td>\n      <td>-0.053402</td>\n      <td>-0.036802</td>\n      <td>0.057828</td>\n      <td>0.150301</td>\n      <td>0.020526</td>\n      <td>-0.413149</td>\n      <td>0.126196</td>\n      <td>0.307310</td>\n      <td>0.182301</td>\n      <td>-0.121925</td>\n      <td>0.189220</td>\n      <td>0.157032</td>\n      <td>0.136725</td>\n      <td>-0.461674</td>\n      <td>-0.020370</td>\n      <td>0.745425</td>\n      <td>0.150443</td>\n      <td>-0.224761</td>\n      <td>0.296795</td>\n      <td>0.013498</td>\n      <td>-0.309460</td>\n      <td>0.118734</td>\n      <td>-0.330776</td>\n      <td>0.074880</td>\n      <td>0.313029</td>\n      <td>-0.113355</td>\n      <td>-0.233489</td>\n      <td>0.215368</td>\n      <td>0.074380</td>\n      <td>0.071018</td>\n      <td>-0.170584</td>\n      <td>0.302396</td>\n      <td>0.560400</td>\n      <td>0.503578</td>\n      <td>0.258928</td>\n      <td>0.524561</td>\n      <td>0.112997</td>\n      <td>0.078601</td>\n      <td>0.145657</td>\n      <td>0.240603</td>\n      <td>-0.030985</td>\n      <td>-0.358218</td>\n      <td>-0.130811</td>\n      <td>-0.310706</td>\n      <td>0.278885</td>\n      <td>0.160765</td>\n      <td>-0.041355</td>\n      <td>-0.080748</td>\n      <td>-0.089060</td>\n      <td>-0.202069</td>\n      <td>0.226148</td>\n      <td>-0.072207</td>\n      <td>-0.257342</td>\n      <td>-0.207745</td>\n      <td>0.418329</td>\n      <td>0.034823</td>\n      <td>0.071462</td>\n      <td>0.119447</td>\n      <td>-0.059441</td>\n      <td>-0.048216</td>\n      <td>-0.308524</td>\n      <td>0.334301</td>\n      <td>-0.368105</td>\n      <td>-0.035209</td>\n      <td>0.066770</td>\n      <td>-0.072121</td>\n      <td>0.173889</td>\n      <td>0.433051</td>\n      <td>-0.193934</td>\n      <td>0.040075</td>\n      <td>-0.137883</td>\n      <td>0.203224</td>\n      <td>0.358552</td>\n      <td>-0.277534</td>\n      <td>0.167971</td>\n      <td>0.185265</td>\n      <td>-0.316817</td>\n      <td>0.087062</td>\n      <td>-0.064222</td>\n      <td>0.088192</td>\n      <td>0.069077</td>\n      <td>0.215891</td>\n      <td>0.078315</td>\n      <td>0.213914</td>\n      <td>0.282584</td>\n      <td>0.018605</td>\n      <td>-0.441069</td>\n      <td>-0.051702</td>\n      <td>0.212115</td>\n      <td>0.166402</td>\n      <td>-0.094944</td>\n      <td>0.459076</td>\n      <td>0.270304</td>\n      <td>-0.338150</td>\n      <td>-0.292271</td>\n      <td>-0.424333</td>\n      <td>0.248754</td>\n      <td>0.135993</td>\n      <td>-0.123694</td>\n      <td>-0.162628</td>\n      <td>-0.026158</td>\n      <td>0.545900</td>\n      <td>0.068664</td>\n      <td>0.073108</td>\n      <td>-0.033413</td>\n      <td>0.153956</td>\n      <td>0.400309</td>\n      <td>0.014063</td>\n      <td>-0.144829</td>\n      <td>0.423704</td>\n      <td>-0.310697</td>\n      <td>-0.565725</td>\n      <td>0.243562</td>\n      <td>0.038082</td>\n      <td>-0.259731</td>\n      <td>-0.160869</td>\n      <td>-0.130605</td>\n      <td>0.002855</td>\n      <td>0.132899</td>\n      <td>0.355293</td>\n      <td>0.119231</td>\n      <td>0.083701</td>\n      <td>-0.061923</td>\n      <td>-0.121270</td>\n      <td>-0.487989</td>\n      <td>0.239034</td>\n      <td>0.340356</td>\n      <td>0.198060</td>\n      <td>0.016782</td>\n      <td>0.073750</td>\n      <td>-0.219563</td>\n      <td>0.073500</td>\n      <td>0.137802</td>\n      <td>-0.136318</td>\n      <td>-0.084467</td>\n      <td>0.098981</td>\n      <td>-0.038492</td>\n      <td>-0.054478</td>\n      <td>-0.034997</td>\n      <td>0.102692</td>\n      <td>-0.440833</td>\n      <td>-0.030499</td>\n      <td>0.294646</td>\n      <td>-0.061308</td>\n      <td>0.546649</td>\n      <td>0.045260</td>\n      <td>0.088310</td>\n      <td>-0.202147</td>\n      <td>0.303553</td>\n      <td>-0.151507</td>\n      <td>-0.386074</td>\n      <td>-0.163417</td>\n      <td>0.432547</td>\n      <td>0.067472</td>\n      <td>0.188812</td>\n      <td>0.178715</td>\n      <td>-0.062067</td>\n      <td>-0.089850</td>\n      <td>0.120208</td>\n      <td>0.227616</td>\n      <td>0.109611</td>\n      <td>0.561448</td>\n      <td>-0.397279</td>\n      <td>-0.239775</td>\n      <td>-0.376003</td>\n      <td>-0.201369</td>\n      <td>0.439500</td>\n      <td>0.114194</td>\n      <td>0.225814</td>\n      <td>0.041255</td>\n      <td>-0.251847</td>\n      <td>-0.066098</td>\n      <td>0.004132</td>\n      <td>-0.292236</td>\n      <td>-0.147804</td>\n      <td>-0.104149</td>\n      <td>-0.274381</td>\n      <td>0.220314</td>\n      <td>0.156473</td>\n      <td>0.093490</td>\n      <td>-0.493545</td>\n      <td>0.354638</td>\n      <td>-0.295340</td>\n      <td>-0.016712</td>\n      <td>-0.105776</td>\n      <td>-0.135300</td>\n      <td>0.557548</td>\n      <td>0.082171</td>\n      <td>-0.191544</td>\n      <td>-0.064381</td>\n      <td>-0.145588</td>\n      <td>0.318829</td>\n      <td>0.082383</td>\n      <td>0.174325</td>\n      <td>-0.002391</td>\n      <td>-0.214820</td>\n      <td>0.254752</td>\n      <td>0.116520</td>\n      <td>0.292421</td>\n      <td>0.104102</td>\n      <td>0.227521</td>\n      <td>0.002342</td>\n      <td>-0.019536</td>\n      <td>-0.041649</td>\n      <td>-0.000390</td>\n      <td>0.102694</td>\n      <td>-0.043032</td>\n      <td>0.091723</td>\n      <td>-0.132449</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.224686</td>\n      <td>0.139598</td>\n      <td>0.108120</td>\n      <td>-0.051431</td>\n      <td>0.124639</td>\n      <td>-0.146919</td>\n      <td>0.009397</td>\n      <td>-0.076450</td>\n      <td>-0.057992</td>\n      <td>-0.182568</td>\n      <td>-0.011725</td>\n      <td>0.090582</td>\n      <td>0.098747</td>\n      <td>-0.098377</td>\n      <td>0.027371</td>\n      <td>-0.278408</td>\n      <td>0.022246</td>\n      <td>0.113622</td>\n      <td>0.423185</td>\n      <td>0.259460</td>\n      <td>0.067126</td>\n      <td>-0.949673</td>\n      <td>-0.220340</td>\n      <td>-0.087974</td>\n      <td>0.089272</td>\n      <td>-0.009048</td>\n      <td>-0.081135</td>\n      <td>-0.106222</td>\n      <td>-0.156119</td>\n      <td>0.062679</td>\n      <td>0.101260</td>\n      <td>0.155487</td>\n      <td>-0.525824</td>\n      <td>-0.162470</td>\n      <td>0.279062</td>\n      <td>-0.055759</td>\n      <td>-0.005481</td>\n      <td>0.259231</td>\n      <td>0.285417</td>\n      <td>0.037650</td>\n      <td>0.155340</td>\n      <td>-0.309472</td>\n      <td>0.166988</td>\n      <td>-0.343516</td>\n      <td>0.210384</td>\n      <td>-0.090235</td>\n      <td>0.328635</td>\n      <td>-0.119247</td>\n      <td>0.300086</td>\n      <td>0.122281</td>\n      <td>0.046533</td>\n      <td>0.224036</td>\n      <td>0.038425</td>\n      <td>0.228496</td>\n      <td>-0.169686</td>\n      <td>-0.539142</td>\n      <td>-0.125522</td>\n      <td>0.417447</td>\n      <td>0.488795</td>\n      <td>0.443821</td>\n      <td>-0.141856</td>\n      <td>-0.028551</td>\n      <td>0.301847</td>\n      <td>0.153022</td>\n      <td>0.609838</td>\n      <td>0.202789</td>\n      <td>0.075323</td>\n      <td>-0.236674</td>\n      <td>-0.045330</td>\n      <td>0.251661</td>\n      <td>-0.302042</td>\n      <td>-0.467181</td>\n      <td>0.008336</td>\n      <td>0.079019</td>\n      <td>-0.386661</td>\n      <td>-0.060748</td>\n      <td>0.076541</td>\n      <td>0.186478</td>\n      <td>-0.195288</td>\n      <td>-0.117982</td>\n      <td>-0.010120</td>\n      <td>-0.047565</td>\n      <td>-0.084024</td>\n      <td>0.162113</td>\n      <td>0.060886</td>\n      <td>-0.038068</td>\n      <td>-0.023373</td>\n      <td>0.226406</td>\n      <td>0.203735</td>\n      <td>-0.063867</td>\n      <td>0.331452</td>\n      <td>0.122181</td>\n      <td>0.052440</td>\n      <td>-0.275033</td>\n      <td>-0.151542</td>\n      <td>0.110324</td>\n      <td>-0.057964</td>\n      <td>-0.063879</td>\n      <td>-0.468583</td>\n      <td>0.296925</td>\n      <td>-0.317584</td>\n      <td>0.002456</td>\n      <td>0.131400</td>\n      <td>-0.049492</td>\n      <td>0.351513</td>\n      <td>0.262963</td>\n      <td>0.050179</td>\n      <td>-0.416606</td>\n      <td>-0.108688</td>\n      <td>0.279020</td>\n      <td>0.187148</td>\n      <td>-0.094144</td>\n      <td>-0.105822</td>\n      <td>-0.062931</td>\n      <td>-0.365287</td>\n      <td>-0.208583</td>\n      <td>0.069718</td>\n      <td>0.035326</td>\n      <td>-0.052281</td>\n      <td>-0.240490</td>\n      <td>0.362602</td>\n      <td>0.225711</td>\n      <td>-0.684493</td>\n      <td>-0.119297</td>\n      <td>-0.027820</td>\n      <td>0.507100</td>\n      <td>0.045060</td>\n      <td>-0.208374</td>\n      <td>0.610801</td>\n      <td>0.196478</td>\n      <td>0.139748</td>\n      <td>-0.007963</td>\n      <td>0.064250</td>\n      <td>0.106813</td>\n      <td>0.267366</td>\n      <td>0.302590</td>\n      <td>0.086619</td>\n      <td>-0.040683</td>\n      <td>0.306676</td>\n      <td>0.067943</td>\n      <td>0.079773</td>\n      <td>0.320232</td>\n      <td>0.188125</td>\n      <td>0.047553</td>\n      <td>0.172382</td>\n      <td>0.108560</td>\n      <td>-0.228358</td>\n      <td>0.235052</td>\n      <td>0.199982</td>\n      <td>0.268948</td>\n      <td>0.056929</td>\n      <td>0.404265</td>\n      <td>-0.054521</td>\n      <td>-0.069474</td>\n      <td>0.081982</td>\n      <td>0.073801</td>\n      <td>0.101196</td>\n      <td>0.104848</td>\n      <td>0.068738</td>\n      <td>-0.483133</td>\n      <td>-0.222031</td>\n      <td>-0.002438</td>\n      <td>0.251032</td>\n      <td>0.343779</td>\n      <td>-0.048711</td>\n      <td>-0.163192</td>\n      <td>0.140117</td>\n      <td>0.131374</td>\n      <td>0.256049</td>\n      <td>-0.452066</td>\n      <td>0.198757</td>\n      <td>0.328482</td>\n      <td>-0.205885</td>\n      <td>-0.256584</td>\n      <td>-0.380374</td>\n      <td>0.371758</td>\n      <td>0.011327</td>\n      <td>0.000135</td>\n      <td>0.346537</td>\n      <td>0.306987</td>\n      <td>0.270705</td>\n      <td>-0.111971</td>\n      <td>0.255775</td>\n      <td>-0.385067</td>\n      <td>0.143123</td>\n      <td>0.374777</td>\n      <td>-0.748106</td>\n      <td>0.183797</td>\n      <td>0.046408</td>\n      <td>-0.456501</td>\n      <td>-0.473792</td>\n      <td>-0.125127</td>\n      <td>0.210746</td>\n      <td>0.149871</td>\n      <td>0.493633</td>\n      <td>0.274586</td>\n      <td>-0.242301</td>\n      <td>0.050019</td>\n      <td>-0.104351</td>\n      <td>-0.050414</td>\n      <td>0.144237</td>\n      <td>0.076188</td>\n      <td>-0.032773</td>\n      <td>0.019644</td>\n      <td>-0.118328</td>\n      <td>0.190937</td>\n      <td>0.143970</td>\n      <td>0.121806</td>\n      <td>-0.111245</td>\n      <td>-0.218350</td>\n      <td>0.170546</td>\n      <td>-0.060848</td>\n      <td>0.020044</td>\n      <td>-0.075605</td>\n      <td>-0.149593</td>\n      <td>-3.573384</td>\n      <td>-0.285961</td>\n      <td>0.031387</td>\n      <td>0.032163</td>\n      <td>0.112445</td>\n      <td>-0.206838</td>\n      <td>0.107499</td>\n      <td>0.175006</td>\n      <td>-0.085400</td>\n      <td>0.149484</td>\n      <td>-0.165676</td>\n      <td>0.125858</td>\n      <td>0.128427</td>\n      <td>-0.408282</td>\n      <td>0.137211</td>\n      <td>0.373565</td>\n      <td>0.423122</td>\n      <td>-0.208116</td>\n      <td>-0.236664</td>\n      <td>0.169469</td>\n      <td>0.065424</td>\n      <td>-0.161202</td>\n      <td>0.307285</td>\n      <td>-0.180178</td>\n      <td>-0.315024</td>\n      <td>0.322721</td>\n      <td>0.054105</td>\n      <td>0.432655</td>\n      <td>0.104102</td>\n      <td>0.077890</td>\n      <td>0.006997</td>\n      <td>0.329715</td>\n      <td>0.671053</td>\n      <td>0.137491</td>\n      <td>0.182870</td>\n      <td>0.335589</td>\n      <td>-0.110827</td>\n      <td>0.049733</td>\n      <td>0.169726</td>\n      <td>0.445681</td>\n      <td>0.182337</td>\n      <td>0.140015</td>\n      <td>0.091656</td>\n      <td>0.290521</td>\n      <td>-0.000366</td>\n      <td>0.137558</td>\n      <td>0.255673</td>\n      <td>-0.181095</td>\n      <td>0.142776</td>\n      <td>0.080911</td>\n      <td>-0.082483</td>\n      <td>0.354889</td>\n      <td>0.059673</td>\n      <td>0.066205</td>\n      <td>-0.047651</td>\n      <td>0.288218</td>\n      <td>-0.254087</td>\n      <td>0.065648</td>\n      <td>-0.303014</td>\n      <td>0.310328</td>\n      <td>0.340694</td>\n      <td>-0.326767</td>\n      <td>0.347965</td>\n      <td>0.023820</td>\n      <td>-0.195283</td>\n      <td>0.650697</td>\n      <td>-0.314169</td>\n      <td>0.036400</td>\n      <td>-0.011542</td>\n      <td>0.250829</td>\n      <td>-0.080236</td>\n      <td>-0.069415</td>\n      <td>0.360355</td>\n      <td>-0.199484</td>\n      <td>0.565230</td>\n      <td>0.319521</td>\n      <td>-0.148349</td>\n      <td>-0.412954</td>\n      <td>0.206655</td>\n      <td>-0.244139</td>\n      <td>0.238886</td>\n      <td>0.049362</td>\n      <td>0.007377</td>\n      <td>-0.035468</td>\n      <td>-0.262011</td>\n      <td>0.412155</td>\n      <td>0.186450</td>\n      <td>-0.365440</td>\n      <td>-0.025972</td>\n      <td>-0.134480</td>\n      <td>0.121959</td>\n      <td>0.036242</td>\n      <td>0.247163</td>\n      <td>-0.003246</td>\n      <td>-0.265416</td>\n      <td>-0.318600</td>\n      <td>-0.043874</td>\n      <td>0.766777</td>\n      <td>0.016044</td>\n      <td>0.027015</td>\n      <td>0.390844</td>\n      <td>-0.054624</td>\n      <td>0.145646</td>\n      <td>-0.028515</td>\n      <td>0.047920</td>\n      <td>0.219566</td>\n      <td>0.028649</td>\n      <td>0.115967</td>\n      <td>-0.313340</td>\n      <td>-0.060640</td>\n      <td>-0.053464</td>\n      <td>0.084669</td>\n      <td>0.566006</td>\n      <td>-0.121154</td>\n      <td>-0.022832</td>\n      <td>-0.060789</td>\n      <td>-0.028456</td>\n      <td>0.117863</td>\n      <td>-0.074391</td>\n      <td>-0.267706</td>\n      <td>0.117541</td>\n      <td>0.155247</td>\n      <td>0.035813</td>\n      <td>-0.261000</td>\n      <td>-0.154158</td>\n      <td>0.226752</td>\n      <td>0.350654</td>\n      <td>0.185459</td>\n      <td>-1.078435</td>\n      <td>0.489120</td>\n      <td>0.108247</td>\n      <td>-0.266860</td>\n      <td>0.648395</td>\n      <td>0.287255</td>\n      <td>-0.117844</td>\n      <td>0.354066</td>\n      <td>-0.416070</td>\n      <td>0.345341</td>\n      <td>0.081981</td>\n      <td>0.127965</td>\n      <td>0.430397</td>\n      <td>-0.033595</td>\n      <td>0.146656</td>\n      <td>-0.145624</td>\n      <td>0.264575</td>\n      <td>-0.743011</td>\n      <td>0.066035</td>\n      <td>0.111831</td>\n      <td>0.342469</td>\n      <td>-0.121586</td>\n      <td>-0.025412</td>\n      <td>0.168367</td>\n      <td>-0.033360</td>\n      <td>0.066971</td>\n      <td>-0.284110</td>\n      <td>-0.227425</td>\n      <td>0.059063</td>\n      <td>-0.028301</td>\n      <td>0.208341</td>\n      <td>0.172644</td>\n      <td>-0.195995</td>\n      <td>-0.667109</td>\n      <td>-1.659434</td>\n      <td>0.234415</td>\n      <td>0.411392</td>\n      <td>0.557919</td>\n      <td>0.171800</td>\n      <td>0.130507</td>\n      <td>-0.081883</td>\n      <td>-0.085766</td>\n      <td>0.128735</td>\n      <td>0.082796</td>\n      <td>0.103305</td>\n      <td>0.057104</td>\n      <td>0.503316</td>\n      <td>-0.050694</td>\n      <td>0.051762</td>\n      <td>-0.043146</td>\n      <td>-0.236718</td>\n      <td>0.212253</td>\n      <td>-0.284349</td>\n      <td>0.225131</td>\n      <td>0.317554</td>\n      <td>0.233766</td>\n      <td>0.166256</td>\n      <td>-0.067030</td>\n      <td>0.125299</td>\n      <td>0.325762</td>\n      <td>-0.413970</td>\n      <td>0.624875</td>\n      <td>0.131539</td>\n      <td>0.093488</td>\n      <td>0.330448</td>\n      <td>0.166375</td>\n      <td>-0.142191</td>\n      <td>0.048861</td>\n      <td>-0.120444</td>\n      <td>-0.253740</td>\n      <td>0.072407</td>\n      <td>0.067012</td>\n      <td>0.207322</td>\n      <td>0.190528</td>\n      <td>0.064146</td>\n      <td>0.251848</td>\n      <td>0.099032</td>\n      <td>0.165080</td>\n      <td>-0.137715</td>\n      <td>-0.140599</td>\n      <td>0.209397</td>\n      <td>-0.117788</td>\n      <td>-0.106100</td>\n      <td>0.333269</td>\n      <td>-0.041248</td>\n      <td>0.127098</td>\n      <td>-0.093145</td>\n      <td>-0.167996</td>\n      <td>-0.006999</td>\n      <td>0.056609</td>\n      <td>0.141041</td>\n      <td>0.481331</td>\n      <td>-0.103233</td>\n      <td>0.327625</td>\n      <td>0.272374</td>\n      <td>0.196827</td>\n      <td>0.363769</td>\n      <td>0.037420</td>\n      <td>0.105824</td>\n      <td>0.148034</td>\n      <td>-0.010754</td>\n      <td>-0.015208</td>\n      <td>4.724798</td>\n      <td>0.138368</td>\n      <td>0.190422</td>\n      <td>0.003221</td>\n      <td>0.348576</td>\n      <td>-0.065356</td>\n      <td>0.328944</td>\n      <td>-0.051279</td>\n      <td>0.037169</td>\n      <td>0.442660</td>\n      <td>-0.079182</td>\n      <td>0.145942</td>\n      <td>-0.380691</td>\n      <td>-0.099134</td>\n      <td>-0.266771</td>\n      <td>0.314336</td>\n      <td>0.191153</td>\n      <td>-0.050169</td>\n      <td>0.161240</td>\n      <td>0.229753</td>\n      <td>-0.225471</td>\n      <td>0.489279</td>\n      <td>-0.081877</td>\n      <td>-0.146585</td>\n      <td>-0.173832</td>\n      <td>-0.183103</td>\n      <td>-0.082342</td>\n      <td>0.021002</td>\n      <td>-0.266867</td>\n      <td>0.138740</td>\n      <td>0.096133</td>\n      <td>-0.015366</td>\n      <td>-0.029622</td>\n      <td>-0.374682</td>\n      <td>0.414259</td>\n      <td>-0.090948</td>\n      <td>-0.127830</td>\n      <td>-0.053346</td>\n      <td>0.499038</td>\n      <td>0.171812</td>\n      <td>-0.047922</td>\n      <td>0.474412</td>\n      <td>0.269469</td>\n      <td>0.120755</td>\n      <td>-0.271437</td>\n      <td>0.359295</td>\n      <td>0.356456</td>\n      <td>0.413065</td>\n      <td>-0.531105</td>\n      <td>-0.067939</td>\n      <td>-0.269381</td>\n      <td>0.134331</td>\n      <td>-0.347513</td>\n      <td>0.075619</td>\n      <td>0.244737</td>\n      <td>-0.087617</td>\n      <td>-0.105489</td>\n      <td>0.162576</td>\n      <td>0.053325</td>\n      <td>0.107291</td>\n      <td>0.131072</td>\n      <td>-0.161796</td>\n      <td>0.182106</td>\n      <td>-0.027949</td>\n      <td>-0.365878</td>\n      <td>-0.106784</td>\n      <td>-0.101995</td>\n      <td>-0.096707</td>\n      <td>0.455633</td>\n      <td>0.016582</td>\n      <td>-0.106859</td>\n      <td>0.047870</td>\n      <td>-0.118545</td>\n      <td>0.522409</td>\n      <td>-0.014396</td>\n      <td>-0.184805</td>\n      <td>-0.137146</td>\n      <td>-0.316302</td>\n      <td>-0.093624</td>\n      <td>-0.666288</td>\n      <td>0.257855</td>\n      <td>-0.164358</td>\n      <td>0.212663</td>\n      <td>0.082727</td>\n      <td>0.181313</td>\n      <td>0.189990</td>\n      <td>-0.163090</td>\n      <td>-0.235879</td>\n      <td>1.271474</td>\n      <td>-0.066186</td>\n      <td>-0.128729</td>\n      <td>-0.005365</td>\n      <td>0.469518</td>\n      <td>0.195297</td>\n      <td>-0.148033</td>\n      <td>0.540526</td>\n      <td>-0.038647</td>\n      <td>0.053315</td>\n      <td>0.487796</td>\n      <td>-0.068527</td>\n      <td>0.077622</td>\n      <td>0.021009</td>\n      <td>0.236534</td>\n      <td>0.055601</td>\n      <td>0.285885</td>\n      <td>-0.160817</td>\n      <td>-0.049537</td>\n      <td>-0.503420</td>\n      <td>-0.093137</td>\n      <td>0.011491</td>\n      <td>-0.493284</td>\n      <td>0.207891</td>\n      <td>0.352552</td>\n      <td>-0.302255</td>\n      <td>-0.046455</td>\n      <td>-0.051519</td>\n      <td>-0.118091</td>\n      <td>0.251326</td>\n      <td>-0.070982</td>\n      <td>-0.426790</td>\n      <td>0.017510</td>\n      <td>0.127080</td>\n      <td>0.409236</td>\n      <td>-0.080476</td>\n      <td>0.285003</td>\n      <td>0.257981</td>\n      <td>0.041792</td>\n      <td>-0.331380</td>\n      <td>-0.024571</td>\n      <td>0.560636</td>\n      <td>0.156476</td>\n      <td>-0.317845</td>\n      <td>0.302103</td>\n      <td>0.048535</td>\n      <td>-0.362336</td>\n      <td>0.088438</td>\n      <td>-0.399602</td>\n      <td>0.187124</td>\n      <td>0.268476</td>\n      <td>-0.284721</td>\n      <td>-0.087856</td>\n      <td>0.150417</td>\n      <td>-0.117420</td>\n      <td>0.160845</td>\n      <td>-0.296682</td>\n      <td>0.273953</td>\n      <td>0.572014</td>\n      <td>0.568337</td>\n      <td>0.387098</td>\n      <td>0.439211</td>\n      <td>0.022400</td>\n      <td>0.082213</td>\n      <td>0.178233</td>\n      <td>0.297233</td>\n      <td>-0.095604</td>\n      <td>-0.361939</td>\n      <td>-0.065236</td>\n      <td>-0.194765</td>\n      <td>0.403799</td>\n      <td>0.063449</td>\n      <td>0.041826</td>\n      <td>-0.149846</td>\n      <td>-0.047050</td>\n      <td>0.010768</td>\n      <td>0.164774</td>\n      <td>-0.080460</td>\n      <td>-0.202783</td>\n      <td>-0.149669</td>\n      <td>0.379859</td>\n      <td>-0.184855</td>\n      <td>0.150104</td>\n      <td>0.207748</td>\n      <td>-0.278440</td>\n      <td>0.071606</td>\n      <td>-0.112504</td>\n      <td>0.321108</td>\n      <td>-0.361182</td>\n      <td>0.109440</td>\n      <td>0.031743</td>\n      <td>-0.189667</td>\n      <td>0.149004</td>\n      <td>0.292923</td>\n      <td>-0.003158</td>\n      <td>-0.080441</td>\n      <td>-0.127086</td>\n      <td>0.239413</td>\n      <td>0.244251</td>\n      <td>-0.357834</td>\n      <td>0.158927</td>\n      <td>0.299974</td>\n      <td>-0.271492</td>\n      <td>0.100035</td>\n      <td>-0.115619</td>\n      <td>0.154402</td>\n      <td>0.097091</td>\n      <td>0.079012</td>\n      <td>0.139044</td>\n      <td>0.174378</td>\n      <td>0.166514</td>\n      <td>-0.037346</td>\n      <td>-0.662391</td>\n      <td>-0.031776</td>\n      <td>0.058510</td>\n      <td>0.194450</td>\n      <td>-0.159812</td>\n      <td>0.448170</td>\n      <td>0.328815</td>\n      <td>-0.298211</td>\n      <td>-0.175982</td>\n      <td>-0.378731</td>\n      <td>0.157533</td>\n      <td>0.266792</td>\n      <td>-0.096635</td>\n      <td>-0.078272</td>\n      <td>-0.080815</td>\n      <td>0.670782</td>\n      <td>0.118917</td>\n      <td>-0.152896</td>\n      <td>0.005151</td>\n      <td>0.045711</td>\n      <td>0.304243</td>\n      <td>0.061555</td>\n      <td>-0.135009</td>\n      <td>0.315377</td>\n      <td>-0.285532</td>\n      <td>-0.581321</td>\n      <td>0.311918</td>\n      <td>0.115056</td>\n      <td>-0.187161</td>\n      <td>-0.242095</td>\n      <td>-0.140811</td>\n      <td>0.193934</td>\n      <td>0.059399</td>\n      <td>0.276174</td>\n      <td>0.140397</td>\n      <td>-0.001556</td>\n      <td>-0.006784</td>\n      <td>0.035666</td>\n      <td>-0.385703</td>\n      <td>0.174802</td>\n      <td>0.439088</td>\n      <td>0.195584</td>\n      <td>-0.048269</td>\n      <td>0.062359</td>\n      <td>-0.203821</td>\n      <td>0.059232</td>\n      <td>0.215975</td>\n      <td>-0.070030</td>\n      <td>0.029931</td>\n      <td>0.101400</td>\n      <td>-0.025347</td>\n      <td>-0.042545</td>\n      <td>0.017275</td>\n      <td>-0.082978</td>\n      <td>-0.488424</td>\n      <td>0.019588</td>\n      <td>0.249976</td>\n      <td>-0.036452</td>\n      <td>0.487762</td>\n      <td>0.067219</td>\n      <td>0.025660</td>\n      <td>-0.067392</td>\n      <td>0.196855</td>\n      <td>-0.096831</td>\n      <td>-0.394196</td>\n      <td>0.005774</td>\n      <td>0.631414</td>\n      <td>-0.001073</td>\n      <td>0.126442</td>\n      <td>0.246264</td>\n      <td>-0.059893</td>\n      <td>-0.157777</td>\n      <td>0.239462</td>\n      <td>0.335225</td>\n      <td>-0.025558</td>\n      <td>0.492118</td>\n      <td>-0.297662</td>\n      <td>-0.276724</td>\n      <td>-0.403481</td>\n      <td>-0.100482</td>\n      <td>0.420888</td>\n      <td>0.131512</td>\n      <td>0.193989</td>\n      <td>-0.014339</td>\n      <td>-0.278737</td>\n      <td>-0.025549</td>\n      <td>0.035356</td>\n      <td>-0.265374</td>\n      <td>-0.119169</td>\n      <td>-0.093115</td>\n      <td>-0.274699</td>\n      <td>0.151397</td>\n      <td>0.157824</td>\n      <td>0.024121</td>\n      <td>-0.268482</td>\n      <td>0.248347</td>\n      <td>-0.186100</td>\n      <td>-0.014369</td>\n      <td>-0.050867</td>\n      <td>-0.168469</td>\n      <td>0.689579</td>\n      <td>0.146725</td>\n      <td>-0.092388</td>\n      <td>-0.083103</td>\n      <td>0.019245</td>\n      <td>0.283348</td>\n      <td>0.074712</td>\n      <td>0.066673</td>\n      <td>0.012640</td>\n      <td>-0.098305</td>\n      <td>0.222184</td>\n      <td>0.195031</td>\n      <td>0.363898</td>\n      <td>0.077204</td>\n      <td>0.148679</td>\n      <td>-0.043145</td>\n      <td>0.072902</td>\n      <td>0.113459</td>\n      <td>-0.039636</td>\n      <td>0.154138</td>\n      <td>-0.079439</td>\n      <td>-0.028031</td>\n      <td>-0.112473</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5797</th>\n      <td>-0.183800</td>\n      <td>0.165602</td>\n      <td>0.171669</td>\n      <td>-0.052956</td>\n      <td>0.278973</td>\n      <td>-0.130962</td>\n      <td>0.071228</td>\n      <td>0.057740</td>\n      <td>-0.056535</td>\n      <td>-0.209858</td>\n      <td>0.040720</td>\n      <td>0.107985</td>\n      <td>0.234738</td>\n      <td>0.008502</td>\n      <td>-0.075491</td>\n      <td>-0.251127</td>\n      <td>0.147424</td>\n      <td>0.144499</td>\n      <td>0.494582</td>\n      <td>0.306974</td>\n      <td>0.094688</td>\n      <td>-0.972764</td>\n      <td>-0.137449</td>\n      <td>-0.160281</td>\n      <td>0.017787</td>\n      <td>-0.021394</td>\n      <td>0.012869</td>\n      <td>0.005371</td>\n      <td>-0.215681</td>\n      <td>-0.079269</td>\n      <td>0.015699</td>\n      <td>0.198888</td>\n      <td>-0.515358</td>\n      <td>-0.188952</td>\n      <td>0.234708</td>\n      <td>-0.086872</td>\n      <td>-0.031004</td>\n      <td>0.088533</td>\n      <td>0.352518</td>\n      <td>-0.055020</td>\n      <td>-0.059648</td>\n      <td>-0.189838</td>\n      <td>0.319680</td>\n      <td>-0.391937</td>\n      <td>0.079372</td>\n      <td>0.068417</td>\n      <td>0.282101</td>\n      <td>-0.132524</td>\n      <td>0.147947</td>\n      <td>0.140314</td>\n      <td>0.075820</td>\n      <td>0.354148</td>\n      <td>0.119526</td>\n      <td>-0.002260</td>\n      <td>-0.008709</td>\n      <td>-0.728890</td>\n      <td>-0.160195</td>\n      <td>0.466399</td>\n      <td>0.336880</td>\n      <td>0.368612</td>\n      <td>-0.049891</td>\n      <td>-0.103828</td>\n      <td>0.238289</td>\n      <td>0.116275</td>\n      <td>0.372342</td>\n      <td>0.127452</td>\n      <td>0.069002</td>\n      <td>-0.169457</td>\n      <td>-0.042329</td>\n      <td>0.142040</td>\n      <td>-0.321264</td>\n      <td>-0.439664</td>\n      <td>0.110230</td>\n      <td>0.197935</td>\n      <td>-0.527432</td>\n      <td>-0.010925</td>\n      <td>0.078714</td>\n      <td>0.263842</td>\n      <td>-0.025440</td>\n      <td>-0.137584</td>\n      <td>0.104055</td>\n      <td>-0.047856</td>\n      <td>-0.167662</td>\n      <td>0.254448</td>\n      <td>-0.006499</td>\n      <td>-0.099957</td>\n      <td>0.089659</td>\n      <td>0.167602</td>\n      <td>0.116169</td>\n      <td>-0.087352</td>\n      <td>0.224377</td>\n      <td>0.095085</td>\n      <td>0.087274</td>\n      <td>-0.347316</td>\n      <td>-0.051647</td>\n      <td>0.071334</td>\n      <td>-0.180330</td>\n      <td>-0.123110</td>\n      <td>-0.392315</td>\n      <td>0.456517</td>\n      <td>-0.141040</td>\n      <td>0.020599</td>\n      <td>0.202348</td>\n      <td>-0.063767</td>\n      <td>0.366351</td>\n      <td>0.235891</td>\n      <td>0.087692</td>\n      <td>-0.383498</td>\n      <td>-0.262828</td>\n      <td>0.248724</td>\n      <td>0.200279</td>\n      <td>-0.117193</td>\n      <td>-0.089936</td>\n      <td>0.022948</td>\n      <td>-0.347339</td>\n      <td>-0.240669</td>\n      <td>0.031732</td>\n      <td>0.087027</td>\n      <td>0.006320</td>\n      <td>-0.275179</td>\n      <td>0.182102</td>\n      <td>0.149824</td>\n      <td>-0.618584</td>\n      <td>-0.059428</td>\n      <td>-0.119899</td>\n      <td>0.478920</td>\n      <td>-0.058893</td>\n      <td>-0.239811</td>\n      <td>0.594525</td>\n      <td>0.208125</td>\n      <td>0.154506</td>\n      <td>0.160430</td>\n      <td>-0.054102</td>\n      <td>0.136897</td>\n      <td>0.248089</td>\n      <td>0.277288</td>\n      <td>0.019002</td>\n      <td>-0.057653</td>\n      <td>0.273099</td>\n      <td>-0.140730</td>\n      <td>-0.024730</td>\n      <td>0.156882</td>\n      <td>0.211476</td>\n      <td>0.106192</td>\n      <td>0.129335</td>\n      <td>0.175490</td>\n      <td>-0.256518</td>\n      <td>0.146562</td>\n      <td>0.224432</td>\n      <td>0.160117</td>\n      <td>0.076052</td>\n      <td>0.419558</td>\n      <td>-0.105563</td>\n      <td>-0.246532</td>\n      <td>-0.069292</td>\n      <td>0.229191</td>\n      <td>-0.067568</td>\n      <td>0.047140</td>\n      <td>-0.005471</td>\n      <td>-0.345878</td>\n      <td>-0.312605</td>\n      <td>0.063178</td>\n      <td>0.167227</td>\n      <td>0.288438</td>\n      <td>-0.045123</td>\n      <td>-0.208878</td>\n      <td>0.049437</td>\n      <td>0.049607</td>\n      <td>0.127715</td>\n      <td>-0.430703</td>\n      <td>0.305041</td>\n      <td>0.237689</td>\n      <td>-0.217557</td>\n      <td>-0.165059</td>\n      <td>-0.216847</td>\n      <td>0.400648</td>\n      <td>0.118239</td>\n      <td>0.037590</td>\n      <td>0.223998</td>\n      <td>0.433889</td>\n      <td>0.271647</td>\n      <td>-0.157249</td>\n      <td>0.287142</td>\n      <td>-0.384944</td>\n      <td>0.009272</td>\n      <td>0.327464</td>\n      <td>-0.565579</td>\n      <td>0.099891</td>\n      <td>0.145617</td>\n      <td>-0.215092</td>\n      <td>-0.510071</td>\n      <td>-0.131367</td>\n      <td>0.330615</td>\n      <td>0.191556</td>\n      <td>0.374407</td>\n      <td>0.212004</td>\n      <td>-0.176874</td>\n      <td>0.110514</td>\n      <td>-0.089610</td>\n      <td>0.014662</td>\n      <td>0.157101</td>\n      <td>-0.000313</td>\n      <td>0.027682</td>\n      <td>0.006716</td>\n      <td>-0.123089</td>\n      <td>0.141583</td>\n      <td>0.095132</td>\n      <td>0.145961</td>\n      <td>-0.046628</td>\n      <td>-0.417901</td>\n      <td>0.134852</td>\n      <td>-0.056276</td>\n      <td>-0.096934</td>\n      <td>0.055077</td>\n      <td>0.082040</td>\n      <td>-2.882313</td>\n      <td>-0.247634</td>\n      <td>0.015631</td>\n      <td>0.113560</td>\n      <td>-0.103799</td>\n      <td>-0.237519</td>\n      <td>0.199018</td>\n      <td>0.087931</td>\n      <td>-0.274387</td>\n      <td>0.021310</td>\n      <td>-0.238471</td>\n      <td>-0.180123</td>\n      <td>0.181862</td>\n      <td>-0.341813</td>\n      <td>0.204152</td>\n      <td>0.376023</td>\n      <td>0.424304</td>\n      <td>-0.122498</td>\n      <td>-0.287502</td>\n      <td>0.223616</td>\n      <td>0.103763</td>\n      <td>-0.148789</td>\n      <td>0.308311</td>\n      <td>-0.228412</td>\n      <td>-0.396483</td>\n      <td>0.270812</td>\n      <td>-0.035155</td>\n      <td>0.617137</td>\n      <td>0.126271</td>\n      <td>0.073144</td>\n      <td>0.039875</td>\n      <td>0.416543</td>\n      <td>0.780946</td>\n      <td>0.077763</td>\n      <td>-0.001080</td>\n      <td>0.284301</td>\n      <td>-0.015807</td>\n      <td>0.098068</td>\n      <td>0.029924</td>\n      <td>0.413759</td>\n      <td>0.286685</td>\n      <td>0.132754</td>\n      <td>0.065874</td>\n      <td>0.147840</td>\n      <td>-0.000816</td>\n      <td>0.253687</td>\n      <td>0.512638</td>\n      <td>-0.167215</td>\n      <td>0.093433</td>\n      <td>0.168881</td>\n      <td>0.108314</td>\n      <td>0.309984</td>\n      <td>-0.041571</td>\n      <td>0.044469</td>\n      <td>-0.136723</td>\n      <td>0.211373</td>\n      <td>-0.356278</td>\n      <td>0.154720</td>\n      <td>-0.247896</td>\n      <td>0.362811</td>\n      <td>0.426105</td>\n      <td>-0.268492</td>\n      <td>0.381618</td>\n      <td>0.058557</td>\n      <td>-0.361041</td>\n      <td>0.372211</td>\n      <td>-0.058382</td>\n      <td>-0.072765</td>\n      <td>0.116027</td>\n      <td>0.154523</td>\n      <td>-0.054554</td>\n      <td>-0.249889</td>\n      <td>0.367868</td>\n      <td>-0.122318</td>\n      <td>0.644729</td>\n      <td>0.197869</td>\n      <td>-0.172396</td>\n      <td>-0.345600</td>\n      <td>0.335623</td>\n      <td>-0.273300</td>\n      <td>0.268791</td>\n      <td>0.122981</td>\n      <td>-0.045008</td>\n      <td>0.047277</td>\n      <td>-0.283486</td>\n      <td>0.469759</td>\n      <td>0.262965</td>\n      <td>-0.290420</td>\n      <td>0.054679</td>\n      <td>-0.191597</td>\n      <td>-0.143793</td>\n      <td>0.091227</td>\n      <td>0.382634</td>\n      <td>0.093982</td>\n      <td>-0.402856</td>\n      <td>-0.257157</td>\n      <td>0.007309</td>\n      <td>0.818317</td>\n      <td>-0.056282</td>\n      <td>0.004714</td>\n      <td>0.332163</td>\n      <td>-0.038529</td>\n      <td>0.210475</td>\n      <td>-0.023986</td>\n      <td>0.130618</td>\n      <td>0.125285</td>\n      <td>-0.129686</td>\n      <td>0.201329</td>\n      <td>-0.336717</td>\n      <td>0.050248</td>\n      <td>-0.190869</td>\n      <td>0.126133</td>\n      <td>0.479797</td>\n      <td>-0.125874</td>\n      <td>-0.070950</td>\n      <td>-0.143692</td>\n      <td>-0.130181</td>\n      <td>0.171741</td>\n      <td>-0.076880</td>\n      <td>-0.388324</td>\n      <td>0.257433</td>\n      <td>0.143909</td>\n      <td>0.151024</td>\n      <td>-0.297866</td>\n      <td>-0.128111</td>\n      <td>0.376689</td>\n      <td>0.175521</td>\n      <td>0.118543</td>\n      <td>-1.133204</td>\n      <td>0.476384</td>\n      <td>0.069075</td>\n      <td>-0.356579</td>\n      <td>0.563429</td>\n      <td>0.285773</td>\n      <td>-0.155253</td>\n      <td>0.305933</td>\n      <td>-0.367506</td>\n      <td>0.540473</td>\n      <td>-0.021083</td>\n      <td>0.066829</td>\n      <td>0.402238</td>\n      <td>-0.075639</td>\n      <td>0.109660</td>\n      <td>-0.123663</td>\n      <td>0.239306</td>\n      <td>-0.705540</td>\n      <td>0.060390</td>\n      <td>0.073319</td>\n      <td>0.389951</td>\n      <td>-0.127743</td>\n      <td>-0.055209</td>\n      <td>0.170106</td>\n      <td>-0.089865</td>\n      <td>0.123567</td>\n      <td>-0.181661</td>\n      <td>-0.124836</td>\n      <td>-0.023484</td>\n      <td>0.244534</td>\n      <td>0.274888</td>\n      <td>0.183664</td>\n      <td>-0.052647</td>\n      <td>-0.804746</td>\n      <td>-1.542918</td>\n      <td>0.306663</td>\n      <td>0.391627</td>\n      <td>0.741156</td>\n      <td>-0.032830</td>\n      <td>0.094576</td>\n      <td>-0.022537</td>\n      <td>0.060180</td>\n      <td>0.105492</td>\n      <td>0.188438</td>\n      <td>0.057571</td>\n      <td>0.074640</td>\n      <td>0.437863</td>\n      <td>-0.137878</td>\n      <td>0.034812</td>\n      <td>-0.141849</td>\n      <td>-0.247712</td>\n      <td>0.279302</td>\n      <td>-0.404679</td>\n      <td>0.172610</td>\n      <td>0.236249</td>\n      <td>0.231468</td>\n      <td>0.270060</td>\n      <td>0.060169</td>\n      <td>0.073054</td>\n      <td>0.194955</td>\n      <td>-0.459749</td>\n      <td>0.557014</td>\n      <td>0.182896</td>\n      <td>0.156694</td>\n      <td>0.158557</td>\n      <td>0.142131</td>\n      <td>-0.224718</td>\n      <td>0.012433</td>\n      <td>-0.079857</td>\n      <td>-0.403267</td>\n      <td>0.034917</td>\n      <td>-0.190151</td>\n      <td>0.304608</td>\n      <td>0.197498</td>\n      <td>0.148168</td>\n      <td>0.146923</td>\n      <td>0.102384</td>\n      <td>0.227980</td>\n      <td>0.035478</td>\n      <td>0.012525</td>\n      <td>0.088056</td>\n      <td>-0.237266</td>\n      <td>-0.013573</td>\n      <td>0.428837</td>\n      <td>0.010675</td>\n      <td>-0.011769</td>\n      <td>-0.206249</td>\n      <td>-0.093406</td>\n      <td>-0.055180</td>\n      <td>0.272245</td>\n      <td>0.113926</td>\n      <td>0.415245</td>\n      <td>-0.130399</td>\n      <td>0.292666</td>\n      <td>0.346836</td>\n      <td>0.326721</td>\n      <td>0.422867</td>\n      <td>0.221530</td>\n      <td>0.013407</td>\n      <td>0.104407</td>\n      <td>0.030790</td>\n      <td>0.004473</td>\n      <td>4.587753</td>\n      <td>0.243332</td>\n      <td>0.110125</td>\n      <td>0.082508</td>\n      <td>0.216538</td>\n      <td>-0.103398</td>\n      <td>0.499644</td>\n      <td>0.061743</td>\n      <td>-0.043593</td>\n      <td>0.285424</td>\n      <td>-0.119368</td>\n      <td>0.134653</td>\n      <td>-0.305073</td>\n      <td>-0.134592</td>\n      <td>-0.288831</td>\n      <td>0.482646</td>\n      <td>0.230063</td>\n      <td>0.267644</td>\n      <td>0.106669</td>\n      <td>0.168055</td>\n      <td>-0.120968</td>\n      <td>0.520531</td>\n      <td>-0.173883</td>\n      <td>-0.146917</td>\n      <td>-0.128240</td>\n      <td>-0.228523</td>\n      <td>-0.183027</td>\n      <td>-0.016809</td>\n      <td>-0.307935</td>\n      <td>0.234795</td>\n      <td>0.059595</td>\n      <td>-0.040782</td>\n      <td>-0.088659</td>\n      <td>-0.355450</td>\n      <td>0.449389</td>\n      <td>-0.062130</td>\n      <td>-0.260569</td>\n      <td>-0.006634</td>\n      <td>0.399993</td>\n      <td>0.040218</td>\n      <td>-0.007471</td>\n      <td>0.482334</td>\n      <td>0.200412</td>\n      <td>0.028204</td>\n      <td>-0.021496</td>\n      <td>0.396561</td>\n      <td>0.390499</td>\n      <td>0.654072</td>\n      <td>-0.567551</td>\n      <td>-0.032819</td>\n      <td>-0.232219</td>\n      <td>0.084525</td>\n      <td>-0.319877</td>\n      <td>0.190442</td>\n      <td>0.321887</td>\n      <td>-0.010718</td>\n      <td>-0.052586</td>\n      <td>0.174991</td>\n      <td>0.134580</td>\n      <td>0.043581</td>\n      <td>0.093323</td>\n      <td>-0.222376</td>\n      <td>0.190514</td>\n      <td>-0.259734</td>\n      <td>-0.319978</td>\n      <td>-0.094615</td>\n      <td>-0.202123</td>\n      <td>-0.146689</td>\n      <td>0.343829</td>\n      <td>-0.132182</td>\n      <td>-0.128431</td>\n      <td>0.085141</td>\n      <td>0.012305</td>\n      <td>0.669596</td>\n      <td>0.035358</td>\n      <td>-0.067230</td>\n      <td>-0.077602</td>\n      <td>-0.287588</td>\n      <td>-0.124050</td>\n      <td>-0.624496</td>\n      <td>0.215740</td>\n      <td>-0.245675</td>\n      <td>0.171061</td>\n      <td>0.169850</td>\n      <td>0.131098</td>\n      <td>0.243320</td>\n      <td>-0.197165</td>\n      <td>-0.150008</td>\n      <td>1.173477</td>\n      <td>-0.001061</td>\n      <td>-0.010695</td>\n      <td>0.073038</td>\n      <td>0.475230</td>\n      <td>0.088670</td>\n      <td>-0.085849</td>\n      <td>0.470349</td>\n      <td>-0.007309</td>\n      <td>0.038967</td>\n      <td>0.458922</td>\n      <td>0.034546</td>\n      <td>0.127686</td>\n      <td>0.000078</td>\n      <td>0.219262</td>\n      <td>0.105941</td>\n      <td>0.465120</td>\n      <td>-0.190167</td>\n      <td>-0.095677</td>\n      <td>-0.425898</td>\n      <td>0.032250</td>\n      <td>-0.012627</td>\n      <td>-0.504825</td>\n      <td>0.212114</td>\n      <td>0.476855</td>\n      <td>-0.380684</td>\n      <td>0.064843</td>\n      <td>-0.155434</td>\n      <td>-0.048308</td>\n      <td>0.410547</td>\n      <td>-0.091522</td>\n      <td>-0.325071</td>\n      <td>-0.056600</td>\n      <td>0.127995</td>\n      <td>0.509393</td>\n      <td>-0.102349</td>\n      <td>0.332908</td>\n      <td>-0.040991</td>\n      <td>0.108821</td>\n      <td>-0.328062</td>\n      <td>-0.019397</td>\n      <td>0.678357</td>\n      <td>0.112795</td>\n      <td>-0.170870</td>\n      <td>0.247348</td>\n      <td>0.202667</td>\n      <td>-0.443963</td>\n      <td>0.150961</td>\n      <td>-0.555595</td>\n      <td>0.167062</td>\n      <td>0.238220</td>\n      <td>-0.200563</td>\n      <td>-0.242487</td>\n      <td>0.193388</td>\n      <td>-0.307706</td>\n      <td>0.174820</td>\n      <td>-0.405816</td>\n      <td>0.403904</td>\n      <td>0.660097</td>\n      <td>0.552302</td>\n      <td>0.382159</td>\n      <td>0.294486</td>\n      <td>0.020989</td>\n      <td>-0.010870</td>\n      <td>0.257566</td>\n      <td>0.252134</td>\n      <td>-0.117760</td>\n      <td>-0.213748</td>\n      <td>0.148738</td>\n      <td>0.028456</td>\n      <td>0.377452</td>\n      <td>0.239205</td>\n      <td>-0.124741</td>\n      <td>-0.279801</td>\n      <td>-0.054819</td>\n      <td>0.030798</td>\n      <td>0.141301</td>\n      <td>-0.031256</td>\n      <td>-0.121317</td>\n      <td>-0.124733</td>\n      <td>0.296470</td>\n      <td>-0.017738</td>\n      <td>0.064700</td>\n      <td>0.251342</td>\n      <td>-0.344373</td>\n      <td>0.064800</td>\n      <td>-0.049627</td>\n      <td>0.258808</td>\n      <td>-0.109924</td>\n      <td>-0.026413</td>\n      <td>0.071289</td>\n      <td>-0.261268</td>\n      <td>0.200874</td>\n      <td>0.076666</td>\n      <td>-0.064258</td>\n      <td>0.008165</td>\n      <td>-0.067243</td>\n      <td>0.224314</td>\n      <td>0.264509</td>\n      <td>-0.296599</td>\n      <td>0.252199</td>\n      <td>0.417744</td>\n      <td>-0.354891</td>\n      <td>0.131328</td>\n      <td>-0.193734</td>\n      <td>0.225204</td>\n      <td>0.143845</td>\n      <td>0.115912</td>\n      <td>0.131098</td>\n      <td>0.184448</td>\n      <td>0.174881</td>\n      <td>-0.017394</td>\n      <td>-0.626449</td>\n      <td>-0.041126</td>\n      <td>0.149054</td>\n      <td>0.040721</td>\n      <td>-0.166991</td>\n      <td>0.285461</td>\n      <td>0.305882</td>\n      <td>-0.096526</td>\n      <td>-0.234143</td>\n      <td>-0.406645</td>\n      <td>0.096393</td>\n      <td>0.232464</td>\n      <td>-0.059252</td>\n      <td>-0.064103</td>\n      <td>-0.088494</td>\n      <td>0.674842</td>\n      <td>0.143414</td>\n      <td>-0.174938</td>\n      <td>0.089183</td>\n      <td>-0.063448</td>\n      <td>0.303750</td>\n      <td>0.029114</td>\n      <td>-0.251200</td>\n      <td>0.172916</td>\n      <td>-0.411044</td>\n      <td>-0.545882</td>\n      <td>0.261899</td>\n      <td>0.188732</td>\n      <td>-0.250089</td>\n      <td>-0.161904</td>\n      <td>-0.198985</td>\n      <td>0.323969</td>\n      <td>0.132217</td>\n      <td>0.065788</td>\n      <td>0.173405</td>\n      <td>-0.039658</td>\n      <td>-0.138034</td>\n      <td>-0.010410</td>\n      <td>-0.425527</td>\n      <td>0.048467</td>\n      <td>0.462357</td>\n      <td>0.063406</td>\n      <td>0.028772</td>\n      <td>0.064880</td>\n      <td>-0.016006</td>\n      <td>0.156429</td>\n      <td>0.201508</td>\n      <td>0.110440</td>\n      <td>-0.012298</td>\n      <td>-0.048098</td>\n      <td>0.047057</td>\n      <td>0.010535</td>\n      <td>-0.011116</td>\n      <td>-0.023430</td>\n      <td>-0.418387</td>\n      <td>-0.030550</td>\n      <td>0.148495</td>\n      <td>-0.026301</td>\n      <td>0.675363</td>\n      <td>-0.014671</td>\n      <td>-0.038551</td>\n      <td>-0.187886</td>\n      <td>0.172552</td>\n      <td>-0.068385</td>\n      <td>-0.286752</td>\n      <td>-0.073326</td>\n      <td>0.533121</td>\n      <td>-0.075095</td>\n      <td>0.299108</td>\n      <td>0.544477</td>\n      <td>-0.002998</td>\n      <td>-0.110569</td>\n      <td>0.150285</td>\n      <td>0.332477</td>\n      <td>0.018159</td>\n      <td>0.310728</td>\n      <td>-0.305854</td>\n      <td>-0.206219</td>\n      <td>-0.377258</td>\n      <td>0.248645</td>\n      <td>0.475760</td>\n      <td>0.126378</td>\n      <td>0.095324</td>\n      <td>-0.114514</td>\n      <td>-0.334674</td>\n      <td>-0.068975</td>\n      <td>-0.014995</td>\n      <td>-0.341405</td>\n      <td>-0.134235</td>\n      <td>-0.060644</td>\n      <td>-0.264637</td>\n      <td>0.083678</td>\n      <td>0.027673</td>\n      <td>-0.095689</td>\n      <td>-0.172272</td>\n      <td>0.101961</td>\n      <td>-0.207517</td>\n      <td>0.034880</td>\n      <td>-0.026079</td>\n      <td>-0.035831</td>\n      <td>0.668570</td>\n      <td>0.324005</td>\n      <td>-0.296785</td>\n      <td>-0.292222</td>\n      <td>0.109131</td>\n      <td>0.228367</td>\n      <td>0.048022</td>\n      <td>0.002982</td>\n      <td>-0.136173</td>\n      <td>-0.114503</td>\n      <td>0.249840</td>\n      <td>0.213267</td>\n      <td>0.283380</td>\n      <td>0.108792</td>\n      <td>0.143121</td>\n      <td>0.063068</td>\n      <td>0.114636</td>\n      <td>0.087869</td>\n      <td>0.048760</td>\n      <td>0.256539</td>\n      <td>-0.102271</td>\n      <td>-0.049987</td>\n      <td>0.000990</td>\n    </tr>\n    <tr>\n      <th>5798</th>\n      <td>-0.137193</td>\n      <td>0.145458</td>\n      <td>0.285109</td>\n      <td>-0.066712</td>\n      <td>0.093408</td>\n      <td>-0.177522</td>\n      <td>0.163986</td>\n      <td>-0.047016</td>\n      <td>-0.007576</td>\n      <td>-0.214706</td>\n      <td>-0.058691</td>\n      <td>0.319188</td>\n      <td>0.190422</td>\n      <td>0.076463</td>\n      <td>-0.063008</td>\n      <td>-0.267173</td>\n      <td>0.041295</td>\n      <td>0.093231</td>\n      <td>0.581905</td>\n      <td>0.328003</td>\n      <td>0.056584</td>\n      <td>-0.864065</td>\n      <td>-0.203620</td>\n      <td>-0.174470</td>\n      <td>0.135858</td>\n      <td>-0.086740</td>\n      <td>0.006643</td>\n      <td>-0.154021</td>\n      <td>-0.166658</td>\n      <td>0.063497</td>\n      <td>0.119456</td>\n      <td>0.131839</td>\n      <td>-0.428446</td>\n      <td>-0.283512</td>\n      <td>0.227354</td>\n      <td>0.001393</td>\n      <td>0.089353</td>\n      <td>0.082466</td>\n      <td>0.259382</td>\n      <td>-0.116425</td>\n      <td>0.114204</td>\n      <td>-0.200304</td>\n      <td>0.268641</td>\n      <td>-0.213897</td>\n      <td>0.151884</td>\n      <td>-0.076445</td>\n      <td>0.271173</td>\n      <td>-0.260567</td>\n      <td>0.304493</td>\n      <td>0.194877</td>\n      <td>0.062892</td>\n      <td>0.150694</td>\n      <td>0.152871</td>\n      <td>0.174529</td>\n      <td>-0.044254</td>\n      <td>-0.740778</td>\n      <td>-0.142606</td>\n      <td>0.454390</td>\n      <td>0.412939</td>\n      <td>0.341537</td>\n      <td>-0.198301</td>\n      <td>-0.038304</td>\n      <td>0.246027</td>\n      <td>0.122126</td>\n      <td>0.553864</td>\n      <td>0.178828</td>\n      <td>0.074974</td>\n      <td>-0.209458</td>\n      <td>-0.061300</td>\n      <td>0.114607</td>\n      <td>-0.248671</td>\n      <td>-0.328152</td>\n      <td>0.144857</td>\n      <td>0.042990</td>\n      <td>-0.543537</td>\n      <td>0.137574</td>\n      <td>0.111852</td>\n      <td>0.442921</td>\n      <td>-0.072612</td>\n      <td>-0.036291</td>\n      <td>0.121957</td>\n      <td>0.018479</td>\n      <td>-0.152546</td>\n      <td>0.131298</td>\n      <td>-0.037037</td>\n      <td>-0.153288</td>\n      <td>0.046613</td>\n      <td>0.202089</td>\n      <td>0.248485</td>\n      <td>0.000386</td>\n      <td>0.130957</td>\n      <td>-0.042190</td>\n      <td>0.089310</td>\n      <td>-0.371683</td>\n      <td>-0.113110</td>\n      <td>0.086186</td>\n      <td>-0.035291</td>\n      <td>-0.043324</td>\n      <td>-0.283935</td>\n      <td>0.311606</td>\n      <td>-0.238291</td>\n      <td>-0.136672</td>\n      <td>0.155173</td>\n      <td>-0.085528</td>\n      <td>0.419597</td>\n      <td>0.364502</td>\n      <td>0.149295</td>\n      <td>-0.450078</td>\n      <td>-0.206489</td>\n      <td>0.310355</td>\n      <td>0.237552</td>\n      <td>-0.108931</td>\n      <td>-0.173596</td>\n      <td>-0.007025</td>\n      <td>-0.432121</td>\n      <td>-0.126545</td>\n      <td>-0.044900</td>\n      <td>0.096462</td>\n      <td>-0.049026</td>\n      <td>-0.296358</td>\n      <td>0.235604</td>\n      <td>0.038211</td>\n      <td>-0.643704</td>\n      <td>-0.164600</td>\n      <td>-0.137978</td>\n      <td>0.509269</td>\n      <td>-0.093508</td>\n      <td>-0.396823</td>\n      <td>0.663942</td>\n      <td>0.112753</td>\n      <td>0.228380</td>\n      <td>0.024075</td>\n      <td>0.024981</td>\n      <td>-0.051495</td>\n      <td>0.373561</td>\n      <td>0.330433</td>\n      <td>0.112064</td>\n      <td>0.003050</td>\n      <td>0.330785</td>\n      <td>-0.106191</td>\n      <td>0.005969</td>\n      <td>0.219147</td>\n      <td>0.167658</td>\n      <td>0.147235</td>\n      <td>0.091115</td>\n      <td>0.140891</td>\n      <td>-0.251173</td>\n      <td>0.173266</td>\n      <td>0.135739</td>\n      <td>0.308271</td>\n      <td>-0.056549</td>\n      <td>0.431241</td>\n      <td>-0.126551</td>\n      <td>-0.157846</td>\n      <td>0.093940</td>\n      <td>0.255150</td>\n      <td>0.003652</td>\n      <td>0.182590</td>\n      <td>-0.012523</td>\n      <td>-0.359231</td>\n      <td>-0.326903</td>\n      <td>0.084762</td>\n      <td>0.112342</td>\n      <td>0.360331</td>\n      <td>-0.024473</td>\n      <td>-0.316633</td>\n      <td>0.080717</td>\n      <td>0.010139</td>\n      <td>0.131647</td>\n      <td>-0.510711</td>\n      <td>0.190655</td>\n      <td>0.173257</td>\n      <td>-0.284240</td>\n      <td>-0.201848</td>\n      <td>-0.255966</td>\n      <td>0.441580</td>\n      <td>0.058903</td>\n      <td>0.086974</td>\n      <td>0.209829</td>\n      <td>0.358047</td>\n      <td>0.376028</td>\n      <td>-0.117540</td>\n      <td>0.198105</td>\n      <td>-0.300244</td>\n      <td>0.305447</td>\n      <td>0.398138</td>\n      <td>-0.523188</td>\n      <td>0.174043</td>\n      <td>0.204542</td>\n      <td>-0.281990</td>\n      <td>-0.456775</td>\n      <td>-0.154793</td>\n      <td>0.285681</td>\n      <td>0.043810</td>\n      <td>0.340361</td>\n      <td>0.189553</td>\n      <td>-0.226872</td>\n      <td>0.033108</td>\n      <td>-0.068877</td>\n      <td>-0.035568</td>\n      <td>0.168230</td>\n      <td>0.023838</td>\n      <td>0.026076</td>\n      <td>0.021686</td>\n      <td>-0.232444</td>\n      <td>0.279637</td>\n      <td>-0.006176</td>\n      <td>0.106174</td>\n      <td>-0.021702</td>\n      <td>-0.177450</td>\n      <td>0.140583</td>\n      <td>-0.032721</td>\n      <td>-0.091545</td>\n      <td>-0.062717</td>\n      <td>-0.043442</td>\n      <td>-3.070965</td>\n      <td>-0.176808</td>\n      <td>-0.081365</td>\n      <td>0.150761</td>\n      <td>0.024611</td>\n      <td>-0.338437</td>\n      <td>0.183892</td>\n      <td>0.101928</td>\n      <td>-0.192602</td>\n      <td>0.032685</td>\n      <td>-0.243075</td>\n      <td>-0.075472</td>\n      <td>0.229951</td>\n      <td>-0.403881</td>\n      <td>0.116726</td>\n      <td>0.296445</td>\n      <td>0.353647</td>\n      <td>-0.064704</td>\n      <td>-0.302995</td>\n      <td>0.114335</td>\n      <td>0.031398</td>\n      <td>-0.156333</td>\n      <td>0.255067</td>\n      <td>-0.063343</td>\n      <td>-0.288846</td>\n      <td>0.409042</td>\n      <td>-0.048090</td>\n      <td>0.513258</td>\n      <td>0.187238</td>\n      <td>0.119399</td>\n      <td>0.022758</td>\n      <td>0.332491</td>\n      <td>0.662134</td>\n      <td>0.170822</td>\n      <td>0.057783</td>\n      <td>0.322016</td>\n      <td>-0.074484</td>\n      <td>0.012290</td>\n      <td>0.102976</td>\n      <td>0.390084</td>\n      <td>0.114792</td>\n      <td>0.127745</td>\n      <td>-0.015288</td>\n      <td>0.213890</td>\n      <td>-0.001971</td>\n      <td>0.169215</td>\n      <td>0.396323</td>\n      <td>-0.229223</td>\n      <td>0.041697</td>\n      <td>0.160436</td>\n      <td>0.061427</td>\n      <td>0.242570</td>\n      <td>-0.004780</td>\n      <td>0.141248</td>\n      <td>-0.171765</td>\n      <td>0.285437</td>\n      <td>-0.303174</td>\n      <td>0.149199</td>\n      <td>-0.406177</td>\n      <td>0.447216</td>\n      <td>0.493108</td>\n      <td>-0.156004</td>\n      <td>0.420629</td>\n      <td>0.006293</td>\n      <td>-0.356126</td>\n      <td>0.164467</td>\n      <td>-0.454964</td>\n      <td>0.086939</td>\n      <td>-0.051795</td>\n      <td>0.100888</td>\n      <td>-0.090847</td>\n      <td>-0.278282</td>\n      <td>0.321673</td>\n      <td>-0.185105</td>\n      <td>0.613843</td>\n      <td>0.284683</td>\n      <td>-0.149865</td>\n      <td>-0.396418</td>\n      <td>0.147105</td>\n      <td>-0.271327</td>\n      <td>0.304151</td>\n      <td>-0.049969</td>\n      <td>0.115837</td>\n      <td>0.039303</td>\n      <td>-0.316830</td>\n      <td>0.469439</td>\n      <td>0.233150</td>\n      <td>-0.258309</td>\n      <td>-0.002399</td>\n      <td>-0.157735</td>\n      <td>0.052592</td>\n      <td>0.047975</td>\n      <td>0.233044</td>\n      <td>0.108142</td>\n      <td>-0.247049</td>\n      <td>-0.309014</td>\n      <td>0.015384</td>\n      <td>0.652661</td>\n      <td>-0.219742</td>\n      <td>0.040949</td>\n      <td>0.482838</td>\n      <td>0.037195</td>\n      <td>0.267366</td>\n      <td>0.083669</td>\n      <td>0.163652</td>\n      <td>0.130848</td>\n      <td>-0.030017</td>\n      <td>0.076752</td>\n      <td>-0.287414</td>\n      <td>0.059392</td>\n      <td>-0.348479</td>\n      <td>0.149241</td>\n      <td>0.502908</td>\n      <td>-0.075915</td>\n      <td>0.002490</td>\n      <td>-0.102679</td>\n      <td>-0.295635</td>\n      <td>0.048229</td>\n      <td>-0.033648</td>\n      <td>-0.122788</td>\n      <td>0.155117</td>\n      <td>0.154661</td>\n      <td>-0.002394</td>\n      <td>-0.235206</td>\n      <td>-0.161113</td>\n      <td>0.317246</td>\n      <td>0.086470</td>\n      <td>0.268052</td>\n      <td>-1.088316</td>\n      <td>0.572844</td>\n      <td>0.250123</td>\n      <td>-0.249332</td>\n      <td>0.572504</td>\n      <td>0.289968</td>\n      <td>-0.056845</td>\n      <td>0.354439</td>\n      <td>-0.385794</td>\n      <td>0.514765</td>\n      <td>-0.034586</td>\n      <td>0.081908</td>\n      <td>0.694056</td>\n      <td>0.012635</td>\n      <td>0.114672</td>\n      <td>-0.177682</td>\n      <td>0.200928</td>\n      <td>-0.724951</td>\n      <td>0.023766</td>\n      <td>0.051795</td>\n      <td>0.347250</td>\n      <td>-0.017165</td>\n      <td>-0.132733</td>\n      <td>0.114947</td>\n      <td>0.017597</td>\n      <td>0.142115</td>\n      <td>-0.236726</td>\n      <td>-0.154013</td>\n      <td>0.126272</td>\n      <td>0.257089</td>\n      <td>0.212672</td>\n      <td>0.101434</td>\n      <td>-0.005463</td>\n      <td>-0.733822</td>\n      <td>-1.462212</td>\n      <td>0.298860</td>\n      <td>0.449988</td>\n      <td>0.733735</td>\n      <td>-0.047313</td>\n      <td>0.145938</td>\n      <td>0.087589</td>\n      <td>-0.005651</td>\n      <td>0.062469</td>\n      <td>0.173531</td>\n      <td>0.093825</td>\n      <td>-0.029331</td>\n      <td>0.444125</td>\n      <td>-0.021245</td>\n      <td>-0.117827</td>\n      <td>-0.054896</td>\n      <td>-0.165349</td>\n      <td>0.231078</td>\n      <td>-0.454145</td>\n      <td>0.172870</td>\n      <td>0.157398</td>\n      <td>0.218177</td>\n      <td>0.180482</td>\n      <td>-0.245151</td>\n      <td>0.159684</td>\n      <td>0.412418</td>\n      <td>-0.476833</td>\n      <td>0.602345</td>\n      <td>0.236185</td>\n      <td>0.057630</td>\n      <td>-0.057051</td>\n      <td>0.136654</td>\n      <td>-0.238345</td>\n      <td>0.143253</td>\n      <td>-0.011439</td>\n      <td>-0.291248</td>\n      <td>0.008720</td>\n      <td>-0.160638</td>\n      <td>0.189454</td>\n      <td>0.080644</td>\n      <td>0.140982</td>\n      <td>0.278438</td>\n      <td>0.168202</td>\n      <td>0.262578</td>\n      <td>-0.104650</td>\n      <td>0.037699</td>\n      <td>0.136130</td>\n      <td>-0.215785</td>\n      <td>-0.282512</td>\n      <td>0.302961</td>\n      <td>-0.053165</td>\n      <td>0.005802</td>\n      <td>0.060965</td>\n      <td>-0.098238</td>\n      <td>-0.040212</td>\n      <td>-0.090117</td>\n      <td>0.168087</td>\n      <td>0.398319</td>\n      <td>-0.121302</td>\n      <td>0.281986</td>\n      <td>0.317084</td>\n      <td>0.313597</td>\n      <td>0.358396</td>\n      <td>0.009464</td>\n      <td>0.033063</td>\n      <td>0.061557</td>\n      <td>0.028780</td>\n      <td>-0.001849</td>\n      <td>4.551806</td>\n      <td>0.088864</td>\n      <td>0.324186</td>\n      <td>0.097335</td>\n      <td>0.344112</td>\n      <td>-0.047675</td>\n      <td>0.406895</td>\n      <td>0.207308</td>\n      <td>-0.037088</td>\n      <td>0.404700</td>\n      <td>-0.060503</td>\n      <td>0.111403</td>\n      <td>-0.275313</td>\n      <td>-0.039107</td>\n      <td>-0.246947</td>\n      <td>0.280948</td>\n      <td>0.148235</td>\n      <td>0.080893</td>\n      <td>0.138986</td>\n      <td>0.339330</td>\n      <td>-0.127223</td>\n      <td>0.573909</td>\n      <td>-0.206516</td>\n      <td>-0.074858</td>\n      <td>-0.116951</td>\n      <td>-0.095827</td>\n      <td>0.018867</td>\n      <td>0.065639</td>\n      <td>-0.363526</td>\n      <td>0.109518</td>\n      <td>-0.032558</td>\n      <td>0.017930</td>\n      <td>-0.078727</td>\n      <td>-0.323117</td>\n      <td>0.471757</td>\n      <td>-0.207449</td>\n      <td>-0.244216</td>\n      <td>-0.023191</td>\n      <td>0.405933</td>\n      <td>-0.038485</td>\n      <td>-0.001030</td>\n      <td>0.430866</td>\n      <td>0.281027</td>\n      <td>0.116462</td>\n      <td>0.009130</td>\n      <td>0.253667</td>\n      <td>0.384996</td>\n      <td>0.290595</td>\n      <td>-0.673216</td>\n      <td>-0.124293</td>\n      <td>-0.053923</td>\n      <td>0.071955</td>\n      <td>-0.390182</td>\n      <td>0.080488</td>\n      <td>0.247643</td>\n      <td>-0.105171</td>\n      <td>-0.139464</td>\n      <td>0.106522</td>\n      <td>0.039369</td>\n      <td>0.145788</td>\n      <td>0.028975</td>\n      <td>-0.202725</td>\n      <td>0.215933</td>\n      <td>-0.220481</td>\n      <td>-0.393871</td>\n      <td>-0.128812</td>\n      <td>-0.161213</td>\n      <td>-0.149792</td>\n      <td>0.406497</td>\n      <td>-0.071667</td>\n      <td>-0.118462</td>\n      <td>0.166328</td>\n      <td>-0.021144</td>\n      <td>0.613396</td>\n      <td>0.035741</td>\n      <td>-0.240043</td>\n      <td>-0.090992</td>\n      <td>-0.387077</td>\n      <td>-0.023804</td>\n      <td>-0.801698</td>\n      <td>0.247690</td>\n      <td>-0.174178</td>\n      <td>0.303614</td>\n      <td>0.107106</td>\n      <td>0.160636</td>\n      <td>0.390093</td>\n      <td>-0.105251</td>\n      <td>-0.252691</td>\n      <td>1.398387</td>\n      <td>-0.056694</td>\n      <td>-0.004260</td>\n      <td>0.077852</td>\n      <td>0.508024</td>\n      <td>0.251119</td>\n      <td>-0.285997</td>\n      <td>0.468437</td>\n      <td>-0.084137</td>\n      <td>0.060491</td>\n      <td>0.448465</td>\n      <td>-0.019801</td>\n      <td>-0.147670</td>\n      <td>0.010732</td>\n      <td>0.125005</td>\n      <td>0.080282</td>\n      <td>0.290036</td>\n      <td>-0.078184</td>\n      <td>-0.164123</td>\n      <td>-0.351651</td>\n      <td>-0.042004</td>\n      <td>0.071081</td>\n      <td>-0.441032</td>\n      <td>0.226924</td>\n      <td>0.514121</td>\n      <td>-0.331835</td>\n      <td>0.001260</td>\n      <td>-0.186533</td>\n      <td>-0.017505</td>\n      <td>0.353123</td>\n      <td>0.017999</td>\n      <td>-0.378622</td>\n      <td>-0.013584</td>\n      <td>0.069780</td>\n      <td>0.422661</td>\n      <td>-0.084303</td>\n      <td>0.404565</td>\n      <td>0.063142</td>\n      <td>-0.017066</td>\n      <td>-0.237108</td>\n      <td>-0.078840</td>\n      <td>0.743538</td>\n      <td>0.122300</td>\n      <td>-0.205030</td>\n      <td>0.319238</td>\n      <td>0.023861</td>\n      <td>-0.453573</td>\n      <td>0.186198</td>\n      <td>-0.312835</td>\n      <td>0.110523</td>\n      <td>0.206434</td>\n      <td>-0.258428</td>\n      <td>-0.181687</td>\n      <td>0.177046</td>\n      <td>-0.161115</td>\n      <td>0.122846</td>\n      <td>-0.356982</td>\n      <td>0.258704</td>\n      <td>0.550243</td>\n      <td>0.417440</td>\n      <td>0.360353</td>\n      <td>0.322013</td>\n      <td>0.066126</td>\n      <td>0.154839</td>\n      <td>0.292147</td>\n      <td>0.244444</td>\n      <td>-0.108820</td>\n      <td>-0.356691</td>\n      <td>0.039248</td>\n      <td>-0.135377</td>\n      <td>0.392029</td>\n      <td>0.159210</td>\n      <td>-0.133916</td>\n      <td>-0.291248</td>\n      <td>-0.094988</td>\n      <td>0.039504</td>\n      <td>0.018454</td>\n      <td>-0.122758</td>\n      <td>-0.230232</td>\n      <td>-0.149548</td>\n      <td>0.280438</td>\n      <td>-0.058096</td>\n      <td>0.187801</td>\n      <td>0.027801</td>\n      <td>-0.321697</td>\n      <td>0.052298</td>\n      <td>0.022998</td>\n      <td>0.191597</td>\n      <td>-0.085138</td>\n      <td>0.028338</td>\n      <td>0.056786</td>\n      <td>-0.230278</td>\n      <td>0.198877</td>\n      <td>0.274416</td>\n      <td>-0.182979</td>\n      <td>-0.001415</td>\n      <td>-0.096938</td>\n      <td>0.198821</td>\n      <td>0.183084</td>\n      <td>-0.281436</td>\n      <td>0.267579</td>\n      <td>0.346376</td>\n      <td>-0.365949</td>\n      <td>0.143158</td>\n      <td>-0.065634</td>\n      <td>-0.006935</td>\n      <td>0.042851</td>\n      <td>0.107270</td>\n      <td>0.093657</td>\n      <td>0.067380</td>\n      <td>0.124359</td>\n      <td>-0.029758</td>\n      <td>-0.462490</td>\n      <td>0.033878</td>\n      <td>0.279373</td>\n      <td>0.090543</td>\n      <td>-0.185027</td>\n      <td>0.562772</td>\n      <td>0.211780</td>\n      <td>-0.083107</td>\n      <td>-0.174525</td>\n      <td>-0.511897</td>\n      <td>0.115093</td>\n      <td>0.170689</td>\n      <td>0.019645</td>\n      <td>-0.026293</td>\n      <td>-0.215979</td>\n      <td>0.564337</td>\n      <td>0.188991</td>\n      <td>-0.289473</td>\n      <td>-0.034662</td>\n      <td>-0.056462</td>\n      <td>0.182796</td>\n      <td>-0.011095</td>\n      <td>-0.233039</td>\n      <td>0.319495</td>\n      <td>-0.223945</td>\n      <td>-0.550600</td>\n      <td>0.251502</td>\n      <td>-0.107099</td>\n      <td>-0.157845</td>\n      <td>-0.251505</td>\n      <td>-0.106310</td>\n      <td>0.352285</td>\n      <td>0.129397</td>\n      <td>0.228652</td>\n      <td>0.096468</td>\n      <td>-0.055256</td>\n      <td>-0.077870</td>\n      <td>-0.069502</td>\n      <td>-0.413190</td>\n      <td>0.212244</td>\n      <td>0.498257</td>\n      <td>0.240471</td>\n      <td>-0.034320</td>\n      <td>0.038215</td>\n      <td>-0.125672</td>\n      <td>0.266316</td>\n      <td>0.174048</td>\n      <td>-0.006858</td>\n      <td>0.166562</td>\n      <td>0.090929</td>\n      <td>0.088613</td>\n      <td>0.105961</td>\n      <td>-0.102771</td>\n      <td>0.004453</td>\n      <td>-0.393167</td>\n      <td>-0.180170</td>\n      <td>0.278069</td>\n      <td>0.137691</td>\n      <td>0.603098</td>\n      <td>0.069573</td>\n      <td>0.072448</td>\n      <td>-0.222114</td>\n      <td>0.266283</td>\n      <td>-0.130260</td>\n      <td>-0.219240</td>\n      <td>-0.105882</td>\n      <td>0.512538</td>\n      <td>0.036792</td>\n      <td>0.316452</td>\n      <td>0.575138</td>\n      <td>0.100380</td>\n      <td>-0.168109</td>\n      <td>0.300144</td>\n      <td>0.430823</td>\n      <td>-0.069484</td>\n      <td>0.438238</td>\n      <td>-0.348733</td>\n      <td>-0.201232</td>\n      <td>-0.235494</td>\n      <td>0.105221</td>\n      <td>0.406333</td>\n      <td>0.234754</td>\n      <td>0.174787</td>\n      <td>-0.203285</td>\n      <td>-0.300867</td>\n      <td>-0.111074</td>\n      <td>-0.081603</td>\n      <td>-0.368640</td>\n      <td>-0.108363</td>\n      <td>-0.000391</td>\n      <td>-0.250889</td>\n      <td>0.140267</td>\n      <td>0.119072</td>\n      <td>-0.195856</td>\n      <td>-0.071644</td>\n      <td>0.209413</td>\n      <td>-0.213366</td>\n      <td>-0.021730</td>\n      <td>0.036682</td>\n      <td>0.006657</td>\n      <td>0.759058</td>\n      <td>0.209435</td>\n      <td>-0.150627</td>\n      <td>-0.281938</td>\n      <td>0.108604</td>\n      <td>0.084329</td>\n      <td>0.238416</td>\n      <td>0.039098</td>\n      <td>-0.122804</td>\n      <td>-0.134536</td>\n      <td>0.243362</td>\n      <td>0.451409</td>\n      <td>0.190678</td>\n      <td>0.082955</td>\n      <td>0.194071</td>\n      <td>0.091210</td>\n      <td>0.221798</td>\n      <td>0.169295</td>\n      <td>-0.040467</td>\n      <td>0.197197</td>\n      <td>-0.028993</td>\n      <td>-0.054743</td>\n      <td>0.039319</td>\n    </tr>\n    <tr>\n      <th>5799</th>\n      <td>-0.186172</td>\n      <td>0.179648</td>\n      <td>0.243748</td>\n      <td>-0.100095</td>\n      <td>0.083960</td>\n      <td>-0.264884</td>\n      <td>0.079414</td>\n      <td>-0.051943</td>\n      <td>0.027550</td>\n      <td>-0.117400</td>\n      <td>-0.082906</td>\n      <td>0.163883</td>\n      <td>0.084087</td>\n      <td>-0.070525</td>\n      <td>-0.012431</td>\n      <td>-0.201493</td>\n      <td>0.126095</td>\n      <td>0.039986</td>\n      <td>0.482932</td>\n      <td>0.263607</td>\n      <td>0.112378</td>\n      <td>-0.734594</td>\n      <td>-0.218636</td>\n      <td>-0.075598</td>\n      <td>0.046997</td>\n      <td>-0.279304</td>\n      <td>-0.059241</td>\n      <td>-0.069265</td>\n      <td>-0.315325</td>\n      <td>-0.070364</td>\n      <td>-0.040366</td>\n      <td>0.309079</td>\n      <td>-0.385736</td>\n      <td>-0.293288</td>\n      <td>0.274859</td>\n      <td>-0.117947</td>\n      <td>-0.028346</td>\n      <td>0.176443</td>\n      <td>0.416964</td>\n      <td>0.096570</td>\n      <td>0.127654</td>\n      <td>-0.375939</td>\n      <td>0.099659</td>\n      <td>-0.228607</td>\n      <td>0.227760</td>\n      <td>-0.129032</td>\n      <td>0.302493</td>\n      <td>-0.154176</td>\n      <td>0.243663</td>\n      <td>0.239816</td>\n      <td>0.070451</td>\n      <td>0.385107</td>\n      <td>-0.039737</td>\n      <td>-0.100099</td>\n      <td>-0.204769</td>\n      <td>-0.650722</td>\n      <td>-0.173833</td>\n      <td>0.390868</td>\n      <td>0.312984</td>\n      <td>0.264680</td>\n      <td>-0.064332</td>\n      <td>-0.050500</td>\n      <td>0.271060</td>\n      <td>0.225866</td>\n      <td>0.536240</td>\n      <td>0.024610</td>\n      <td>-0.023891</td>\n      <td>-0.289053</td>\n      <td>0.104585</td>\n      <td>0.327920</td>\n      <td>-0.302079</td>\n      <td>-0.421168</td>\n      <td>0.092903</td>\n      <td>0.168724</td>\n      <td>-0.561593</td>\n      <td>0.003771</td>\n      <td>0.015795</td>\n      <td>0.240465</td>\n      <td>-0.252855</td>\n      <td>-0.064424</td>\n      <td>-0.052367</td>\n      <td>-0.118202</td>\n      <td>-0.038713</td>\n      <td>0.248863</td>\n      <td>0.008950</td>\n      <td>-0.141860</td>\n      <td>0.135016</td>\n      <td>0.311653</td>\n      <td>0.166037</td>\n      <td>-0.051419</td>\n      <td>0.179497</td>\n      <td>-0.057369</td>\n      <td>-0.021990</td>\n      <td>-0.285449</td>\n      <td>0.019858</td>\n      <td>0.052515</td>\n      <td>-0.143881</td>\n      <td>-0.063700</td>\n      <td>-0.297794</td>\n      <td>0.245798</td>\n      <td>-0.403736</td>\n      <td>0.011080</td>\n      <td>0.071263</td>\n      <td>-0.068837</td>\n      <td>0.404638</td>\n      <td>0.460989</td>\n      <td>0.077451</td>\n      <td>-0.435507</td>\n      <td>0.082564</td>\n      <td>0.291737</td>\n      <td>0.114560</td>\n      <td>-0.054926</td>\n      <td>-0.135359</td>\n      <td>-0.081196</td>\n      <td>-0.261137</td>\n      <td>-0.057429</td>\n      <td>0.008658</td>\n      <td>0.112302</td>\n      <td>-0.013723</td>\n      <td>-0.222782</td>\n      <td>0.167539</td>\n      <td>0.313987</td>\n      <td>-0.711246</td>\n      <td>-0.079570</td>\n      <td>-0.071453</td>\n      <td>0.356817</td>\n      <td>0.078928</td>\n      <td>-0.105963</td>\n      <td>0.539841</td>\n      <td>0.163797</td>\n      <td>0.123566</td>\n      <td>-0.094415</td>\n      <td>0.049853</td>\n      <td>0.314647</td>\n      <td>0.321343</td>\n      <td>0.265864</td>\n      <td>-0.129583</td>\n      <td>-0.099111</td>\n      <td>0.238108</td>\n      <td>-0.005669</td>\n      <td>0.098866</td>\n      <td>0.279564</td>\n      <td>0.120293</td>\n      <td>-0.029945</td>\n      <td>0.047578</td>\n      <td>0.167044</td>\n      <td>-0.308297</td>\n      <td>0.228981</td>\n      <td>0.279752</td>\n      <td>0.296659</td>\n      <td>0.118933</td>\n      <td>0.339381</td>\n      <td>-0.158626</td>\n      <td>-0.116315</td>\n      <td>0.118420</td>\n      <td>0.118906</td>\n      <td>0.121271</td>\n      <td>0.114213</td>\n      <td>0.126573</td>\n      <td>-0.383104</td>\n      <td>-0.285285</td>\n      <td>-0.097045</td>\n      <td>0.172136</td>\n      <td>0.411588</td>\n      <td>-0.024511</td>\n      <td>-0.063496</td>\n      <td>0.114363</td>\n      <td>0.285120</td>\n      <td>0.183627</td>\n      <td>-0.551634</td>\n      <td>0.336677</td>\n      <td>0.433881</td>\n      <td>-0.228686</td>\n      <td>-0.190196</td>\n      <td>-0.200961</td>\n      <td>0.520081</td>\n      <td>0.066223</td>\n      <td>-0.000911</td>\n      <td>0.230123</td>\n      <td>0.488370</td>\n      <td>0.219507</td>\n      <td>-0.162138</td>\n      <td>0.187894</td>\n      <td>-0.433112</td>\n      <td>-0.063303</td>\n      <td>0.456164</td>\n      <td>-0.544107</td>\n      <td>0.077095</td>\n      <td>0.212714</td>\n      <td>-0.303538</td>\n      <td>-0.439197</td>\n      <td>-0.078554</td>\n      <td>0.314915</td>\n      <td>0.121862</td>\n      <td>0.551503</td>\n      <td>0.151515</td>\n      <td>-0.235570</td>\n      <td>0.140029</td>\n      <td>0.054401</td>\n      <td>-0.035657</td>\n      <td>0.108660</td>\n      <td>0.114163</td>\n      <td>0.114349</td>\n      <td>-0.101480</td>\n      <td>-0.075614</td>\n      <td>0.186068</td>\n      <td>0.238603</td>\n      <td>0.049488</td>\n      <td>-0.042062</td>\n      <td>-0.144498</td>\n      <td>0.246942</td>\n      <td>-0.164953</td>\n      <td>0.094125</td>\n      <td>-0.181604</td>\n      <td>-0.270030</td>\n      <td>-2.613631</td>\n      <td>-0.154875</td>\n      <td>-0.000388</td>\n      <td>0.028412</td>\n      <td>0.095190</td>\n      <td>-0.148041</td>\n      <td>0.021894</td>\n      <td>-0.004996</td>\n      <td>-0.137383</td>\n      <td>0.133740</td>\n      <td>-0.208798</td>\n      <td>0.028142</td>\n      <td>0.101741</td>\n      <td>-0.221582</td>\n      <td>0.236867</td>\n      <td>0.595965</td>\n      <td>0.618525</td>\n      <td>-0.153878</td>\n      <td>-0.244164</td>\n      <td>0.140347</td>\n      <td>0.309651</td>\n      <td>-0.272180</td>\n      <td>0.228845</td>\n      <td>-0.213810</td>\n      <td>-0.349507</td>\n      <td>0.199534</td>\n      <td>0.099227</td>\n      <td>0.544011</td>\n      <td>0.231616</td>\n      <td>0.057494</td>\n      <td>0.111521</td>\n      <td>0.169195</td>\n      <td>0.629199</td>\n      <td>0.039075</td>\n      <td>0.101793</td>\n      <td>0.405730</td>\n      <td>-0.108902</td>\n      <td>0.186346</td>\n      <td>0.118911</td>\n      <td>0.420780</td>\n      <td>0.244964</td>\n      <td>0.107629</td>\n      <td>0.116573</td>\n      <td>0.233170</td>\n      <td>-0.104364</td>\n      <td>0.037288</td>\n      <td>0.452113</td>\n      <td>-0.145186</td>\n      <td>0.125079</td>\n      <td>0.072902</td>\n      <td>0.067903</td>\n      <td>0.176885</td>\n      <td>0.128570</td>\n      <td>0.112522</td>\n      <td>0.002515</td>\n      <td>0.263346</td>\n      <td>-0.176577</td>\n      <td>0.154864</td>\n      <td>-0.337913</td>\n      <td>0.113759</td>\n      <td>0.442688</td>\n      <td>-0.166883</td>\n      <td>0.309062</td>\n      <td>-0.156985</td>\n      <td>-0.244082</td>\n      <td>0.732316</td>\n      <td>-0.405418</td>\n      <td>0.049349</td>\n      <td>0.076629</td>\n      <td>0.178855</td>\n      <td>-0.304320</td>\n      <td>-0.169461</td>\n      <td>0.236815</td>\n      <td>-0.167521</td>\n      <td>0.691785</td>\n      <td>0.248205</td>\n      <td>-0.049451</td>\n      <td>-0.406117</td>\n      <td>0.329825</td>\n      <td>-0.105802</td>\n      <td>0.143669</td>\n      <td>-0.086492</td>\n      <td>-0.011968</td>\n      <td>0.036297</td>\n      <td>-0.319344</td>\n      <td>0.484090</td>\n      <td>0.142401</td>\n      <td>-0.356139</td>\n      <td>0.173572</td>\n      <td>-0.037524</td>\n      <td>0.093677</td>\n      <td>0.091989</td>\n      <td>0.045011</td>\n      <td>0.091866</td>\n      <td>-0.479279</td>\n      <td>-0.451516</td>\n      <td>-0.043454</td>\n      <td>0.744394</td>\n      <td>-0.080858</td>\n      <td>0.029481</td>\n      <td>0.275468</td>\n      <td>-0.051814</td>\n      <td>0.212620</td>\n      <td>0.076816</td>\n      <td>0.094565</td>\n      <td>0.147443</td>\n      <td>0.135361</td>\n      <td>0.157544</td>\n      <td>-0.337319</td>\n      <td>0.065566</td>\n      <td>-0.019312</td>\n      <td>0.201026</td>\n      <td>0.522426</td>\n      <td>-0.001718</td>\n      <td>0.113716</td>\n      <td>0.016947</td>\n      <td>-0.213174</td>\n      <td>0.145095</td>\n      <td>-0.028782</td>\n      <td>-0.140021</td>\n      <td>0.004434</td>\n      <td>0.106501</td>\n      <td>0.185751</td>\n      <td>-0.363230</td>\n      <td>-0.103315</td>\n      <td>0.197040</td>\n      <td>0.297601</td>\n      <td>0.050599</td>\n      <td>-1.157263</td>\n      <td>0.399676</td>\n      <td>0.202964</td>\n      <td>-0.318514</td>\n      <td>0.595788</td>\n      <td>0.298106</td>\n      <td>-0.230524</td>\n      <td>0.286361</td>\n      <td>-0.462133</td>\n      <td>0.462410</td>\n      <td>0.000655</td>\n      <td>0.154841</td>\n      <td>0.407353</td>\n      <td>-0.051175</td>\n      <td>0.152394</td>\n      <td>-0.191501</td>\n      <td>0.315363</td>\n      <td>-0.713991</td>\n      <td>0.023393</td>\n      <td>0.080166</td>\n      <td>0.292353</td>\n      <td>0.031207</td>\n      <td>-0.088986</td>\n      <td>0.029142</td>\n      <td>-0.092543</td>\n      <td>0.009028</td>\n      <td>-0.192571</td>\n      <td>-0.057543</td>\n      <td>-0.002781</td>\n      <td>0.109127</td>\n      <td>0.140147</td>\n      <td>0.191238</td>\n      <td>0.028455</td>\n      <td>-0.702601</td>\n      <td>-1.635913</td>\n      <td>0.195723</td>\n      <td>0.304195</td>\n      <td>0.470695</td>\n      <td>-0.003513</td>\n      <td>0.220291</td>\n      <td>0.008413</td>\n      <td>0.040587</td>\n      <td>-0.065108</td>\n      <td>0.141092</td>\n      <td>0.120222</td>\n      <td>-0.052011</td>\n      <td>0.362969</td>\n      <td>-0.036765</td>\n      <td>0.036839</td>\n      <td>-0.079194</td>\n      <td>-0.276581</td>\n      <td>0.202655</td>\n      <td>-0.269324</td>\n      <td>0.148433</td>\n      <td>0.212523</td>\n      <td>0.383705</td>\n      <td>0.054178</td>\n      <td>-0.179690</td>\n      <td>0.011319</td>\n      <td>0.251478</td>\n      <td>-0.290414</td>\n      <td>0.648630</td>\n      <td>0.127239</td>\n      <td>0.121155</td>\n      <td>0.201968</td>\n      <td>0.302226</td>\n      <td>-0.139846</td>\n      <td>0.146931</td>\n      <td>-0.161217</td>\n      <td>-0.229362</td>\n      <td>0.017145</td>\n      <td>0.014194</td>\n      <td>0.181324</td>\n      <td>0.166731</td>\n      <td>0.208772</td>\n      <td>0.112288</td>\n      <td>0.012724</td>\n      <td>0.250854</td>\n      <td>-0.029290</td>\n      <td>0.164436</td>\n      <td>0.219871</td>\n      <td>-0.215580</td>\n      <td>-0.198178</td>\n      <td>0.460069</td>\n      <td>0.042292</td>\n      <td>-0.025143</td>\n      <td>-0.035915</td>\n      <td>-0.283712</td>\n      <td>-0.114488</td>\n      <td>0.154105</td>\n      <td>0.107332</td>\n      <td>0.355283</td>\n      <td>0.014019</td>\n      <td>0.336870</td>\n      <td>0.219213</td>\n      <td>0.333101</td>\n      <td>0.308347</td>\n      <td>0.154799</td>\n      <td>0.130365</td>\n      <td>-0.012931</td>\n      <td>0.004880</td>\n      <td>-0.049432</td>\n      <td>4.661571</td>\n      <td>-0.117565</td>\n      <td>0.313893</td>\n      <td>0.104418</td>\n      <td>0.422277</td>\n      <td>-0.288065</td>\n      <td>0.554124</td>\n      <td>0.167493</td>\n      <td>0.209744</td>\n      <td>0.141981</td>\n      <td>-0.156686</td>\n      <td>0.060455</td>\n      <td>-0.225565</td>\n      <td>-0.123161</td>\n      <td>-0.345784</td>\n      <td>0.573043</td>\n      <td>0.070739</td>\n      <td>0.023572</td>\n      <td>0.294897</td>\n      <td>-0.022959</td>\n      <td>-0.236122</td>\n      <td>0.465324</td>\n      <td>-0.121645</td>\n      <td>-0.267481</td>\n      <td>-0.087629</td>\n      <td>-0.226174</td>\n      <td>-0.137586</td>\n      <td>0.026345</td>\n      <td>-0.295932</td>\n      <td>0.064476</td>\n      <td>0.089143</td>\n      <td>-0.042295</td>\n      <td>-0.046266</td>\n      <td>-0.238839</td>\n      <td>0.281470</td>\n      <td>0.001068</td>\n      <td>-0.169031</td>\n      <td>0.017892</td>\n      <td>0.347520</td>\n      <td>0.114027</td>\n      <td>0.026536</td>\n      <td>0.539894</td>\n      <td>0.258185</td>\n      <td>0.217490</td>\n      <td>-0.030520</td>\n      <td>0.407385</td>\n      <td>0.278295</td>\n      <td>0.330414</td>\n      <td>-0.487602</td>\n      <td>-0.046118</td>\n      <td>-0.106195</td>\n      <td>0.018743</td>\n      <td>-0.328588</td>\n      <td>-0.053319</td>\n      <td>0.127363</td>\n      <td>-0.132542</td>\n      <td>-0.227554</td>\n      <td>0.057860</td>\n      <td>0.112070</td>\n      <td>-0.000287</td>\n      <td>0.030610</td>\n      <td>0.028930</td>\n      <td>0.198467</td>\n      <td>-0.303935</td>\n      <td>-0.429734</td>\n      <td>-0.318576</td>\n      <td>-0.224360</td>\n      <td>-0.187954</td>\n      <td>0.573480</td>\n      <td>-0.117913</td>\n      <td>-0.152062</td>\n      <td>0.116279</td>\n      <td>-0.055709</td>\n      <td>0.393670</td>\n      <td>0.018342</td>\n      <td>-0.287295</td>\n      <td>-0.118957</td>\n      <td>-0.241666</td>\n      <td>-0.060184</td>\n      <td>-0.568609</td>\n      <td>0.229785</td>\n      <td>-0.164359</td>\n      <td>0.106208</td>\n      <td>0.025059</td>\n      <td>0.109468</td>\n      <td>0.230713</td>\n      <td>-0.120605</td>\n      <td>-0.185707</td>\n      <td>1.030239</td>\n      <td>-0.072780</td>\n      <td>-0.116536</td>\n      <td>0.109835</td>\n      <td>0.420646</td>\n      <td>0.176525</td>\n      <td>-0.161021</td>\n      <td>0.563187</td>\n      <td>-0.015506</td>\n      <td>0.246615</td>\n      <td>0.554169</td>\n      <td>-0.086021</td>\n      <td>0.106225</td>\n      <td>-0.002149</td>\n      <td>0.049488</td>\n      <td>-0.074861</td>\n      <td>0.226262</td>\n      <td>-0.123194</td>\n      <td>-0.047453</td>\n      <td>-0.456172</td>\n      <td>-0.090524</td>\n      <td>0.097732</td>\n      <td>-0.551270</td>\n      <td>0.288022</td>\n      <td>0.383723</td>\n      <td>-0.329955</td>\n      <td>0.030013</td>\n      <td>-0.066330</td>\n      <td>0.022795</td>\n      <td>0.353780</td>\n      <td>-0.121599</td>\n      <td>-0.254203</td>\n      <td>0.112318</td>\n      <td>0.251033</td>\n      <td>0.343733</td>\n      <td>-0.052206</td>\n      <td>0.339885</td>\n      <td>-0.014592</td>\n      <td>0.151311</td>\n      <td>-0.416623</td>\n      <td>0.003609</td>\n      <td>0.870064</td>\n      <td>0.265041</td>\n      <td>-0.280053</td>\n      <td>0.256732</td>\n      <td>0.207401</td>\n      <td>-0.444582</td>\n      <td>0.219045</td>\n      <td>-0.324177</td>\n      <td>0.179530</td>\n      <td>0.236902</td>\n      <td>-0.187423</td>\n      <td>-0.134858</td>\n      <td>0.135275</td>\n      <td>-0.198047</td>\n      <td>0.062975</td>\n      <td>-0.374228</td>\n      <td>0.195483</td>\n      <td>0.432166</td>\n      <td>0.604630</td>\n      <td>0.213389</td>\n      <td>0.416887</td>\n      <td>0.012604</td>\n      <td>-0.006210</td>\n      <td>0.177151</td>\n      <td>0.329447</td>\n      <td>0.002061</td>\n      <td>-0.324079</td>\n      <td>-0.032845</td>\n      <td>-0.135431</td>\n      <td>0.275498</td>\n      <td>0.048453</td>\n      <td>0.105030</td>\n      <td>-0.149223</td>\n      <td>-0.079357</td>\n      <td>-0.132657</td>\n      <td>0.281119</td>\n      <td>-0.147471</td>\n      <td>-0.231247</td>\n      <td>0.074115</td>\n      <td>0.237074</td>\n      <td>0.002784</td>\n      <td>0.092143</td>\n      <td>0.229385</td>\n      <td>-0.262094</td>\n      <td>-0.150538</td>\n      <td>-0.092701</td>\n      <td>0.318429</td>\n      <td>-0.150850</td>\n      <td>-0.155615</td>\n      <td>0.087423</td>\n      <td>-0.293090</td>\n      <td>0.104949</td>\n      <td>0.332403</td>\n      <td>-0.032639</td>\n      <td>-0.158297</td>\n      <td>-0.066522</td>\n      <td>0.215349</td>\n      <td>0.188424</td>\n      <td>-0.257978</td>\n      <td>0.405168</td>\n      <td>0.340767</td>\n      <td>-0.291075</td>\n      <td>0.083339</td>\n      <td>-0.212573</td>\n      <td>-0.018753</td>\n      <td>0.049122</td>\n      <td>0.186532</td>\n      <td>0.170497</td>\n      <td>0.092862</td>\n      <td>0.199120</td>\n      <td>-0.128425</td>\n      <td>-0.608483</td>\n      <td>-0.163588</td>\n      <td>0.001773</td>\n      <td>0.271846</td>\n      <td>-0.197258</td>\n      <td>0.571889</td>\n      <td>0.346139</td>\n      <td>-0.123787</td>\n      <td>-0.153672</td>\n      <td>-0.341038</td>\n      <td>0.144858</td>\n      <td>0.083497</td>\n      <td>0.014404</td>\n      <td>0.018678</td>\n      <td>-0.058541</td>\n      <td>0.727416</td>\n      <td>0.259999</td>\n      <td>-0.198122</td>\n      <td>0.040902</td>\n      <td>0.029725</td>\n      <td>0.345610</td>\n      <td>0.094131</td>\n      <td>-0.155435</td>\n      <td>0.433426</td>\n      <td>-0.392165</td>\n      <td>-0.310089</td>\n      <td>0.239597</td>\n      <td>0.195762</td>\n      <td>-0.073282</td>\n      <td>-0.149105</td>\n      <td>-0.063516</td>\n      <td>0.303675</td>\n      <td>0.072645</td>\n      <td>0.361633</td>\n      <td>0.030400</td>\n      <td>-0.193240</td>\n      <td>0.012310</td>\n      <td>-0.078675</td>\n      <td>-0.331328</td>\n      <td>0.343445</td>\n      <td>0.549831</td>\n      <td>0.290103</td>\n      <td>0.043345</td>\n      <td>0.096760</td>\n      <td>-0.080240</td>\n      <td>0.038742</td>\n      <td>0.273721</td>\n      <td>0.105223</td>\n      <td>-0.144940</td>\n      <td>0.151577</td>\n      <td>0.046810</td>\n      <td>-0.096304</td>\n      <td>0.025512</td>\n      <td>0.089983</td>\n      <td>-0.470928</td>\n      <td>0.032236</td>\n      <td>0.239439</td>\n      <td>-0.034409</td>\n      <td>0.438450</td>\n      <td>0.103597</td>\n      <td>0.119883</td>\n      <td>-0.125388</td>\n      <td>0.235254</td>\n      <td>-0.173017</td>\n      <td>-0.535687</td>\n      <td>0.048290</td>\n      <td>0.673012</td>\n      <td>0.112622</td>\n      <td>0.286671</td>\n      <td>0.374532</td>\n      <td>-0.131838</td>\n      <td>-0.038065</td>\n      <td>0.176448</td>\n      <td>0.403942</td>\n      <td>-0.103302</td>\n      <td>0.261344</td>\n      <td>-0.265923</td>\n      <td>-0.165292</td>\n      <td>-0.370454</td>\n      <td>0.072731</td>\n      <td>0.403832</td>\n      <td>0.110122</td>\n      <td>0.219321</td>\n      <td>-0.050819</td>\n      <td>-0.326276</td>\n      <td>-0.176302</td>\n      <td>0.094549</td>\n      <td>-0.181081</td>\n      <td>-0.145063</td>\n      <td>-0.173627</td>\n      <td>-0.119632</td>\n      <td>-0.084858</td>\n      <td>0.136829</td>\n      <td>-0.150102</td>\n      <td>-0.236805</td>\n      <td>0.193063</td>\n      <td>-0.299987</td>\n      <td>0.049597</td>\n      <td>-0.011547</td>\n      <td>0.023055</td>\n      <td>0.704938</td>\n      <td>0.103297</td>\n      <td>-0.091895</td>\n      <td>-0.375727</td>\n      <td>-0.055390</td>\n      <td>0.169564</td>\n      <td>0.190825</td>\n      <td>0.260881</td>\n      <td>-0.111916</td>\n      <td>-0.179431</td>\n      <td>0.291061</td>\n      <td>0.243255</td>\n      <td>0.362094</td>\n      <td>0.010315</td>\n      <td>0.038279</td>\n      <td>0.076084</td>\n      <td>0.139378</td>\n      <td>-0.019313</td>\n      <td>-0.025637</td>\n      <td>0.159266</td>\n      <td>-0.130484</td>\n      <td>0.000459</td>\n      <td>-0.151011</td>\n    </tr>\n    <tr>\n      <th>5800</th>\n      <td>-0.132067</td>\n      <td>0.130876</td>\n      <td>0.161276</td>\n      <td>-0.091357</td>\n      <td>0.177120</td>\n      <td>-0.033335</td>\n      <td>-0.097726</td>\n      <td>0.020873</td>\n      <td>0.154972</td>\n      <td>-0.165567</td>\n      <td>-0.088746</td>\n      <td>0.181572</td>\n      <td>0.051574</td>\n      <td>-0.242877</td>\n      <td>-0.129602</td>\n      <td>-0.295245</td>\n      <td>0.023770</td>\n      <td>0.104712</td>\n      <td>0.456264</td>\n      <td>0.377980</td>\n      <td>-0.022440</td>\n      <td>-1.012767</td>\n      <td>-0.115168</td>\n      <td>-0.154799</td>\n      <td>-0.013734</td>\n      <td>-0.019550</td>\n      <td>-0.111024</td>\n      <td>-0.095495</td>\n      <td>0.119128</td>\n      <td>0.018157</td>\n      <td>0.098169</td>\n      <td>0.296869</td>\n      <td>-0.408826</td>\n      <td>-0.054825</td>\n      <td>0.289302</td>\n      <td>-0.160595</td>\n      <td>-0.025372</td>\n      <td>0.195634</td>\n      <td>0.373598</td>\n      <td>-0.034114</td>\n      <td>0.157825</td>\n      <td>-0.285136</td>\n      <td>0.280263</td>\n      <td>-0.185379</td>\n      <td>0.250970</td>\n      <td>-0.173470</td>\n      <td>0.258732</td>\n      <td>-0.048782</td>\n      <td>0.241807</td>\n      <td>0.163020</td>\n      <td>-0.039766</td>\n      <td>0.158218</td>\n      <td>0.154478</td>\n      <td>0.100481</td>\n      <td>-0.067844</td>\n      <td>-0.520916</td>\n      <td>-0.175131</td>\n      <td>0.567349</td>\n      <td>0.479005</td>\n      <td>0.281838</td>\n      <td>-0.048371</td>\n      <td>-0.053719</td>\n      <td>0.242180</td>\n      <td>0.104336</td>\n      <td>0.583708</td>\n      <td>0.062588</td>\n      <td>0.038784</td>\n      <td>-0.175716</td>\n      <td>0.007910</td>\n      <td>0.294931</td>\n      <td>-0.335829</td>\n      <td>-0.314827</td>\n      <td>0.064629</td>\n      <td>0.160302</td>\n      <td>-0.440031</td>\n      <td>0.019835</td>\n      <td>0.127358</td>\n      <td>0.204301</td>\n      <td>-0.200790</td>\n      <td>-0.045241</td>\n      <td>0.065962</td>\n      <td>0.035095</td>\n      <td>-0.090812</td>\n      <td>0.186240</td>\n      <td>0.161651</td>\n      <td>-0.030852</td>\n      <td>0.008009</td>\n      <td>0.067464</td>\n      <td>0.032515</td>\n      <td>-0.005720</td>\n      <td>0.236870</td>\n      <td>0.134924</td>\n      <td>0.007866</td>\n      <td>-0.160228</td>\n      <td>-0.040366</td>\n      <td>0.110597</td>\n      <td>-0.153580</td>\n      <td>-0.095041</td>\n      <td>-0.267969</td>\n      <td>0.342028</td>\n      <td>-0.426397</td>\n      <td>-0.072382</td>\n      <td>0.169967</td>\n      <td>-0.087653</td>\n      <td>0.337757</td>\n      <td>0.300350</td>\n      <td>0.146923</td>\n      <td>-0.468155</td>\n      <td>-0.028496</td>\n      <td>0.233338</td>\n      <td>0.223446</td>\n      <td>-0.088716</td>\n      <td>0.001344</td>\n      <td>-0.096621</td>\n      <td>-0.337288</td>\n      <td>-0.244056</td>\n      <td>0.009975</td>\n      <td>0.116920</td>\n      <td>-0.088993</td>\n      <td>-0.106189</td>\n      <td>0.322011</td>\n      <td>0.186147</td>\n      <td>-0.664997</td>\n      <td>-0.174551</td>\n      <td>-0.060549</td>\n      <td>0.402668</td>\n      <td>0.002570</td>\n      <td>-0.202801</td>\n      <td>0.404086</td>\n      <td>0.078558</td>\n      <td>0.159959</td>\n      <td>0.098288</td>\n      <td>-0.013981</td>\n      <td>0.140102</td>\n      <td>0.162698</td>\n      <td>0.290902</td>\n      <td>0.089186</td>\n      <td>-0.076557</td>\n      <td>0.244866</td>\n      <td>0.184985</td>\n      <td>-0.065959</td>\n      <td>0.253094</td>\n      <td>0.277447</td>\n      <td>0.033763</td>\n      <td>0.187806</td>\n      <td>0.144026</td>\n      <td>-0.243744</td>\n      <td>0.098339</td>\n      <td>0.280649</td>\n      <td>0.232030</td>\n      <td>0.038787</td>\n      <td>0.228194</td>\n      <td>-0.126166</td>\n      <td>-0.153720</td>\n      <td>0.204803</td>\n      <td>0.084816</td>\n      <td>-0.047566</td>\n      <td>0.060588</td>\n      <td>-0.060030</td>\n      <td>-0.501611</td>\n      <td>-0.228924</td>\n      <td>0.144340</td>\n      <td>0.072455</td>\n      <td>0.276446</td>\n      <td>-0.021490</td>\n      <td>-0.098037</td>\n      <td>0.026923</td>\n      <td>0.175153</td>\n      <td>0.217400</td>\n      <td>-0.435691</td>\n      <td>0.362069</td>\n      <td>0.494104</td>\n      <td>-0.259501</td>\n      <td>-0.152133</td>\n      <td>-0.308774</td>\n      <td>0.333595</td>\n      <td>0.026328</td>\n      <td>0.025571</td>\n      <td>0.240006</td>\n      <td>0.261135</td>\n      <td>0.237247</td>\n      <td>-0.114374</td>\n      <td>0.177882</td>\n      <td>-0.342713</td>\n      <td>0.095928</td>\n      <td>0.294129</td>\n      <td>-0.687883</td>\n      <td>0.136921</td>\n      <td>0.194213</td>\n      <td>-0.385504</td>\n      <td>-0.464953</td>\n      <td>-0.204849</td>\n      <td>0.316583</td>\n      <td>0.259625</td>\n      <td>0.422941</td>\n      <td>0.163696</td>\n      <td>-0.289955</td>\n      <td>0.029273</td>\n      <td>-0.083988</td>\n      <td>-0.105565</td>\n      <td>0.140017</td>\n      <td>0.079198</td>\n      <td>-0.070238</td>\n      <td>0.103108</td>\n      <td>-0.087865</td>\n      <td>0.212782</td>\n      <td>0.012427</td>\n      <td>0.124866</td>\n      <td>0.045378</td>\n      <td>-0.155994</td>\n      <td>0.114740</td>\n      <td>-0.079507</td>\n      <td>0.149586</td>\n      <td>-0.080360</td>\n      <td>-0.142490</td>\n      <td>-3.402281</td>\n      <td>-0.171687</td>\n      <td>-0.022906</td>\n      <td>0.009940</td>\n      <td>-0.019952</td>\n      <td>-0.262242</td>\n      <td>0.195969</td>\n      <td>0.086375</td>\n      <td>-0.071964</td>\n      <td>0.165730</td>\n      <td>-0.217202</td>\n      <td>0.193918</td>\n      <td>0.262841</td>\n      <td>-0.326173</td>\n      <td>0.191733</td>\n      <td>0.359744</td>\n      <td>0.303201</td>\n      <td>0.045664</td>\n      <td>-0.253745</td>\n      <td>0.068239</td>\n      <td>0.116238</td>\n      <td>-0.234616</td>\n      <td>0.374068</td>\n      <td>-0.273942</td>\n      <td>-0.280154</td>\n      <td>0.394067</td>\n      <td>0.102716</td>\n      <td>0.466559</td>\n      <td>0.051316</td>\n      <td>0.019489</td>\n      <td>0.026435</td>\n      <td>0.250856</td>\n      <td>0.625440</td>\n      <td>0.175520</td>\n      <td>0.157388</td>\n      <td>0.193710</td>\n      <td>-0.088733</td>\n      <td>0.110520</td>\n      <td>0.269308</td>\n      <td>0.355151</td>\n      <td>0.261032</td>\n      <td>0.224363</td>\n      <td>0.155667</td>\n      <td>0.210714</td>\n      <td>-0.098982</td>\n      <td>0.124875</td>\n      <td>0.313645</td>\n      <td>-0.194729</td>\n      <td>0.069003</td>\n      <td>0.049257</td>\n      <td>0.012012</td>\n      <td>0.250487</td>\n      <td>-0.059249</td>\n      <td>0.126176</td>\n      <td>0.022173</td>\n      <td>0.267386</td>\n      <td>-0.115375</td>\n      <td>-0.017114</td>\n      <td>-0.384084</td>\n      <td>0.336613</td>\n      <td>0.260834</td>\n      <td>-0.307693</td>\n      <td>0.555903</td>\n      <td>0.119331</td>\n      <td>-0.218577</td>\n      <td>0.545526</td>\n      <td>-0.493640</td>\n      <td>0.069068</td>\n      <td>-0.056986</td>\n      <td>0.183574</td>\n      <td>-0.165052</td>\n      <td>-0.226207</td>\n      <td>0.455939</td>\n      <td>-0.193798</td>\n      <td>0.653875</td>\n      <td>0.406409</td>\n      <td>-0.238414</td>\n      <td>-0.427574</td>\n      <td>0.214363</td>\n      <td>-0.214675</td>\n      <td>0.210685</td>\n      <td>-0.027837</td>\n      <td>0.016673</td>\n      <td>0.019637</td>\n      <td>-0.212047</td>\n      <td>0.330150</td>\n      <td>0.183370</td>\n      <td>-0.217040</td>\n      <td>0.009215</td>\n      <td>-0.141354</td>\n      <td>0.049000</td>\n      <td>0.003130</td>\n      <td>0.152152</td>\n      <td>-0.037681</td>\n      <td>-0.230806</td>\n      <td>-0.232363</td>\n      <td>-0.119546</td>\n      <td>0.704545</td>\n      <td>-0.009581</td>\n      <td>-0.037799</td>\n      <td>0.230507</td>\n      <td>-0.030165</td>\n      <td>0.248764</td>\n      <td>0.027655</td>\n      <td>0.059678</td>\n      <td>0.216176</td>\n      <td>0.060032</td>\n      <td>0.140843</td>\n      <td>-0.376290</td>\n      <td>-0.075518</td>\n      <td>-0.075188</td>\n      <td>0.020103</td>\n      <td>0.543174</td>\n      <td>-0.099281</td>\n      <td>-0.195389</td>\n      <td>0.038402</td>\n      <td>-0.135274</td>\n      <td>0.083009</td>\n      <td>-0.135363</td>\n      <td>-0.405671</td>\n      <td>0.093424</td>\n      <td>0.161085</td>\n      <td>0.123508</td>\n      <td>-0.354957</td>\n      <td>-0.134463</td>\n      <td>0.188011</td>\n      <td>0.229838</td>\n      <td>0.132700</td>\n      <td>-1.069505</td>\n      <td>0.373192</td>\n      <td>0.240243</td>\n      <td>-0.358311</td>\n      <td>0.544328</td>\n      <td>0.173348</td>\n      <td>-0.082199</td>\n      <td>0.358428</td>\n      <td>-0.342664</td>\n      <td>0.371657</td>\n      <td>0.042953</td>\n      <td>0.130622</td>\n      <td>0.415231</td>\n      <td>-0.026164</td>\n      <td>0.173615</td>\n      <td>-0.139402</td>\n      <td>0.210499</td>\n      <td>-0.817762</td>\n      <td>-0.040684</td>\n      <td>0.111804</td>\n      <td>0.409951</td>\n      <td>-0.075350</td>\n      <td>0.051719</td>\n      <td>0.048335</td>\n      <td>0.014924</td>\n      <td>0.132361</td>\n      <td>-0.348198</td>\n      <td>-0.190974</td>\n      <td>-0.102708</td>\n      <td>0.095096</td>\n      <td>0.235608</td>\n      <td>0.170968</td>\n      <td>0.016238</td>\n      <td>-0.762695</td>\n      <td>-1.575914</td>\n      <td>0.189854</td>\n      <td>0.332282</td>\n      <td>0.293242</td>\n      <td>0.073670</td>\n      <td>0.241289</td>\n      <td>-0.093960</td>\n      <td>-0.054013</td>\n      <td>0.085197</td>\n      <td>0.129575</td>\n      <td>0.185716</td>\n      <td>0.003847</td>\n      <td>0.454327</td>\n      <td>-0.189133</td>\n      <td>0.090189</td>\n      <td>-0.102188</td>\n      <td>-0.075440</td>\n      <td>0.384681</td>\n      <td>-0.263391</td>\n      <td>0.044862</td>\n      <td>0.283937</td>\n      <td>0.098191</td>\n      <td>0.082367</td>\n      <td>-0.012841</td>\n      <td>-0.011989</td>\n      <td>0.335970</td>\n      <td>-0.274440</td>\n      <td>0.668942</td>\n      <td>-0.021293</td>\n      <td>0.078891</td>\n      <td>0.444623</td>\n      <td>0.181930</td>\n      <td>-0.014003</td>\n      <td>0.063660</td>\n      <td>-0.121199</td>\n      <td>-0.429821</td>\n      <td>-0.061743</td>\n      <td>-0.004597</td>\n      <td>0.192542</td>\n      <td>0.180476</td>\n      <td>0.067924</td>\n      <td>0.292306</td>\n      <td>0.092897</td>\n      <td>0.079233</td>\n      <td>-0.142078</td>\n      <td>0.037863</td>\n      <td>0.170807</td>\n      <td>-0.144208</td>\n      <td>-0.068382</td>\n      <td>0.274768</td>\n      <td>-0.031804</td>\n      <td>0.060594</td>\n      <td>-0.124615</td>\n      <td>-0.241654</td>\n      <td>0.050052</td>\n      <td>0.159044</td>\n      <td>0.055887</td>\n      <td>0.356343</td>\n      <td>-0.195894</td>\n      <td>0.323094</td>\n      <td>0.046594</td>\n      <td>0.206693</td>\n      <td>0.420075</td>\n      <td>0.145882</td>\n      <td>0.007916</td>\n      <td>0.158233</td>\n      <td>-0.011257</td>\n      <td>-0.126677</td>\n      <td>4.767896</td>\n      <td>0.133264</td>\n      <td>0.097943</td>\n      <td>0.119734</td>\n      <td>0.389167</td>\n      <td>0.017960</td>\n      <td>0.301269</td>\n      <td>0.151791</td>\n      <td>0.030799</td>\n      <td>0.473789</td>\n      <td>-0.204830</td>\n      <td>0.194116</td>\n      <td>-0.335194</td>\n      <td>0.007883</td>\n      <td>-0.459456</td>\n      <td>0.333442</td>\n      <td>0.128281</td>\n      <td>-0.003074</td>\n      <td>0.208963</td>\n      <td>0.325823</td>\n      <td>-0.093666</td>\n      <td>0.519885</td>\n      <td>0.007528</td>\n      <td>-0.207777</td>\n      <td>-0.171610</td>\n      <td>-0.269769</td>\n      <td>-0.122947</td>\n      <td>-0.042812</td>\n      <td>-0.189893</td>\n      <td>0.178181</td>\n      <td>0.167261</td>\n      <td>-0.035295</td>\n      <td>-0.016297</td>\n      <td>-0.466228</td>\n      <td>0.400818</td>\n      <td>-0.044528</td>\n      <td>-0.086911</td>\n      <td>0.057404</td>\n      <td>0.592008</td>\n      <td>0.096207</td>\n      <td>-0.142642</td>\n      <td>0.501376</td>\n      <td>0.291177</td>\n      <td>0.088048</td>\n      <td>-0.181443</td>\n      <td>0.470153</td>\n      <td>0.336471</td>\n      <td>0.586060</td>\n      <td>-0.397033</td>\n      <td>-0.036363</td>\n      <td>-0.106999</td>\n      <td>0.120221</td>\n      <td>-0.254843</td>\n      <td>0.058020</td>\n      <td>0.228707</td>\n      <td>-0.120829</td>\n      <td>0.031817</td>\n      <td>0.174521</td>\n      <td>0.023215</td>\n      <td>0.060007</td>\n      <td>0.202050</td>\n      <td>-0.074929</td>\n      <td>0.286043</td>\n      <td>-0.148116</td>\n      <td>-0.487437</td>\n      <td>-0.210278</td>\n      <td>-0.154281</td>\n      <td>-0.062967</td>\n      <td>0.345353</td>\n      <td>0.013007</td>\n      <td>0.042138</td>\n      <td>0.055535</td>\n      <td>-0.071465</td>\n      <td>0.543413</td>\n      <td>0.073271</td>\n      <td>-0.163895</td>\n      <td>-0.116848</td>\n      <td>-0.344962</td>\n      <td>-0.054594</td>\n      <td>-0.683111</td>\n      <td>0.220683</td>\n      <td>-0.256915</td>\n      <td>0.155769</td>\n      <td>0.165155</td>\n      <td>0.228100</td>\n      <td>0.164370</td>\n      <td>-0.162400</td>\n      <td>-0.093309</td>\n      <td>1.319607</td>\n      <td>-0.204972</td>\n      <td>-0.223999</td>\n      <td>-0.173336</td>\n      <td>0.466747</td>\n      <td>0.174668</td>\n      <td>-0.383195</td>\n      <td>0.527666</td>\n      <td>0.005899</td>\n      <td>0.108961</td>\n      <td>0.258122</td>\n      <td>-0.123884</td>\n      <td>0.086010</td>\n      <td>0.139231</td>\n      <td>0.236788</td>\n      <td>0.032440</td>\n      <td>0.209263</td>\n      <td>-0.147246</td>\n      <td>0.070680</td>\n      <td>-0.451074</td>\n      <td>0.073525</td>\n      <td>0.032705</td>\n      <td>-0.471250</td>\n      <td>0.193761</td>\n      <td>0.423801</td>\n      <td>-0.320447</td>\n      <td>-0.001936</td>\n      <td>-0.077355</td>\n      <td>0.031141</td>\n      <td>0.391592</td>\n      <td>-0.190227</td>\n      <td>-0.397756</td>\n      <td>0.015962</td>\n      <td>0.286114</td>\n      <td>0.282336</td>\n      <td>-0.015202</td>\n      <td>0.438908</td>\n      <td>0.053051</td>\n      <td>0.230438</td>\n      <td>-0.459325</td>\n      <td>0.141847</td>\n      <td>0.486188</td>\n      <td>0.189290</td>\n      <td>-0.203692</td>\n      <td>0.314315</td>\n      <td>-0.015356</td>\n      <td>-0.358939</td>\n      <td>0.285941</td>\n      <td>-0.453243</td>\n      <td>0.167612</td>\n      <td>0.204932</td>\n      <td>-0.391239</td>\n      <td>-0.266462</td>\n      <td>0.154510</td>\n      <td>-0.238417</td>\n      <td>0.159891</td>\n      <td>-0.247684</td>\n      <td>0.226233</td>\n      <td>0.619110</td>\n      <td>0.569341</td>\n      <td>0.281572</td>\n      <td>0.477477</td>\n      <td>-0.017737</td>\n      <td>0.070347</td>\n      <td>0.256809</td>\n      <td>0.374816</td>\n      <td>-0.079692</td>\n      <td>-0.242586</td>\n      <td>0.044298</td>\n      <td>-0.281989</td>\n      <td>0.387108</td>\n      <td>0.023107</td>\n      <td>0.019294</td>\n      <td>-0.154033</td>\n      <td>-0.007949</td>\n      <td>0.009666</td>\n      <td>0.219392</td>\n      <td>-0.152540</td>\n      <td>-0.112072</td>\n      <td>-0.053614</td>\n      <td>0.355407</td>\n      <td>-0.207847</td>\n      <td>0.027902</td>\n      <td>0.155566</td>\n      <td>-0.047144</td>\n      <td>0.112756</td>\n      <td>-0.182509</td>\n      <td>0.404179</td>\n      <td>-0.145474</td>\n      <td>0.066371</td>\n      <td>0.111475</td>\n      <td>-0.147998</td>\n      <td>0.081463</td>\n      <td>0.169748</td>\n      <td>-0.195488</td>\n      <td>-0.160839</td>\n      <td>-0.012413</td>\n      <td>0.260353</td>\n      <td>0.280414</td>\n      <td>-0.403983</td>\n      <td>0.257666</td>\n      <td>0.366892</td>\n      <td>-0.332332</td>\n      <td>0.206630</td>\n      <td>-0.129587</td>\n      <td>0.262763</td>\n      <td>0.099482</td>\n      <td>0.121180</td>\n      <td>0.104501</td>\n      <td>0.206057</td>\n      <td>0.201226</td>\n      <td>-0.049926</td>\n      <td>-0.465961</td>\n      <td>-0.117912</td>\n      <td>0.020375</td>\n      <td>0.210493</td>\n      <td>-0.102061</td>\n      <td>0.412956</td>\n      <td>0.280114</td>\n      <td>-0.191353</td>\n      <td>-0.287512</td>\n      <td>-0.391526</td>\n      <td>0.228060</td>\n      <td>0.285010</td>\n      <td>-0.124472</td>\n      <td>-0.063915</td>\n      <td>-0.119393</td>\n      <td>0.694497</td>\n      <td>0.036722</td>\n      <td>0.071856</td>\n      <td>-0.063877</td>\n      <td>0.001684</td>\n      <td>0.382080</td>\n      <td>0.052258</td>\n      <td>-0.149866</td>\n      <td>0.377124</td>\n      <td>-0.222215</td>\n      <td>-0.619077</td>\n      <td>0.366178</td>\n      <td>0.157134</td>\n      <td>-0.270881</td>\n      <td>-0.206389</td>\n      <td>-0.166507</td>\n      <td>0.160963</td>\n      <td>0.162765</td>\n      <td>0.092605</td>\n      <td>0.019058</td>\n      <td>0.006810</td>\n      <td>-0.135174</td>\n      <td>-0.019600</td>\n      <td>-0.420353</td>\n      <td>0.131304</td>\n      <td>0.561871</td>\n      <td>0.247528</td>\n      <td>0.066850</td>\n      <td>0.127000</td>\n      <td>-0.080651</td>\n      <td>0.147129</td>\n      <td>0.270234</td>\n      <td>-0.103363</td>\n      <td>0.000844</td>\n      <td>0.123820</td>\n      <td>0.036597</td>\n      <td>-0.056737</td>\n      <td>0.096311</td>\n      <td>0.077604</td>\n      <td>-0.522206</td>\n      <td>-0.005860</td>\n      <td>0.406634</td>\n      <td>-0.045904</td>\n      <td>0.498449</td>\n      <td>0.011239</td>\n      <td>-0.000838</td>\n      <td>-0.099138</td>\n      <td>0.137752</td>\n      <td>-0.095763</td>\n      <td>-0.411436</td>\n      <td>-0.142437</td>\n      <td>0.615729</td>\n      <td>0.056552</td>\n      <td>0.234512</td>\n      <td>0.408369</td>\n      <td>-0.038255</td>\n      <td>-0.212576</td>\n      <td>0.263245</td>\n      <td>0.209351</td>\n      <td>0.047378</td>\n      <td>0.435127</td>\n      <td>-0.341885</td>\n      <td>-0.063315</td>\n      <td>-0.395247</td>\n      <td>-0.124030</td>\n      <td>0.500349</td>\n      <td>0.108620</td>\n      <td>0.200862</td>\n      <td>0.003457</td>\n      <td>-0.393931</td>\n      <td>-0.069050</td>\n      <td>0.066490</td>\n      <td>-0.147501</td>\n      <td>-0.083016</td>\n      <td>-0.099370</td>\n      <td>-0.309697</td>\n      <td>-0.011674</td>\n      <td>0.008094</td>\n      <td>-0.136677</td>\n      <td>-0.269840</td>\n      <td>0.087978</td>\n      <td>-0.218604</td>\n      <td>0.006561</td>\n      <td>0.062727</td>\n      <td>-0.196419</td>\n      <td>0.476781</td>\n      <td>0.156788</td>\n      <td>-0.185793</td>\n      <td>-0.044427</td>\n      <td>-0.048074</td>\n      <td>0.399349</td>\n      <td>0.035793</td>\n      <td>0.137853</td>\n      <td>-0.111256</td>\n      <td>-0.155004</td>\n      <td>0.352759</td>\n      <td>0.130666</td>\n      <td>0.344784</td>\n      <td>0.169702</td>\n      <td>0.175369</td>\n      <td>-0.121725</td>\n      <td>0.040599</td>\n      <td>-0.032955</td>\n      <td>-0.053189</td>\n      <td>0.147725</td>\n      <td>-0.009660</td>\n      <td>-0.067827</td>\n      <td>-0.001753</td>\n    </tr>\n    <tr>\n      <th>5801</th>\n      <td>-0.197204</td>\n      <td>0.164000</td>\n      <td>0.139933</td>\n      <td>-0.060349</td>\n      <td>0.123922</td>\n      <td>-0.014345</td>\n      <td>0.071361</td>\n      <td>0.003858</td>\n      <td>-0.048259</td>\n      <td>-0.140171</td>\n      <td>0.033974</td>\n      <td>0.258364</td>\n      <td>0.103917</td>\n      <td>-0.137829</td>\n      <td>-0.077513</td>\n      <td>-0.288034</td>\n      <td>0.224880</td>\n      <td>0.034143</td>\n      <td>0.464200</td>\n      <td>0.316458</td>\n      <td>0.040942</td>\n      <td>-0.845762</td>\n      <td>-0.172125</td>\n      <td>-0.152267</td>\n      <td>0.036629</td>\n      <td>0.016338</td>\n      <td>-0.086001</td>\n      <td>-0.066104</td>\n      <td>-0.223363</td>\n      <td>0.050770</td>\n      <td>0.040453</td>\n      <td>0.217819</td>\n      <td>-0.506178</td>\n      <td>-0.243926</td>\n      <td>0.271712</td>\n      <td>-0.013115</td>\n      <td>0.052959</td>\n      <td>0.190609</td>\n      <td>0.313460</td>\n      <td>-0.014070</td>\n      <td>0.126671</td>\n      <td>-0.148991</td>\n      <td>0.284928</td>\n      <td>-0.309423</td>\n      <td>0.043310</td>\n      <td>0.076997</td>\n      <td>0.251565</td>\n      <td>-0.117050</td>\n      <td>0.296240</td>\n      <td>0.189658</td>\n      <td>0.095776</td>\n      <td>0.162875</td>\n      <td>0.072554</td>\n      <td>0.026161</td>\n      <td>-0.056362</td>\n      <td>-0.711017</td>\n      <td>-0.245344</td>\n      <td>0.379634</td>\n      <td>0.470131</td>\n      <td>0.435069</td>\n      <td>-0.094786</td>\n      <td>-0.113622</td>\n      <td>0.149384</td>\n      <td>0.248887</td>\n      <td>0.532551</td>\n      <td>0.023685</td>\n      <td>0.014519</td>\n      <td>-0.213488</td>\n      <td>-0.015437</td>\n      <td>0.261454</td>\n      <td>-0.230526</td>\n      <td>-0.432995</td>\n      <td>0.091653</td>\n      <td>0.128027</td>\n      <td>-0.456513</td>\n      <td>0.069084</td>\n      <td>0.069847</td>\n      <td>0.256042</td>\n      <td>-0.228045</td>\n      <td>-0.113325</td>\n      <td>0.060683</td>\n      <td>-0.016665</td>\n      <td>-0.096138</td>\n      <td>0.230524</td>\n      <td>-0.026908</td>\n      <td>-0.064551</td>\n      <td>0.111085</td>\n      <td>0.080778</td>\n      <td>0.215236</td>\n      <td>-0.077411</td>\n      <td>0.328641</td>\n      <td>-0.005708</td>\n      <td>-0.059052</td>\n      <td>-0.279319</td>\n      <td>-0.117145</td>\n      <td>0.042483</td>\n      <td>-0.150987</td>\n      <td>-0.078925</td>\n      <td>-0.311772</td>\n      <td>0.413753</td>\n      <td>-0.187577</td>\n      <td>-0.000576</td>\n      <td>0.099592</td>\n      <td>0.015164</td>\n      <td>0.398611</td>\n      <td>0.248995</td>\n      <td>0.068000</td>\n      <td>-0.479217</td>\n      <td>-0.156375</td>\n      <td>0.227330</td>\n      <td>0.268555</td>\n      <td>-0.112749</td>\n      <td>-0.091831</td>\n      <td>-0.100524</td>\n      <td>-0.354948</td>\n      <td>-0.224731</td>\n      <td>0.095654</td>\n      <td>0.094812</td>\n      <td>0.075904</td>\n      <td>-0.190370</td>\n      <td>0.235684</td>\n      <td>0.170623</td>\n      <td>-0.650336</td>\n      <td>0.037762</td>\n      <td>-0.072813</td>\n      <td>0.355756</td>\n      <td>0.054512</td>\n      <td>-0.197220</td>\n      <td>0.676223</td>\n      <td>0.198135</td>\n      <td>0.163415</td>\n      <td>0.002879</td>\n      <td>-0.026995</td>\n      <td>0.129614</td>\n      <td>0.276480</td>\n      <td>0.305983</td>\n      <td>0.078695</td>\n      <td>-0.060980</td>\n      <td>0.213617</td>\n      <td>-0.044270</td>\n      <td>-0.009202</td>\n      <td>0.336505</td>\n      <td>0.151871</td>\n      <td>0.121205</td>\n      <td>0.133944</td>\n      <td>0.229626</td>\n      <td>-0.163213</td>\n      <td>0.205002</td>\n      <td>0.232031</td>\n      <td>0.258762</td>\n      <td>-0.024485</td>\n      <td>0.357546</td>\n      <td>-0.092322</td>\n      <td>-0.142572</td>\n      <td>-0.054280</td>\n      <td>0.212741</td>\n      <td>0.010158</td>\n      <td>0.020578</td>\n      <td>0.012151</td>\n      <td>-0.427382</td>\n      <td>-0.293360</td>\n      <td>0.094190</td>\n      <td>0.105551</td>\n      <td>0.326485</td>\n      <td>-0.053303</td>\n      <td>-0.145053</td>\n      <td>0.058412</td>\n      <td>0.168415</td>\n      <td>0.175386</td>\n      <td>-0.429844</td>\n      <td>0.263693</td>\n      <td>0.454047</td>\n      <td>-0.243650</td>\n      <td>-0.181065</td>\n      <td>-0.306258</td>\n      <td>0.399988</td>\n      <td>0.048889</td>\n      <td>0.003095</td>\n      <td>0.274596</td>\n      <td>0.384392</td>\n      <td>0.211066</td>\n      <td>-0.123679</td>\n      <td>0.178520</td>\n      <td>-0.333539</td>\n      <td>0.216782</td>\n      <td>0.412941</td>\n      <td>-0.598766</td>\n      <td>0.077580</td>\n      <td>0.121292</td>\n      <td>-0.331019</td>\n      <td>-0.369021</td>\n      <td>-0.134539</td>\n      <td>0.265330</td>\n      <td>0.135050</td>\n      <td>0.401057</td>\n      <td>0.161166</td>\n      <td>-0.207191</td>\n      <td>0.073240</td>\n      <td>-0.052879</td>\n      <td>-0.067090</td>\n      <td>0.078067</td>\n      <td>0.041606</td>\n      <td>-0.030979</td>\n      <td>0.001576</td>\n      <td>-0.187513</td>\n      <td>0.249277</td>\n      <td>0.168450</td>\n      <td>0.103464</td>\n      <td>0.004027</td>\n      <td>-0.265379</td>\n      <td>0.143745</td>\n      <td>-0.100512</td>\n      <td>0.011362</td>\n      <td>-0.050863</td>\n      <td>-0.113679</td>\n      <td>-3.527609</td>\n      <td>-0.210857</td>\n      <td>-0.010124</td>\n      <td>0.065307</td>\n      <td>0.032298</td>\n      <td>-0.271220</td>\n      <td>0.201200</td>\n      <td>0.160675</td>\n      <td>-0.161122</td>\n      <td>0.035896</td>\n      <td>-0.274122</td>\n      <td>-0.025667</td>\n      <td>0.114912</td>\n      <td>-0.393177</td>\n      <td>0.167970</td>\n      <td>0.272051</td>\n      <td>0.297932</td>\n      <td>-0.112869</td>\n      <td>-0.257049</td>\n      <td>0.056138</td>\n      <td>0.131760</td>\n      <td>-0.207525</td>\n      <td>0.321884</td>\n      <td>-0.131013</td>\n      <td>-0.310836</td>\n      <td>0.271946</td>\n      <td>-0.006948</td>\n      <td>0.470541</td>\n      <td>0.207552</td>\n      <td>0.068628</td>\n      <td>0.026041</td>\n      <td>0.351575</td>\n      <td>0.748866</td>\n      <td>0.109570</td>\n      <td>0.100211</td>\n      <td>0.357095</td>\n      <td>-0.090962</td>\n      <td>0.079302</td>\n      <td>0.083402</td>\n      <td>0.467054</td>\n      <td>0.278471</td>\n      <td>0.216635</td>\n      <td>0.032652</td>\n      <td>0.211026</td>\n      <td>0.018271</td>\n      <td>0.190264</td>\n      <td>0.456018</td>\n      <td>-0.095922</td>\n      <td>0.115357</td>\n      <td>0.090612</td>\n      <td>0.079463</td>\n      <td>0.348857</td>\n      <td>-0.064717</td>\n      <td>0.099276</td>\n      <td>-0.056905</td>\n      <td>0.305034</td>\n      <td>-0.368010</td>\n      <td>0.073342</td>\n      <td>-0.295572</td>\n      <td>0.349705</td>\n      <td>0.319717</td>\n      <td>-0.536023</td>\n      <td>0.398251</td>\n      <td>0.087451</td>\n      <td>-0.327592</td>\n      <td>0.462104</td>\n      <td>-0.299350</td>\n      <td>0.012221</td>\n      <td>-0.027240</td>\n      <td>0.116132</td>\n      <td>-0.203560</td>\n      <td>-0.132911</td>\n      <td>0.329755</td>\n      <td>-0.191730</td>\n      <td>0.559172</td>\n      <td>0.280156</td>\n      <td>-0.176340</td>\n      <td>-0.422265</td>\n      <td>0.248192</td>\n      <td>-0.173052</td>\n      <td>0.414384</td>\n      <td>-0.044662</td>\n      <td>0.090228</td>\n      <td>0.020486</td>\n      <td>-0.225133</td>\n      <td>0.440259</td>\n      <td>0.175797</td>\n      <td>-0.346676</td>\n      <td>0.087021</td>\n      <td>-0.194521</td>\n      <td>0.119153</td>\n      <td>0.044320</td>\n      <td>0.251317</td>\n      <td>-0.032677</td>\n      <td>-0.297431</td>\n      <td>-0.249727</td>\n      <td>-0.158133</td>\n      <td>0.535342</td>\n      <td>-0.146624</td>\n      <td>-0.063998</td>\n      <td>0.302977</td>\n      <td>0.005873</td>\n      <td>0.106122</td>\n      <td>-0.038791</td>\n      <td>0.136180</td>\n      <td>0.209774</td>\n      <td>-0.014466</td>\n      <td>0.178056</td>\n      <td>-0.420407</td>\n      <td>0.081944</td>\n      <td>-0.129694</td>\n      <td>0.161161</td>\n      <td>0.574781</td>\n      <td>-0.092864</td>\n      <td>-0.030919</td>\n      <td>-0.202957</td>\n      <td>-0.108888</td>\n      <td>0.133909</td>\n      <td>-0.198606</td>\n      <td>-0.336170</td>\n      <td>0.137045</td>\n      <td>0.234870</td>\n      <td>-0.011704</td>\n      <td>-0.385778</td>\n      <td>-0.085587</td>\n      <td>0.227765</td>\n      <td>0.107466</td>\n      <td>0.296755</td>\n      <td>-1.083900</td>\n      <td>0.456709</td>\n      <td>0.227758</td>\n      <td>-0.425759</td>\n      <td>0.503818</td>\n      <td>0.275216</td>\n      <td>-0.120936</td>\n      <td>0.361518</td>\n      <td>-0.371402</td>\n      <td>0.531414</td>\n      <td>0.032738</td>\n      <td>0.071400</td>\n      <td>0.516245</td>\n      <td>-0.113635</td>\n      <td>0.087816</td>\n      <td>-0.195519</td>\n      <td>0.306917</td>\n      <td>-0.882524</td>\n      <td>0.126712</td>\n      <td>0.042555</td>\n      <td>0.366912</td>\n      <td>0.031172</td>\n      <td>-0.120738</td>\n      <td>0.112366</td>\n      <td>-0.030130</td>\n      <td>0.181367</td>\n      <td>-0.267578</td>\n      <td>-0.156898</td>\n      <td>-0.088706</td>\n      <td>0.259204</td>\n      <td>0.217878</td>\n      <td>0.243013</td>\n      <td>0.069513</td>\n      <td>-0.753294</td>\n      <td>-1.555932</td>\n      <td>0.227741</td>\n      <td>0.396136</td>\n      <td>0.499281</td>\n      <td>0.065283</td>\n      <td>0.187489</td>\n      <td>0.014420</td>\n      <td>0.000720</td>\n      <td>0.163176</td>\n      <td>0.210455</td>\n      <td>0.000319</td>\n      <td>-0.045226</td>\n      <td>0.426808</td>\n      <td>-0.122421</td>\n      <td>0.120248</td>\n      <td>-0.226760</td>\n      <td>-0.119434</td>\n      <td>0.107869</td>\n      <td>-0.360259</td>\n      <td>0.178765</td>\n      <td>0.219603</td>\n      <td>0.330241</td>\n      <td>0.114662</td>\n      <td>-0.062618</td>\n      <td>0.069053</td>\n      <td>0.230423</td>\n      <td>-0.304907</td>\n      <td>0.581747</td>\n      <td>0.201269</td>\n      <td>0.086544</td>\n      <td>0.303610</td>\n      <td>0.278336</td>\n      <td>-0.115036</td>\n      <td>0.096076</td>\n      <td>-0.113682</td>\n      <td>-0.229704</td>\n      <td>-0.037115</td>\n      <td>0.046386</td>\n      <td>0.272311</td>\n      <td>0.158997</td>\n      <td>0.098373</td>\n      <td>0.164840</td>\n      <td>0.107994</td>\n      <td>0.130272</td>\n      <td>-0.001865</td>\n      <td>0.055415</td>\n      <td>0.114537</td>\n      <td>-0.222612</td>\n      <td>-0.111380</td>\n      <td>0.273463</td>\n      <td>0.005236</td>\n      <td>0.048216</td>\n      <td>-0.162072</td>\n      <td>-0.169301</td>\n      <td>-0.034169</td>\n      <td>0.040331</td>\n      <td>0.215255</td>\n      <td>0.398604</td>\n      <td>-0.137986</td>\n      <td>0.154077</td>\n      <td>0.152590</td>\n      <td>0.188301</td>\n      <td>0.362347</td>\n      <td>0.127100</td>\n      <td>0.060738</td>\n      <td>0.090573</td>\n      <td>0.041136</td>\n      <td>0.025562</td>\n      <td>4.755297</td>\n      <td>0.104469</td>\n      <td>0.359572</td>\n      <td>0.067171</td>\n      <td>0.349734</td>\n      <td>-0.193996</td>\n      <td>0.463586</td>\n      <td>0.094244</td>\n      <td>0.072032</td>\n      <td>0.337075</td>\n      <td>-0.065795</td>\n      <td>0.054828</td>\n      <td>-0.313711</td>\n      <td>-0.012513</td>\n      <td>-0.260075</td>\n      <td>0.425665</td>\n      <td>0.153244</td>\n      <td>0.097730</td>\n      <td>0.182984</td>\n      <td>0.274651</td>\n      <td>-0.070109</td>\n      <td>0.506160</td>\n      <td>-0.084080</td>\n      <td>-0.289241</td>\n      <td>-0.109700</td>\n      <td>-0.218080</td>\n      <td>-0.122398</td>\n      <td>0.089358</td>\n      <td>-0.249233</td>\n      <td>0.068926</td>\n      <td>0.155156</td>\n      <td>-0.002891</td>\n      <td>-0.108785</td>\n      <td>-0.118666</td>\n      <td>0.297574</td>\n      <td>-0.016073</td>\n      <td>-0.213562</td>\n      <td>0.044780</td>\n      <td>0.422873</td>\n      <td>0.097924</td>\n      <td>0.078061</td>\n      <td>0.376828</td>\n      <td>0.193599</td>\n      <td>0.114676</td>\n      <td>-0.139354</td>\n      <td>0.426733</td>\n      <td>0.300406</td>\n      <td>0.556409</td>\n      <td>-0.505137</td>\n      <td>-0.129443</td>\n      <td>-0.098371</td>\n      <td>0.040871</td>\n      <td>-0.325016</td>\n      <td>0.066747</td>\n      <td>0.282382</td>\n      <td>-0.055999</td>\n      <td>-0.051167</td>\n      <td>0.130796</td>\n      <td>0.071629</td>\n      <td>0.006048</td>\n      <td>0.187473</td>\n      <td>-0.163047</td>\n      <td>0.118924</td>\n      <td>-0.132957</td>\n      <td>-0.440336</td>\n      <td>-0.117296</td>\n      <td>-0.221083</td>\n      <td>-0.114240</td>\n      <td>0.503192</td>\n      <td>-0.111579</td>\n      <td>0.021600</td>\n      <td>0.062059</td>\n      <td>-0.028263</td>\n      <td>0.479920</td>\n      <td>0.037583</td>\n      <td>-0.183235</td>\n      <td>-0.087970</td>\n      <td>-0.276075</td>\n      <td>-0.017940</td>\n      <td>-0.705892</td>\n      <td>0.294718</td>\n      <td>-0.137707</td>\n      <td>0.134199</td>\n      <td>0.138118</td>\n      <td>0.127715</td>\n      <td>0.259978</td>\n      <td>-0.172032</td>\n      <td>-0.174915</td>\n      <td>1.196315</td>\n      <td>-0.005313</td>\n      <td>-0.054269</td>\n      <td>-0.028721</td>\n      <td>0.501213</td>\n      <td>0.083360</td>\n      <td>-0.264184</td>\n      <td>0.587294</td>\n      <td>-0.020360</td>\n      <td>0.097729</td>\n      <td>0.458523</td>\n      <td>0.031435</td>\n      <td>0.117152</td>\n      <td>0.067868</td>\n      <td>0.162279</td>\n      <td>-0.025534</td>\n      <td>0.341441</td>\n      <td>-0.088403</td>\n      <td>-0.025945</td>\n      <td>-0.288340</td>\n      <td>0.073795</td>\n      <td>0.104680</td>\n      <td>-0.528187</td>\n      <td>0.165488</td>\n      <td>0.513850</td>\n      <td>-0.335499</td>\n      <td>0.026776</td>\n      <td>-0.173049</td>\n      <td>-0.011792</td>\n      <td>0.344168</td>\n      <td>-0.139736</td>\n      <td>-0.322970</td>\n      <td>0.063007</td>\n      <td>0.198765</td>\n      <td>0.285336</td>\n      <td>-0.104871</td>\n      <td>0.338772</td>\n      <td>0.141768</td>\n      <td>0.199322</td>\n      <td>-0.373704</td>\n      <td>0.005892</td>\n      <td>0.691010</td>\n      <td>0.159527</td>\n      <td>-0.196880</td>\n      <td>0.251185</td>\n      <td>0.007133</td>\n      <td>-0.421356</td>\n      <td>0.123509</td>\n      <td>-0.427793</td>\n      <td>0.082607</td>\n      <td>0.210277</td>\n      <td>-0.272839</td>\n      <td>-0.106038</td>\n      <td>0.269046</td>\n      <td>-0.218318</td>\n      <td>0.118495</td>\n      <td>-0.310512</td>\n      <td>0.423977</td>\n      <td>0.607719</td>\n      <td>0.487851</td>\n      <td>0.288233</td>\n      <td>0.419006</td>\n      <td>0.096749</td>\n      <td>0.039959</td>\n      <td>0.247011</td>\n      <td>0.278812</td>\n      <td>-0.077872</td>\n      <td>-0.285308</td>\n      <td>0.043751</td>\n      <td>-0.157213</td>\n      <td>0.405377</td>\n      <td>0.148745</td>\n      <td>-0.055537</td>\n      <td>-0.140283</td>\n      <td>-0.049370</td>\n      <td>-0.017800</td>\n      <td>0.065636</td>\n      <td>-0.107775</td>\n      <td>-0.216198</td>\n      <td>-0.157520</td>\n      <td>0.196929</td>\n      <td>-0.103377</td>\n      <td>0.036351</td>\n      <td>0.036444</td>\n      <td>-0.212506</td>\n      <td>-0.054762</td>\n      <td>-0.069451</td>\n      <td>0.182633</td>\n      <td>-0.157330</td>\n      <td>-0.063294</td>\n      <td>0.095219</td>\n      <td>-0.265767</td>\n      <td>0.235105</td>\n      <td>0.267084</td>\n      <td>-0.170242</td>\n      <td>-0.021914</td>\n      <td>-0.074998</td>\n      <td>0.136633</td>\n      <td>0.283889</td>\n      <td>-0.305920</td>\n      <td>0.225190</td>\n      <td>0.294437</td>\n      <td>-0.273334</td>\n      <td>0.122757</td>\n      <td>-0.099111</td>\n      <td>0.074864</td>\n      <td>0.087805</td>\n      <td>0.110155</td>\n      <td>0.109665</td>\n      <td>0.197439</td>\n      <td>0.249975</td>\n      <td>-0.095036</td>\n      <td>-0.567671</td>\n      <td>-0.089320</td>\n      <td>0.076010</td>\n      <td>0.128530</td>\n      <td>-0.195245</td>\n      <td>0.457287</td>\n      <td>0.374000</td>\n      <td>-0.136541</td>\n      <td>-0.173828</td>\n      <td>-0.324554</td>\n      <td>0.134303</td>\n      <td>0.250992</td>\n      <td>-0.064830</td>\n      <td>-0.059560</td>\n      <td>-0.053925</td>\n      <td>0.656420</td>\n      <td>0.131131</td>\n      <td>-0.170839</td>\n      <td>0.055130</td>\n      <td>-0.047512</td>\n      <td>0.317247</td>\n      <td>-0.002407</td>\n      <td>-0.277433</td>\n      <td>0.327366</td>\n      <td>-0.426331</td>\n      <td>-0.539502</td>\n      <td>0.316842</td>\n      <td>0.194402</td>\n      <td>-0.248386</td>\n      <td>-0.131194</td>\n      <td>-0.086684</td>\n      <td>0.263930</td>\n      <td>0.026052</td>\n      <td>0.221463</td>\n      <td>0.151201</td>\n      <td>-0.030184</td>\n      <td>-0.087996</td>\n      <td>-0.107575</td>\n      <td>-0.367033</td>\n      <td>0.280705</td>\n      <td>0.489565</td>\n      <td>0.242375</td>\n      <td>0.052886</td>\n      <td>0.073727</td>\n      <td>-0.108327</td>\n      <td>0.296458</td>\n      <td>0.177088</td>\n      <td>0.032822</td>\n      <td>0.088047</td>\n      <td>0.052572</td>\n      <td>0.065842</td>\n      <td>0.047565</td>\n      <td>-0.067014</td>\n      <td>-0.112564</td>\n      <td>-0.467437</td>\n      <td>-0.078100</td>\n      <td>0.287546</td>\n      <td>-0.018603</td>\n      <td>0.567209</td>\n      <td>-0.071385</td>\n      <td>0.002748</td>\n      <td>-0.144241</td>\n      <td>0.286929</td>\n      <td>-0.044578</td>\n      <td>-0.269166</td>\n      <td>-0.107868</td>\n      <td>0.433616</td>\n      <td>-0.044366</td>\n      <td>0.296187</td>\n      <td>0.462635</td>\n      <td>-0.014226</td>\n      <td>-0.194532</td>\n      <td>0.160193</td>\n      <td>0.358865</td>\n      <td>-0.085263</td>\n      <td>0.454523</td>\n      <td>-0.255431</td>\n      <td>-0.225070</td>\n      <td>-0.337939</td>\n      <td>0.007435</td>\n      <td>0.447520</td>\n      <td>0.180202</td>\n      <td>0.188728</td>\n      <td>-0.158013</td>\n      <td>-0.330933</td>\n      <td>-0.087565</td>\n      <td>0.088043</td>\n      <td>-0.256032</td>\n      <td>-0.124658</td>\n      <td>-0.146170</td>\n      <td>-0.309005</td>\n      <td>0.141499</td>\n      <td>0.119541</td>\n      <td>-0.066728</td>\n      <td>-0.395234</td>\n      <td>0.091152</td>\n      <td>-0.270415</td>\n      <td>-0.179587</td>\n      <td>-0.007913</td>\n      <td>-0.014999</td>\n      <td>0.625769</td>\n      <td>0.284496</td>\n      <td>-0.228505</td>\n      <td>-0.258184</td>\n      <td>0.057018</td>\n      <td>0.215825</td>\n      <td>0.065390</td>\n      <td>0.021443</td>\n      <td>-0.115814</td>\n      <td>-0.214083</td>\n      <td>0.272588</td>\n      <td>0.334561</td>\n      <td>0.198665</td>\n      <td>0.166565</td>\n      <td>0.197325</td>\n      <td>0.050572</td>\n      <td>0.129849</td>\n      <td>-0.003030</td>\n      <td>-0.006511</td>\n      <td>0.203633</td>\n      <td>-0.124874</td>\n      <td>-0.044585</td>\n      <td>-0.119854</td>\n    </tr>\n  </tbody>\n</table>\n<p>5802 rows × 768 columns</p>\n</div>",
            "text/plain": "      BERTEmbed_0  BERTEmbed_1  BERTEmbed_2  BERTEmbed_3  BERTEmbed_4  BERTEmbed_5  BERTEmbed_6  BERTEmbed_7  BERTEmbed_8  BERTEmbed_9  BERTEmbed_10  BERTEmbed_11  BERTEmbed_12  BERTEmbed_13  BERTEmbed_14  BERTEmbed_15  BERTEmbed_16  BERTEmbed_17  BERTEmbed_18  BERTEmbed_19  BERTEmbed_20  \\\n0       -0.112559     0.256139     0.171227    -0.076553     0.143061    -0.062029    -0.072677    -0.112627     0.171462    -0.207341     -0.046663      0.078731     -0.039187     -0.128192     -0.095857     -0.287233      0.018149      0.122441      0.475947      0.299201      0.067449   \n1       -0.227629     0.225224     0.231292    -0.106223     0.059433     0.013205     0.105225     0.030600     0.051235    -0.101789      0.027778      0.269350      0.082311     -0.017754     -0.038839     -0.310158      0.218842      0.006542      0.533929      0.198319      0.105475   \n2       -0.156663     0.144015     0.121652    -0.039961     0.181759     0.034058     0.070990     0.120514    -0.091443    -0.222109      0.044104      0.158231      0.242143     -0.003043     -0.049446     -0.339215      0.057154      0.166102      0.481451      0.215605      0.102058   \n3       -0.035580     0.188858    -0.003260    -0.130486     0.160150    -0.174751     0.095568    -0.080505     0.254445    -0.096919     -0.005112      0.146347     -0.020105     -0.144764      0.070111     -0.461313      0.020317      0.153474      0.449021      0.240020     -0.029829   \n4       -0.224686     0.139598     0.108120    -0.051431     0.124639    -0.146919     0.009397    -0.076450    -0.057992    -0.182568     -0.011725      0.090582      0.098747     -0.098377      0.027371     -0.278408      0.022246      0.113622      0.423185      0.259460      0.067126   \n...           ...          ...          ...          ...          ...          ...          ...          ...          ...          ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...   \n5797    -0.183800     0.165602     0.171669    -0.052956     0.278973    -0.130962     0.071228     0.057740    -0.056535    -0.209858      0.040720      0.107985      0.234738      0.008502     -0.075491     -0.251127      0.147424      0.144499      0.494582      0.306974      0.094688   \n5798    -0.137193     0.145458     0.285109    -0.066712     0.093408    -0.177522     0.163986    -0.047016    -0.007576    -0.214706     -0.058691      0.319188      0.190422      0.076463     -0.063008     -0.267173      0.041295      0.093231      0.581905      0.328003      0.056584   \n5799    -0.186172     0.179648     0.243748    -0.100095     0.083960    -0.264884     0.079414    -0.051943     0.027550    -0.117400     -0.082906      0.163883      0.084087     -0.070525     -0.012431     -0.201493      0.126095      0.039986      0.482932      0.263607      0.112378   \n5800    -0.132067     0.130876     0.161276    -0.091357     0.177120    -0.033335    -0.097726     0.020873     0.154972    -0.165567     -0.088746      0.181572      0.051574     -0.242877     -0.129602     -0.295245      0.023770      0.104712      0.456264      0.377980     -0.022440   \n5801    -0.197204     0.164000     0.139933    -0.060349     0.123922    -0.014345     0.071361     0.003858    -0.048259    -0.140171      0.033974      0.258364      0.103917     -0.137829     -0.077513     -0.288034      0.224880      0.034143      0.464200      0.316458      0.040942   \n\n      BERTEmbed_21  BERTEmbed_22  BERTEmbed_23  BERTEmbed_24  BERTEmbed_25  BERTEmbed_26  BERTEmbed_27  BERTEmbed_28  BERTEmbed_29  BERTEmbed_30  BERTEmbed_31  BERTEmbed_32  BERTEmbed_33  BERTEmbed_34  BERTEmbed_35  BERTEmbed_36  BERTEmbed_37  BERTEmbed_38  BERTEmbed_39  BERTEmbed_40  \\\n0        -0.953918     -0.133265      0.001833      0.146900     -0.076164     -0.179670     -0.067646     -0.104437      0.086202      0.046614      0.256747     -0.470094     -0.067134      0.270478     -0.049564     -0.049378      0.247585      0.324011      0.035134      0.143400   \n1        -0.803174     -0.240670     -0.106467      0.300168      0.006877     -0.116769      0.036999     -0.235160      0.041112      0.045297      0.244439     -0.385171     -0.182888      0.287584     -0.120918     -0.088403      0.316307      0.234557     -0.032739      0.043801   \n2        -1.040277     -0.183183     -0.149181      0.233562      0.003653     -0.096026     -0.003947     -0.140939      0.012934      0.069196      0.152730     -0.470859     -0.074503      0.285940     -0.079989     -0.087406      0.257870      0.337321      0.060606     -0.057308   \n3        -0.746964     -0.110814     -0.152175      0.118142      0.026853     -0.047350     -0.153123     -0.079949      0.071040      0.236494      0.258795     -0.412821     -0.089466      0.274340     -0.049335      0.163575      0.108345      0.280589      0.041556      0.083502   \n4        -0.949673     -0.220340     -0.087974      0.089272     -0.009048     -0.081135     -0.106222     -0.156119      0.062679      0.101260      0.155487     -0.525824     -0.162470      0.279062     -0.055759     -0.005481      0.259231      0.285417      0.037650      0.155340   \n...            ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...   \n5797     -0.972764     -0.137449     -0.160281      0.017787     -0.021394      0.012869      0.005371     -0.215681     -0.079269      0.015699      0.198888     -0.515358     -0.188952      0.234708     -0.086872     -0.031004      0.088533      0.352518     -0.055020     -0.059648   \n5798     -0.864065     -0.203620     -0.174470      0.135858     -0.086740      0.006643     -0.154021     -0.166658      0.063497      0.119456      0.131839     -0.428446     -0.283512      0.227354      0.001393      0.089353      0.082466      0.259382     -0.116425      0.114204   \n5799     -0.734594     -0.218636     -0.075598      0.046997     -0.279304     -0.059241     -0.069265     -0.315325     -0.070364     -0.040366      0.309079     -0.385736     -0.293288      0.274859     -0.117947     -0.028346      0.176443      0.416964      0.096570      0.127654   \n5800     -1.012767     -0.115168     -0.154799     -0.013734     -0.019550     -0.111024     -0.095495      0.119128      0.018157      0.098169      0.296869     -0.408826     -0.054825      0.289302     -0.160595     -0.025372      0.195634      0.373598     -0.034114      0.157825   \n5801     -0.845762     -0.172125     -0.152267      0.036629      0.016338     -0.086001     -0.066104     -0.223363      0.050770      0.040453      0.217819     -0.506178     -0.243926      0.271712     -0.013115      0.052959      0.190609      0.313460     -0.014070      0.126671   \n\n      BERTEmbed_41  BERTEmbed_42  BERTEmbed_43  BERTEmbed_44  BERTEmbed_45  BERTEmbed_46  BERTEmbed_47  BERTEmbed_48  BERTEmbed_49  BERTEmbed_50  BERTEmbed_51  BERTEmbed_52  BERTEmbed_53  BERTEmbed_54  BERTEmbed_55  BERTEmbed_56  BERTEmbed_57  BERTEmbed_58  BERTEmbed_59  BERTEmbed_60  \\\n0        -0.311486      0.167787     -0.224317      0.246488     -0.031889      0.338930     -0.076163      0.295636      0.067720      0.084182      0.333475     -0.072839      0.119350     -0.139098     -0.602282     -0.142682      0.468993      0.478908      0.356846     -0.125217   \n1        -0.103561      0.373260     -0.138574      0.237544      0.025560      0.207588     -0.057392      0.226009      0.149087      0.075969      0.183343      0.123620      0.108489      0.001905     -0.773536     -0.104454      0.408551      0.461901      0.381368     -0.058764   \n2        -0.366120      0.376492     -0.161677      0.403069      0.024838      0.303680     -0.177946      0.350208     -0.023298     -0.020037      0.455113      0.105763     -0.098347      0.033148     -0.702089     -0.204126      0.427760      0.327849      0.341120     -0.067064   \n3        -0.447287      0.161984     -0.230275      0.050764     -0.176455      0.295260     -0.071810      0.204033      0.194393      0.072104      0.069230      0.057365      0.250115     -0.226674     -0.409882     -0.083209      0.409443      0.586730      0.294902     -0.009036   \n4        -0.309472      0.166988     -0.343516      0.210384     -0.090235      0.328635     -0.119247      0.300086      0.122281      0.046533      0.224036      0.038425      0.228496     -0.169686     -0.539142     -0.125522      0.417447      0.488795      0.443821     -0.141856   \n...            ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...   \n5797     -0.189838      0.319680     -0.391937      0.079372      0.068417      0.282101     -0.132524      0.147947      0.140314      0.075820      0.354148      0.119526     -0.002260     -0.008709     -0.728890     -0.160195      0.466399      0.336880      0.368612     -0.049891   \n5798     -0.200304      0.268641     -0.213897      0.151884     -0.076445      0.271173     -0.260567      0.304493      0.194877      0.062892      0.150694      0.152871      0.174529     -0.044254     -0.740778     -0.142606      0.454390      0.412939      0.341537     -0.198301   \n5799     -0.375939      0.099659     -0.228607      0.227760     -0.129032      0.302493     -0.154176      0.243663      0.239816      0.070451      0.385107     -0.039737     -0.100099     -0.204769     -0.650722     -0.173833      0.390868      0.312984      0.264680     -0.064332   \n5800     -0.285136      0.280263     -0.185379      0.250970     -0.173470      0.258732     -0.048782      0.241807      0.163020     -0.039766      0.158218      0.154478      0.100481     -0.067844     -0.520916     -0.175131      0.567349      0.479005      0.281838     -0.048371   \n5801     -0.148991      0.284928     -0.309423      0.043310      0.076997      0.251565     -0.117050      0.296240      0.189658      0.095776      0.162875      0.072554      0.026161     -0.056362     -0.711017     -0.245344      0.379634      0.470131      0.435069     -0.094786   \n\n      BERTEmbed_61  BERTEmbed_62  BERTEmbed_63  BERTEmbed_64  BERTEmbed_65  BERTEmbed_66  BERTEmbed_67  BERTEmbed_68  BERTEmbed_69  BERTEmbed_70  BERTEmbed_71  BERTEmbed_72  BERTEmbed_73  BERTEmbed_74  BERTEmbed_75  BERTEmbed_76  BERTEmbed_77  BERTEmbed_78  BERTEmbed_79  BERTEmbed_80  \\\n0        -0.036384      0.266413      0.157529      0.592066      0.249830      0.041442     -0.208175      0.113400      0.235857     -0.432324     -0.441868      0.027432      0.165315     -0.404585     -0.080296      0.083703      0.230129     -0.349844     -0.016986     -0.008322   \n1        -0.114650      0.245143      0.194444      0.523532     -0.004141      0.124094     -0.287005     -0.035957      0.119354     -0.257367     -0.511525     -0.004055      0.192340     -0.387030      0.074694      0.045565      0.200065     -0.144557     -0.087852      0.124347   \n2        -0.025585      0.207255      0.169362      0.548283      0.250421      0.054299     -0.192055     -0.066694      0.160709     -0.473928     -0.297893      0.178422      0.161338     -0.412606     -0.054201     -0.099775      0.260790     -0.173795     -0.159312      0.236255   \n3        -0.066134      0.378873      0.139024      0.616953      0.147327      0.152984     -0.220352     -0.060052      0.294716     -0.353490     -0.347696      0.019526      0.150951     -0.523688     -0.021887      0.151107     -0.031520     -0.080580     -0.226491      0.072965   \n4        -0.028551      0.301847      0.153022      0.609838      0.202789      0.075323     -0.236674     -0.045330      0.251661     -0.302042     -0.467181      0.008336      0.079019     -0.386661     -0.060748      0.076541      0.186478     -0.195288     -0.117982     -0.010120   \n...            ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...   \n5797     -0.103828      0.238289      0.116275      0.372342      0.127452      0.069002     -0.169457     -0.042329      0.142040     -0.321264     -0.439664      0.110230      0.197935     -0.527432     -0.010925      0.078714      0.263842     -0.025440     -0.137584      0.104055   \n5798     -0.038304      0.246027      0.122126      0.553864      0.178828      0.074974     -0.209458     -0.061300      0.114607     -0.248671     -0.328152      0.144857      0.042990     -0.543537      0.137574      0.111852      0.442921     -0.072612     -0.036291      0.121957   \n5799     -0.050500      0.271060      0.225866      0.536240      0.024610     -0.023891     -0.289053      0.104585      0.327920     -0.302079     -0.421168      0.092903      0.168724     -0.561593      0.003771      0.015795      0.240465     -0.252855     -0.064424     -0.052367   \n5800     -0.053719      0.242180      0.104336      0.583708      0.062588      0.038784     -0.175716      0.007910      0.294931     -0.335829     -0.314827      0.064629      0.160302     -0.440031      0.019835      0.127358      0.204301     -0.200790     -0.045241      0.065962   \n5801     -0.113622      0.149384      0.248887      0.532551      0.023685      0.014519     -0.213488     -0.015437      0.261454     -0.230526     -0.432995      0.091653      0.128027     -0.456513      0.069084      0.069847      0.256042     -0.228045     -0.113325      0.060683   \n\n      BERTEmbed_81  BERTEmbed_82  BERTEmbed_83  BERTEmbed_84  BERTEmbed_85  BERTEmbed_86  BERTEmbed_87  BERTEmbed_88  BERTEmbed_89  BERTEmbed_90  BERTEmbed_91  BERTEmbed_92  BERTEmbed_93  BERTEmbed_94  BERTEmbed_95  BERTEmbed_96  BERTEmbed_97  BERTEmbed_98  BERTEmbed_99  BERTEmbed_100  \\\n0        -0.123397      0.015158      0.079697      0.058765     -0.072907      0.109945      0.173136      0.130883     -0.015202      0.271100      0.027043      0.000853     -0.153687     -0.041729      0.067801     -0.140518     -0.080658     -0.368284      0.411745      -0.362667   \n1        -0.084571     -0.070033      0.252908      0.001638     -0.131021      0.060998      0.108455      0.096317      0.075333      0.134659     -0.002534      0.026052     -0.319822     -0.039080      0.028208     -0.157551     -0.070561     -0.397704      0.472786      -0.071336   \n2        -0.102328     -0.276479      0.002062      0.111890      0.016183      0.030645      0.151044      0.075064     -0.014816      0.120915      0.087660     -0.029492     -0.183027     -0.018089      0.001679     -0.109386     -0.070902     -0.334765      0.428085      -0.251212   \n3         0.024485      0.090849      0.061660      0.160308     -0.000249      0.145427      0.036488      0.143511      0.066186      0.185096      0.064714      0.043788     -0.253447     -0.085037      0.131762     -0.241207     -0.214116     -0.262134      0.428314      -0.205756   \n4        -0.047565     -0.084024      0.162113      0.060886     -0.038068     -0.023373      0.226406      0.203735     -0.063867      0.331452      0.122181      0.052440     -0.275033     -0.151542      0.110324     -0.057964     -0.063879     -0.468583      0.296925      -0.317584   \n...            ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...           ...            ...   \n5797     -0.047856     -0.167662      0.254448     -0.006499     -0.099957      0.089659      0.167602      0.116169     -0.087352      0.224377      0.095085      0.087274     -0.347316     -0.051647      0.071334     -0.180330     -0.123110     -0.392315      0.456517      -0.141040   \n5798      0.018479     -0.152546      0.131298     -0.037037     -0.153288      0.046613      0.202089      0.248485      0.000386      0.130957     -0.042190      0.089310     -0.371683     -0.113110      0.086186     -0.035291     -0.043324     -0.283935      0.311606      -0.238291   \n5799     -0.118202     -0.038713      0.248863      0.008950     -0.141860      0.135016      0.311653      0.166037     -0.051419      0.179497     -0.057369     -0.021990     -0.285449      0.019858      0.052515     -0.143881     -0.063700     -0.297794      0.245798      -0.403736   \n5800      0.035095     -0.090812      0.186240      0.161651     -0.030852      0.008009      0.067464      0.032515     -0.005720      0.236870      0.134924      0.007866     -0.160228     -0.040366      0.110597     -0.153580     -0.095041     -0.267969      0.342028      -0.426397   \n5801     -0.016665     -0.096138      0.230524     -0.026908     -0.064551      0.111085      0.080778      0.215236     -0.077411      0.328641     -0.005708     -0.059052     -0.279319     -0.117145      0.042483     -0.150987     -0.078925     -0.311772      0.413753      -0.187577   \n\n      BERTEmbed_101  BERTEmbed_102  BERTEmbed_103  BERTEmbed_104  BERTEmbed_105  BERTEmbed_106  BERTEmbed_107  BERTEmbed_108  BERTEmbed_109  BERTEmbed_110  BERTEmbed_111  BERTEmbed_112  BERTEmbed_113  BERTEmbed_114  BERTEmbed_115  BERTEmbed_116  BERTEmbed_117  BERTEmbed_118  BERTEmbed_119  \\\n0         -0.006279       0.119001      -0.051532       0.359874       0.312525       0.052078      -0.395427      -0.029553       0.250159       0.161739      -0.116295      -0.077457      -0.084327      -0.371191      -0.204650       0.112661      -0.032450      -0.145746      -0.211910   \n1         -0.084122       0.159971       0.045540       0.337496       0.325120       0.133904      -0.362409      -0.161822       0.270715       0.266550      -0.105878      -0.077163      -0.087837      -0.374546      -0.217475       0.131968       0.229010       0.124254      -0.218547   \n2         -0.124699       0.176491       0.002355       0.218339       0.207843       0.072184      -0.304653      -0.122569       0.143048       0.014649      -0.061174      -0.057008       0.014338      -0.415426      -0.159387       0.002112       0.006859       0.068110      -0.125654   \n3          0.020433       0.053164      -0.046708       0.336227       0.250764       0.184627      -0.307911      -0.054481       0.251481       0.152357      -0.139081      -0.123923      -0.140809      -0.395432      -0.232222      -0.039668       0.076020       0.047020      -0.178981   \n4          0.002456       0.131400      -0.049492       0.351513       0.262963       0.050179      -0.416606      -0.108688       0.279020       0.187148      -0.094144      -0.105822      -0.062931      -0.365287      -0.208583       0.069718       0.035326      -0.052281      -0.240490   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797       0.020599       0.202348      -0.063767       0.366351       0.235891       0.087692      -0.383498      -0.262828       0.248724       0.200279      -0.117193      -0.089936       0.022948      -0.347339      -0.240669       0.031732       0.087027       0.006320      -0.275179   \n5798      -0.136672       0.155173      -0.085528       0.419597       0.364502       0.149295      -0.450078      -0.206489       0.310355       0.237552      -0.108931      -0.173596      -0.007025      -0.432121      -0.126545      -0.044900       0.096462      -0.049026      -0.296358   \n5799       0.011080       0.071263      -0.068837       0.404638       0.460989       0.077451      -0.435507       0.082564       0.291737       0.114560      -0.054926      -0.135359      -0.081196      -0.261137      -0.057429       0.008658       0.112302      -0.013723      -0.222782   \n5800      -0.072382       0.169967      -0.087653       0.337757       0.300350       0.146923      -0.468155      -0.028496       0.233338       0.223446      -0.088716       0.001344      -0.096621      -0.337288      -0.244056       0.009975       0.116920      -0.088993      -0.106189   \n5801      -0.000576       0.099592       0.015164       0.398611       0.248995       0.068000      -0.479217      -0.156375       0.227330       0.268555      -0.112749      -0.091831      -0.100524      -0.354948      -0.224731       0.095654       0.094812       0.075904      -0.190370   \n\n      BERTEmbed_120  BERTEmbed_121  BERTEmbed_122  BERTEmbed_123  BERTEmbed_124  BERTEmbed_125  BERTEmbed_126  BERTEmbed_127  BERTEmbed_128  BERTEmbed_129  BERTEmbed_130  BERTEmbed_131  BERTEmbed_132  BERTEmbed_133  BERTEmbed_134  BERTEmbed_135  BERTEmbed_136  BERTEmbed_137  BERTEmbed_138  \\\n0          0.266981       0.385478      -0.635658      -0.138266       0.017824       0.350206       0.125419      -0.173613       0.482098       0.189541       0.077151      -0.044387      -0.052943       0.079629       0.282330       0.310168       0.130220       0.012737       0.356406   \n1          0.082128       0.147969      -0.632372      -0.033969      -0.046950       0.357366       0.074304      -0.125998       0.567422       0.219362       0.108973       0.239894      -0.025410       0.151556       0.220999       0.407328       0.286155       0.086555       0.204056   \n2          0.121386       0.133449      -0.567735      -0.065501      -0.011225       0.417597      -0.112115      -0.274241       0.552531       0.313544       0.258108       0.292497       0.033401       0.018047       0.098912       0.312311       0.050923       0.078871       0.180917   \n3          0.452181       0.195260      -0.642997      -0.067788      -0.035879       0.492625       0.137494      -0.251071       0.506569       0.142970       0.057660      -0.130678       0.112102       0.009028       0.369862       0.354283       0.145550      -0.029876       0.331898   \n4          0.362602       0.225711      -0.684493      -0.119297      -0.027820       0.507100       0.045060      -0.208374       0.610801       0.196478       0.139748      -0.007963       0.064250       0.106813       0.267366       0.302590       0.086619      -0.040683       0.306676   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797       0.182102       0.149824      -0.618584      -0.059428      -0.119899       0.478920      -0.058893      -0.239811       0.594525       0.208125       0.154506       0.160430      -0.054102       0.136897       0.248089       0.277288       0.019002      -0.057653       0.273099   \n5798       0.235604       0.038211      -0.643704      -0.164600      -0.137978       0.509269      -0.093508      -0.396823       0.663942       0.112753       0.228380       0.024075       0.024981      -0.051495       0.373561       0.330433       0.112064       0.003050       0.330785   \n5799       0.167539       0.313987      -0.711246      -0.079570      -0.071453       0.356817       0.078928      -0.105963       0.539841       0.163797       0.123566      -0.094415       0.049853       0.314647       0.321343       0.265864      -0.129583      -0.099111       0.238108   \n5800       0.322011       0.186147      -0.664997      -0.174551      -0.060549       0.402668       0.002570      -0.202801       0.404086       0.078558       0.159959       0.098288      -0.013981       0.140102       0.162698       0.290902       0.089186      -0.076557       0.244866   \n5801       0.235684       0.170623      -0.650336       0.037762      -0.072813       0.355756       0.054512      -0.197220       0.676223       0.198135       0.163415       0.002879      -0.026995       0.129614       0.276480       0.305983       0.078695      -0.060980       0.213617   \n\n      BERTEmbed_139  BERTEmbed_140  BERTEmbed_141  BERTEmbed_142  BERTEmbed_143  BERTEmbed_144  BERTEmbed_145  BERTEmbed_146  BERTEmbed_147  BERTEmbed_148  BERTEmbed_149  BERTEmbed_150  BERTEmbed_151  BERTEmbed_152  BERTEmbed_153  BERTEmbed_154  BERTEmbed_155  BERTEmbed_156  BERTEmbed_157  \\\n0          0.149958       0.050123       0.237315       0.143462       0.065749       0.179177       0.086108      -0.263236       0.208715       0.119488       0.227063      -0.044106       0.342906      -0.047339      -0.154376       0.074996       0.053576       0.007444       0.149203   \n1          0.105037      -0.066177       0.173624       0.239872       0.090851       0.013560       0.163928      -0.280196       0.192369       0.296814       0.162351       0.100971       0.432011      -0.194543      -0.153222      -0.076628       0.247797       0.037855       0.075523   \n2         -0.246239      -0.111141       0.242654       0.195084       0.006037       0.115205       0.169492      -0.150973       0.028631       0.385599       0.343959      -0.086351       0.354821      -0.143474      -0.157999      -0.027486       0.161247      -0.018787       0.096103   \n3          0.214437      -0.045436       0.242849       0.076237       0.150197       0.122341       0.043499      -0.207265       0.181092       0.297803       0.302810      -0.187244       0.270493      -0.115787      -0.157989       0.224122       0.015160       0.036422      -0.014339   \n4          0.067943       0.079773       0.320232       0.188125       0.047553       0.172382       0.108560      -0.228358       0.235052       0.199982       0.268948       0.056929       0.404265      -0.054521      -0.069474       0.081982       0.073801       0.101196       0.104848   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797      -0.140730      -0.024730       0.156882       0.211476       0.106192       0.129335       0.175490      -0.256518       0.146562       0.224432       0.160117       0.076052       0.419558      -0.105563      -0.246532      -0.069292       0.229191      -0.067568       0.047140   \n5798      -0.106191       0.005969       0.219147       0.167658       0.147235       0.091115       0.140891      -0.251173       0.173266       0.135739       0.308271      -0.056549       0.431241      -0.126551      -0.157846       0.093940       0.255150       0.003652       0.182590   \n5799      -0.005669       0.098866       0.279564       0.120293      -0.029945       0.047578       0.167044      -0.308297       0.228981       0.279752       0.296659       0.118933       0.339381      -0.158626      -0.116315       0.118420       0.118906       0.121271       0.114213   \n5800       0.184985      -0.065959       0.253094       0.277447       0.033763       0.187806       0.144026      -0.243744       0.098339       0.280649       0.232030       0.038787       0.228194      -0.126166      -0.153720       0.204803       0.084816      -0.047566       0.060588   \n5801      -0.044270      -0.009202       0.336505       0.151871       0.121205       0.133944       0.229626      -0.163213       0.205002       0.232031       0.258762      -0.024485       0.357546      -0.092322      -0.142572      -0.054280       0.212741       0.010158       0.020578   \n\n      BERTEmbed_158  BERTEmbed_159  BERTEmbed_160  BERTEmbed_161  BERTEmbed_162  BERTEmbed_163  BERTEmbed_164  BERTEmbed_165  BERTEmbed_166  BERTEmbed_167  BERTEmbed_168  BERTEmbed_169  BERTEmbed_170  BERTEmbed_171  BERTEmbed_172  BERTEmbed_173  BERTEmbed_174  BERTEmbed_175  BERTEmbed_176  \\\n0          0.099868      -0.453321      -0.324889       0.010741       0.231252       0.304312      -0.060922      -0.211887       0.165883       0.083221       0.243373      -0.534628       0.218726       0.307381      -0.167603      -0.243763      -0.379733       0.369471       0.016668   \n1          0.089071      -0.395528      -0.251435       0.154281       0.092694       0.149225      -0.101007      -0.345696       0.077113       0.009493       0.267335      -0.500945       0.414689       0.301062      -0.302927      -0.241013      -0.382438       0.475339       0.009753   \n2         -0.039661      -0.425403      -0.282121      -0.000790       0.115437       0.229407      -0.009209      -0.304521       0.132736       0.100025       0.295487      -0.548718       0.508793       0.212659      -0.285575      -0.119210      -0.188746       0.652089       0.013219   \n3         -0.100939      -0.487877      -0.207670      -0.023456       0.131760       0.445729       0.140985      -0.129708      -0.006917       0.203316       0.214676      -0.414249       0.181311       0.535992      -0.058810      -0.194038      -0.236499       0.344177      -0.082826   \n4          0.068738      -0.483133      -0.222031      -0.002438       0.251032       0.343779      -0.048711      -0.163192       0.140117       0.131374       0.256049      -0.452066       0.198757       0.328482      -0.205885      -0.256584      -0.380374       0.371758       0.011327   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797      -0.005471      -0.345878      -0.312605       0.063178       0.167227       0.288438      -0.045123      -0.208878       0.049437       0.049607       0.127715      -0.430703       0.305041       0.237689      -0.217557      -0.165059      -0.216847       0.400648       0.118239   \n5798      -0.012523      -0.359231      -0.326903       0.084762       0.112342       0.360331      -0.024473      -0.316633       0.080717       0.010139       0.131647      -0.510711       0.190655       0.173257      -0.284240      -0.201848      -0.255966       0.441580       0.058903   \n5799       0.126573      -0.383104      -0.285285      -0.097045       0.172136       0.411588      -0.024511      -0.063496       0.114363       0.285120       0.183627      -0.551634       0.336677       0.433881      -0.228686      -0.190196      -0.200961       0.520081       0.066223   \n5800      -0.060030      -0.501611      -0.228924       0.144340       0.072455       0.276446      -0.021490      -0.098037       0.026923       0.175153       0.217400      -0.435691       0.362069       0.494104      -0.259501      -0.152133      -0.308774       0.333595       0.026328   \n5801       0.012151      -0.427382      -0.293360       0.094190       0.105551       0.326485      -0.053303      -0.145053       0.058412       0.168415       0.175386      -0.429844       0.263693       0.454047      -0.243650      -0.181065      -0.306258       0.399988       0.048889   \n\n      BERTEmbed_177  BERTEmbed_178  BERTEmbed_179  BERTEmbed_180  BERTEmbed_181  BERTEmbed_182  BERTEmbed_183  BERTEmbed_184  BERTEmbed_185  BERTEmbed_186  BERTEmbed_187  BERTEmbed_188  BERTEmbed_189  BERTEmbed_190  BERTEmbed_191  BERTEmbed_192  BERTEmbed_193  BERTEmbed_194  BERTEmbed_195  \\\n0          0.011161       0.330671       0.328514       0.251347      -0.139805       0.192017      -0.420310       0.164875       0.340967      -0.726799       0.125515       0.131599      -0.399806      -0.489963      -0.196015       0.326039       0.152051       0.537931       0.053144   \n1         -0.019691       0.203157       0.356048       0.186692      -0.138737       0.336944      -0.460266       0.046250       0.350409      -0.630069       0.156080       0.093638      -0.188227      -0.351190      -0.058942       0.358646       0.254728       0.385072       0.154378   \n2          0.054829       0.290093       0.320279       0.168257      -0.209247       0.242835      -0.303448      -0.030037       0.227640      -0.641570       0.121716       0.166924      -0.242704      -0.477683      -0.121446       0.464237       0.142659       0.219073       0.183695   \n3         -0.009086       0.273671       0.361212       0.255325      -0.137906       0.186877      -0.354912       0.346547       0.348800      -0.881826       0.206155       0.178205      -0.443061      -0.465032      -0.269095       0.219621       0.228248       0.381883       0.068821   \n4          0.000135       0.346537       0.306987       0.270705      -0.111971       0.255775      -0.385067       0.143123       0.374777      -0.748106       0.183797       0.046408      -0.456501      -0.473792      -0.125127       0.210746       0.149871       0.493633       0.274586   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797       0.037590       0.223998       0.433889       0.271647      -0.157249       0.287142      -0.384944       0.009272       0.327464      -0.565579       0.099891       0.145617      -0.215092      -0.510071      -0.131367       0.330615       0.191556       0.374407       0.212004   \n5798       0.086974       0.209829       0.358047       0.376028      -0.117540       0.198105      -0.300244       0.305447       0.398138      -0.523188       0.174043       0.204542      -0.281990      -0.456775      -0.154793       0.285681       0.043810       0.340361       0.189553   \n5799      -0.000911       0.230123       0.488370       0.219507      -0.162138       0.187894      -0.433112      -0.063303       0.456164      -0.544107       0.077095       0.212714      -0.303538      -0.439197      -0.078554       0.314915       0.121862       0.551503       0.151515   \n5800       0.025571       0.240006       0.261135       0.237247      -0.114374       0.177882      -0.342713       0.095928       0.294129      -0.687883       0.136921       0.194213      -0.385504      -0.464953      -0.204849       0.316583       0.259625       0.422941       0.163696   \n5801       0.003095       0.274596       0.384392       0.211066      -0.123679       0.178520      -0.333539       0.216782       0.412941      -0.598766       0.077580       0.121292      -0.331019      -0.369021      -0.134539       0.265330       0.135050       0.401057       0.161166   \n\n      BERTEmbed_196  BERTEmbed_197  BERTEmbed_198  BERTEmbed_199  BERTEmbed_200  BERTEmbed_201  BERTEmbed_202  BERTEmbed_203  BERTEmbed_204  BERTEmbed_205  BERTEmbed_206  BERTEmbed_207  BERTEmbed_208  BERTEmbed_209  BERTEmbed_210  BERTEmbed_211  BERTEmbed_212  BERTEmbed_213  BERTEmbed_214  \\\n0         -0.263809      -0.035566      -0.064936      -0.107309       0.074838       0.072762      -0.117619       0.090758      -0.100880       0.191830       0.187349       0.127041      -0.025108      -0.165990       0.223326      -0.130356       0.075869      -0.054922      -0.162718   \n1         -0.249765       0.113245       0.037746      -0.056376       0.122183       0.105027      -0.049097       0.102195      -0.179841       0.251561       0.363708       0.198006      -0.045219      -0.263609       0.145885      -0.045822      -0.032397       0.192920      -0.116281   \n2         -0.331932       0.028306      -0.181870       0.004978       0.176789       0.015282      -0.115615       0.152025      -0.113486       0.178382       0.223526       0.186151      -0.084463      -0.341933       0.160031      -0.098737      -0.005475       0.068233      -0.022395   \n3         -0.174993      -0.000809      -0.238583      -0.228395      -0.013334      -0.045344      -0.015050      -0.040867      -0.057101       0.171305       0.090578       0.036180      -0.059382      -0.220203       0.202565      -0.089261       0.183520      -0.137313       0.092158   \n4         -0.242301       0.050019      -0.104351      -0.050414       0.144237       0.076188      -0.032773       0.019644      -0.118328       0.190937       0.143970       0.121806      -0.111245      -0.218350       0.170546      -0.060848       0.020044      -0.075605      -0.149593   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797      -0.176874       0.110514      -0.089610       0.014662       0.157101      -0.000313       0.027682       0.006716      -0.123089       0.141583       0.095132       0.145961      -0.046628      -0.417901       0.134852      -0.056276      -0.096934       0.055077       0.082040   \n5798      -0.226872       0.033108      -0.068877      -0.035568       0.168230       0.023838       0.026076       0.021686      -0.232444       0.279637      -0.006176       0.106174      -0.021702      -0.177450       0.140583      -0.032721      -0.091545      -0.062717      -0.043442   \n5799      -0.235570       0.140029       0.054401      -0.035657       0.108660       0.114163       0.114349      -0.101480      -0.075614       0.186068       0.238603       0.049488      -0.042062      -0.144498       0.246942      -0.164953       0.094125      -0.181604      -0.270030   \n5800      -0.289955       0.029273      -0.083988      -0.105565       0.140017       0.079198      -0.070238       0.103108      -0.087865       0.212782       0.012427       0.124866       0.045378      -0.155994       0.114740      -0.079507       0.149586      -0.080360      -0.142490   \n5801      -0.207191       0.073240      -0.052879      -0.067090       0.078067       0.041606      -0.030979       0.001576      -0.187513       0.249277       0.168450       0.103464       0.004027      -0.265379       0.143745      -0.100512       0.011362      -0.050863      -0.113679   \n\n      BERTEmbed_215  BERTEmbed_216  BERTEmbed_217  BERTEmbed_218  BERTEmbed_219  BERTEmbed_220  BERTEmbed_221  BERTEmbed_222  BERTEmbed_223  BERTEmbed_224  BERTEmbed_225  BERTEmbed_226  BERTEmbed_227  BERTEmbed_228  BERTEmbed_229  BERTEmbed_230  BERTEmbed_231  BERTEmbed_232  BERTEmbed_233  \\\n0         -3.498117      -0.236880       0.016385       0.025485       0.077016      -0.214546       0.164464       0.182428       0.002008       0.169447      -0.125676       0.184625       0.154596      -0.319359       0.153597       0.396978       0.454981      -0.156553      -0.268734   \n1         -3.179759      -0.196065      -0.007169       0.043181       0.042290      -0.384116       0.285349       0.199268      -0.112486       0.034734      -0.243222      -0.184304       0.074179      -0.336608       0.201832       0.180492       0.480714      -0.087649      -0.243396   \n2         -3.204847      -0.141447      -0.026438       0.171242      -0.068086      -0.314257       0.341201       0.311529      -0.195146       0.094182      -0.178546      -0.113799      -0.017555      -0.305044       0.193052       0.287055       0.415349      -0.151645      -0.271396   \n3         -3.610868      -0.231617      -0.027206      -0.038752       0.094744      -0.233482       0.177227      -0.005728      -0.142980       0.283989      -0.158183       0.286960       0.194096      -0.282940       0.111715       0.509651       0.349996      -0.174520      -0.365142   \n4         -3.573384      -0.285961       0.031387       0.032163       0.112445      -0.206838       0.107499       0.175006      -0.085400       0.149484      -0.165676       0.125858       0.128427      -0.408282       0.137211       0.373565       0.423122      -0.208116      -0.236664   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797      -2.882313      -0.247634       0.015631       0.113560      -0.103799      -0.237519       0.199018       0.087931      -0.274387       0.021310      -0.238471      -0.180123       0.181862      -0.341813       0.204152       0.376023       0.424304      -0.122498      -0.287502   \n5798      -3.070965      -0.176808      -0.081365       0.150761       0.024611      -0.338437       0.183892       0.101928      -0.192602       0.032685      -0.243075      -0.075472       0.229951      -0.403881       0.116726       0.296445       0.353647      -0.064704      -0.302995   \n5799      -2.613631      -0.154875      -0.000388       0.028412       0.095190      -0.148041       0.021894      -0.004996      -0.137383       0.133740      -0.208798       0.028142       0.101741      -0.221582       0.236867       0.595965       0.618525      -0.153878      -0.244164   \n5800      -3.402281      -0.171687      -0.022906       0.009940      -0.019952      -0.262242       0.195969       0.086375      -0.071964       0.165730      -0.217202       0.193918       0.262841      -0.326173       0.191733       0.359744       0.303201       0.045664      -0.253745   \n5801      -3.527609      -0.210857      -0.010124       0.065307       0.032298      -0.271220       0.201200       0.160675      -0.161122       0.035896      -0.274122      -0.025667       0.114912      -0.393177       0.167970       0.272051       0.297932      -0.112869      -0.257049   \n\n      BERTEmbed_234  BERTEmbed_235  BERTEmbed_236  BERTEmbed_237  BERTEmbed_238  BERTEmbed_239  BERTEmbed_240  BERTEmbed_241  BERTEmbed_242  BERTEmbed_243  BERTEmbed_244  BERTEmbed_245  BERTEmbed_246  BERTEmbed_247  BERTEmbed_248  BERTEmbed_249  BERTEmbed_250  BERTEmbed_251  BERTEmbed_252  \\\n0          0.202336       0.122704      -0.194059       0.254637      -0.227367      -0.311855       0.225219       0.087716       0.408750       0.078682       0.047307       0.024218       0.320653       0.557417       0.158551       0.123917       0.298512      -0.149285       0.058466   \n1          0.104218       0.341130      -0.145712       0.262657      -0.090222      -0.241125       0.172139       0.004944       0.539119       0.135715       0.008567       0.017065       0.324559       0.766401       0.223282       0.007300       0.365641      -0.033623       0.054330   \n2          0.035253       0.146891      -0.048210       0.420014      -0.091909      -0.163848       0.237720      -0.000511       0.588322       0.084210       0.129060      -0.001547       0.440588       0.693371       0.121086      -0.012625       0.305039      -0.031379      -0.034435   \n3          0.111448       0.017188      -0.256177       0.171137      -0.208715      -0.213327       0.279918       0.042586       0.564589       0.143606       0.077246       0.032110       0.226598       0.521446       0.112921       0.444375       0.311787      -0.054853       0.053673   \n4          0.169469       0.065424      -0.161202       0.307285      -0.180178      -0.315024       0.322721       0.054105       0.432655       0.104102       0.077890       0.006997       0.329715       0.671053       0.137491       0.182870       0.335589      -0.110827       0.049733   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797       0.223616       0.103763      -0.148789       0.308311      -0.228412      -0.396483       0.270812      -0.035155       0.617137       0.126271       0.073144       0.039875       0.416543       0.780946       0.077763      -0.001080       0.284301      -0.015807       0.098068   \n5798       0.114335       0.031398      -0.156333       0.255067      -0.063343      -0.288846       0.409042      -0.048090       0.513258       0.187238       0.119399       0.022758       0.332491       0.662134       0.170822       0.057783       0.322016      -0.074484       0.012290   \n5799       0.140347       0.309651      -0.272180       0.228845      -0.213810      -0.349507       0.199534       0.099227       0.544011       0.231616       0.057494       0.111521       0.169195       0.629199       0.039075       0.101793       0.405730      -0.108902       0.186346   \n5800       0.068239       0.116238      -0.234616       0.374068      -0.273942      -0.280154       0.394067       0.102716       0.466559       0.051316       0.019489       0.026435       0.250856       0.625440       0.175520       0.157388       0.193710      -0.088733       0.110520   \n5801       0.056138       0.131760      -0.207525       0.321884      -0.131013      -0.310836       0.271946      -0.006948       0.470541       0.207552       0.068628       0.026041       0.351575       0.748866       0.109570       0.100211       0.357095      -0.090962       0.079302   \n\n      BERTEmbed_253  BERTEmbed_254  BERTEmbed_255  BERTEmbed_256  BERTEmbed_257  BERTEmbed_258  BERTEmbed_259  BERTEmbed_260  BERTEmbed_261  BERTEmbed_262  BERTEmbed_263  BERTEmbed_264  BERTEmbed_265  BERTEmbed_266  BERTEmbed_267  BERTEmbed_268  BERTEmbed_269  BERTEmbed_270  BERTEmbed_271  \\\n0          0.249471       0.407494       0.263274       0.071685       0.093606       0.303074       0.052460       0.111555       0.245380      -0.197684       0.150766       0.052638      -0.015154       0.287149      -0.014091       0.009641      -0.053864       0.274082      -0.217135   \n1          0.080136       0.418952       0.272747       0.151535       0.051640       0.225289      -0.061753       0.270189       0.452422      -0.208400       0.118558       0.099702      -0.004625       0.411649       0.015993       0.129711      -0.074533       0.225088      -0.396319   \n2          0.006126       0.299577       0.345784       0.080592       0.082613       0.135674      -0.111408       0.445957       0.456607      -0.178324      -0.007740       0.113862       0.074956       0.481392       0.005457       0.233580      -0.058320       0.294973      -0.343200   \n3          0.390642       0.550242       0.317521       0.223242       0.245071       0.180030      -0.008855       0.081058       0.367180       0.080807       0.082388       0.198337       0.020685       0.313999      -0.066791       0.049743       0.031478       0.226368      -0.217628   \n4          0.169726       0.445681       0.182337       0.140015       0.091656       0.290521      -0.000366       0.137558       0.255673      -0.181095       0.142776       0.080911      -0.082483       0.354889       0.059673       0.066205      -0.047651       0.288218      -0.254087   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797       0.029924       0.413759       0.286685       0.132754       0.065874       0.147840      -0.000816       0.253687       0.512638      -0.167215       0.093433       0.168881       0.108314       0.309984      -0.041571       0.044469      -0.136723       0.211373      -0.356278   \n5798       0.102976       0.390084       0.114792       0.127745      -0.015288       0.213890      -0.001971       0.169215       0.396323      -0.229223       0.041697       0.160436       0.061427       0.242570      -0.004780       0.141248      -0.171765       0.285437      -0.303174   \n5799       0.118911       0.420780       0.244964       0.107629       0.116573       0.233170      -0.104364       0.037288       0.452113      -0.145186       0.125079       0.072902       0.067903       0.176885       0.128570       0.112522       0.002515       0.263346      -0.176577   \n5800       0.269308       0.355151       0.261032       0.224363       0.155667       0.210714      -0.098982       0.124875       0.313645      -0.194729       0.069003       0.049257       0.012012       0.250487      -0.059249       0.126176       0.022173       0.267386      -0.115375   \n5801       0.083402       0.467054       0.278471       0.216635       0.032652       0.211026       0.018271       0.190264       0.456018      -0.095922       0.115357       0.090612       0.079463       0.348857      -0.064717       0.099276      -0.056905       0.305034      -0.368010   \n\n      BERTEmbed_272  BERTEmbed_273  BERTEmbed_274  BERTEmbed_275  BERTEmbed_276  BERTEmbed_277  BERTEmbed_278  BERTEmbed_279  BERTEmbed_280  BERTEmbed_281  BERTEmbed_282  BERTEmbed_283  BERTEmbed_284  BERTEmbed_285  BERTEmbed_286  BERTEmbed_287  BERTEmbed_288  BERTEmbed_289  BERTEmbed_290  \\\n0          0.038326      -0.187159       0.279335       0.247314      -0.180083       0.350674       0.034805      -0.170363       0.663317      -0.411698       0.074372      -0.062803       0.274713      -0.132838      -0.179576       0.369982      -0.222017       0.629291       0.372114   \n1          0.044865      -0.144067       0.378246       0.368826      -0.463148       0.354378       0.150233      -0.379255       0.475093      -0.321968      -0.057343       0.008990       0.142343      -0.191032      -0.295985       0.246414      -0.108251       0.501328       0.327485   \n2         -0.086051      -0.307408       0.387312       0.354753      -0.427438       0.479795       0.200130      -0.314773       0.569507      -0.258279      -0.194728      -0.053379       0.056034      -0.015196      -0.131081       0.261108      -0.151185       0.532220       0.313282   \n3         -0.007321      -0.399018       0.453673       0.266275      -0.347746       0.373964      -0.146171      -0.203050       0.733007      -0.283803      -0.012803      -0.071472       0.137610      -0.062926      -0.041954       0.443351      -0.242382       0.575240       0.311491   \n4          0.065648      -0.303014       0.310328       0.340694      -0.326767       0.347965       0.023820      -0.195283       0.650697      -0.314169       0.036400      -0.011542       0.250829      -0.080236      -0.069415       0.360355      -0.199484       0.565230       0.319521   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797       0.154720      -0.247896       0.362811       0.426105      -0.268492       0.381618       0.058557      -0.361041       0.372211      -0.058382      -0.072765       0.116027       0.154523      -0.054554      -0.249889       0.367868      -0.122318       0.644729       0.197869   \n5798       0.149199      -0.406177       0.447216       0.493108      -0.156004       0.420629       0.006293      -0.356126       0.164467      -0.454964       0.086939      -0.051795       0.100888      -0.090847      -0.278282       0.321673      -0.185105       0.613843       0.284683   \n5799       0.154864      -0.337913       0.113759       0.442688      -0.166883       0.309062      -0.156985      -0.244082       0.732316      -0.405418       0.049349       0.076629       0.178855      -0.304320      -0.169461       0.236815      -0.167521       0.691785       0.248205   \n5800      -0.017114      -0.384084       0.336613       0.260834      -0.307693       0.555903       0.119331      -0.218577       0.545526      -0.493640       0.069068      -0.056986       0.183574      -0.165052      -0.226207       0.455939      -0.193798       0.653875       0.406409   \n5801       0.073342      -0.295572       0.349705       0.319717      -0.536023       0.398251       0.087451      -0.327592       0.462104      -0.299350       0.012221      -0.027240       0.116132      -0.203560      -0.132911       0.329755      -0.191730       0.559172       0.280156   \n\n      BERTEmbed_291  BERTEmbed_292  BERTEmbed_293  BERTEmbed_294  BERTEmbed_295  BERTEmbed_296  BERTEmbed_297  BERTEmbed_298  BERTEmbed_299  BERTEmbed_300  BERTEmbed_301  BERTEmbed_302  BERTEmbed_303  BERTEmbed_304  BERTEmbed_305  BERTEmbed_306  BERTEmbed_307  BERTEmbed_308  BERTEmbed_309  \\\n0         -0.166405      -0.408148       0.276812      -0.208603       0.209347      -0.003584       0.006407       0.016974      -0.254865       0.420173       0.110496      -0.285859       0.049026      -0.090400       0.143198       0.065386       0.201401      -0.053070      -0.328640   \n1         -0.212088      -0.448749       0.320620      -0.167318       0.372286       0.009420       0.120864       0.027195      -0.243592       0.236433       0.165549      -0.180362       0.109129      -0.239819      -0.089468       0.113333       0.423506       0.066341      -0.163666   \n2         -0.280039      -0.324210       0.152156      -0.337306       0.321261       0.027640       0.028485      -0.042114      -0.197813       0.497736       0.227063      -0.147483       0.030116      -0.139211       0.011582      -0.028006       0.388773      -0.013760      -0.092919   \n3         -0.151576      -0.489626       0.052303      -0.212201       0.344724      -0.020761       0.020689       0.072646      -0.221001       0.239724       0.219805      -0.254889       0.050592      -0.101746       0.253341       0.045858       0.134396      -0.133380      -0.457550   \n4         -0.148349      -0.412954       0.206655      -0.244139       0.238886       0.049362       0.007377      -0.035468      -0.262011       0.412155       0.186450      -0.365440      -0.025972      -0.134480       0.121959       0.036242       0.247163      -0.003246      -0.265416   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797      -0.172396      -0.345600       0.335623      -0.273300       0.268791       0.122981      -0.045008       0.047277      -0.283486       0.469759       0.262965      -0.290420       0.054679      -0.191597      -0.143793       0.091227       0.382634       0.093982      -0.402856   \n5798      -0.149865      -0.396418       0.147105      -0.271327       0.304151      -0.049969       0.115837       0.039303      -0.316830       0.469439       0.233150      -0.258309      -0.002399      -0.157735       0.052592       0.047975       0.233044       0.108142      -0.247049   \n5799      -0.049451      -0.406117       0.329825      -0.105802       0.143669      -0.086492      -0.011968       0.036297      -0.319344       0.484090       0.142401      -0.356139       0.173572      -0.037524       0.093677       0.091989       0.045011       0.091866      -0.479279   \n5800      -0.238414      -0.427574       0.214363      -0.214675       0.210685      -0.027837       0.016673       0.019637      -0.212047       0.330150       0.183370      -0.217040       0.009215      -0.141354       0.049000       0.003130       0.152152      -0.037681      -0.230806   \n5801      -0.176340      -0.422265       0.248192      -0.173052       0.414384      -0.044662       0.090228       0.020486      -0.225133       0.440259       0.175797      -0.346676       0.087021      -0.194521       0.119153       0.044320       0.251317      -0.032677      -0.297431   \n\n      BERTEmbed_310  BERTEmbed_311  BERTEmbed_312  BERTEmbed_313  BERTEmbed_314  BERTEmbed_315  BERTEmbed_316  BERTEmbed_317  BERTEmbed_318  BERTEmbed_319  BERTEmbed_320  BERTEmbed_321  BERTEmbed_322  BERTEmbed_323  BERTEmbed_324  BERTEmbed_325  BERTEmbed_326  BERTEmbed_327  BERTEmbed_328  \\\n0         -0.187568      -0.095185       0.801337      -0.006089      -0.006455       0.335097       0.019561       0.125312       0.014889      -0.014618       0.266433       0.083708       0.067822      -0.258764      -0.036717      -0.049954       0.134303       0.594728      -0.038062   \n1         -0.182584      -0.184029       0.710894       0.001453      -0.044658       0.246526       0.135383       0.114494      -0.045414       0.106293       0.202201      -0.032879       0.054366      -0.325524      -0.023439      -0.206997       0.230366       0.580575      -0.136127   \n2         -0.225695      -0.080953       0.767048       0.120815       0.037046       0.443890      -0.038200       0.139776      -0.007890       0.042617       0.137429      -0.126990       0.256451      -0.212291       0.013423      -0.107143       0.051471       0.459323      -0.053593   \n3         -0.120918      -0.186633       0.683703       0.010012       0.216744       0.246107       0.141247       0.027905       0.031845      -0.282164       0.259298       0.131470       0.107494      -0.260831      -0.060890      -0.054568       0.088920       0.531682      -0.120472   \n4         -0.318600      -0.043874       0.766777       0.016044       0.027015       0.390844      -0.054624       0.145646      -0.028515       0.047920       0.219566       0.028649       0.115967      -0.313340      -0.060640      -0.053464       0.084669       0.566006      -0.121154   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797      -0.257157       0.007309       0.818317      -0.056282       0.004714       0.332163      -0.038529       0.210475      -0.023986       0.130618       0.125285      -0.129686       0.201329      -0.336717       0.050248      -0.190869       0.126133       0.479797      -0.125874   \n5798      -0.309014       0.015384       0.652661      -0.219742       0.040949       0.482838       0.037195       0.267366       0.083669       0.163652       0.130848      -0.030017       0.076752      -0.287414       0.059392      -0.348479       0.149241       0.502908      -0.075915   \n5799      -0.451516      -0.043454       0.744394      -0.080858       0.029481       0.275468      -0.051814       0.212620       0.076816       0.094565       0.147443       0.135361       0.157544      -0.337319       0.065566      -0.019312       0.201026       0.522426      -0.001718   \n5800      -0.232363      -0.119546       0.704545      -0.009581      -0.037799       0.230507      -0.030165       0.248764       0.027655       0.059678       0.216176       0.060032       0.140843      -0.376290      -0.075518      -0.075188       0.020103       0.543174      -0.099281   \n5801      -0.249727      -0.158133       0.535342      -0.146624      -0.063998       0.302977       0.005873       0.106122      -0.038791       0.136180       0.209774      -0.014466       0.178056      -0.420407       0.081944      -0.129694       0.161161       0.574781      -0.092864   \n\n      BERTEmbed_329  BERTEmbed_330  BERTEmbed_331  BERTEmbed_332  BERTEmbed_333  BERTEmbed_334  BERTEmbed_335  BERTEmbed_336  BERTEmbed_337  BERTEmbed_338  BERTEmbed_339  BERTEmbed_340  BERTEmbed_341  BERTEmbed_342  BERTEmbed_343  BERTEmbed_344  BERTEmbed_345  BERTEmbed_346  BERTEmbed_347  \\\n0          0.054395       0.002550      -0.049797       0.119587      -0.018863      -0.317392       0.237164       0.136618       0.213643      -0.247811      -0.104963       0.063190       0.234255       0.232615      -0.984170       0.438266       0.066568      -0.334039       0.634895   \n1         -0.152403      -0.303149      -0.112432       0.164106      -0.058271      -0.334430       0.247042       0.271443       0.034018      -0.484950      -0.168721       0.282506       0.232015       0.287673      -1.112222       0.519301       0.322298      -0.415515       0.569293   \n2         -0.208996      -0.112290      -0.077355       0.105256      -0.180351      -0.364925       0.073399       0.243976       0.074563      -0.365365      -0.207265       0.258874       0.132119       0.181021      -1.040165       0.479087       0.109802      -0.398881       0.446057   \n3          0.120701       0.159719       0.001423       0.018898       0.067823      -0.213029       0.186586       0.154574      -0.103128      -0.341050      -0.078466       0.346418       0.189367       0.200054      -0.882663       0.150782       0.204444      -0.301041       0.509706   \n4         -0.022832      -0.060789      -0.028456       0.117863      -0.074391      -0.267706       0.117541       0.155247       0.035813      -0.261000      -0.154158       0.226752       0.350654       0.185459      -1.078435       0.489120       0.108247      -0.266860       0.648395   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797      -0.070950      -0.143692      -0.130181       0.171741      -0.076880      -0.388324       0.257433       0.143909       0.151024      -0.297866      -0.128111       0.376689       0.175521       0.118543      -1.133204       0.476384       0.069075      -0.356579       0.563429   \n5798       0.002490      -0.102679      -0.295635       0.048229      -0.033648      -0.122788       0.155117       0.154661      -0.002394      -0.235206      -0.161113       0.317246       0.086470       0.268052      -1.088316       0.572844       0.250123      -0.249332       0.572504   \n5799       0.113716       0.016947      -0.213174       0.145095      -0.028782      -0.140021       0.004434       0.106501       0.185751      -0.363230      -0.103315       0.197040       0.297601       0.050599      -1.157263       0.399676       0.202964      -0.318514       0.595788   \n5800      -0.195389       0.038402      -0.135274       0.083009      -0.135363      -0.405671       0.093424       0.161085       0.123508      -0.354957      -0.134463       0.188011       0.229838       0.132700      -1.069505       0.373192       0.240243      -0.358311       0.544328   \n5801      -0.030919      -0.202957      -0.108888       0.133909      -0.198606      -0.336170       0.137045       0.234870      -0.011704      -0.385778      -0.085587       0.227765       0.107466       0.296755      -1.083900       0.456709       0.227758      -0.425759       0.503818   \n\n      BERTEmbed_348  BERTEmbed_349  BERTEmbed_350  BERTEmbed_351  BERTEmbed_352  BERTEmbed_353  BERTEmbed_354  BERTEmbed_355  BERTEmbed_356  BERTEmbed_357  BERTEmbed_358  BERTEmbed_359  BERTEmbed_360  BERTEmbed_361  BERTEmbed_362  BERTEmbed_363  BERTEmbed_364  BERTEmbed_365  BERTEmbed_366  \\\n0          0.169662      -0.223718       0.348286      -0.298877       0.417768      -0.019382       0.141550       0.456167      -0.005118       0.146783      -0.174523       0.248328      -0.864120       0.078204       0.194633       0.350986      -0.079671      -0.049141       0.081007   \n1          0.148346      -0.190594       0.320587      -0.360830       0.643503       0.153274       0.095032       0.468780      -0.071511       0.089628      -0.156166       0.212629      -0.797346       0.087646       0.162812       0.334125      -0.002642      -0.156543       0.145238   \n2          0.233638      -0.221418       0.420982      -0.365871       0.286442       0.104808       0.106781       0.328672      -0.008369      -0.015763      -0.186026       0.322232      -0.774753       0.172023       0.086287       0.247828       0.124542       0.007692       0.133141   \n3          0.192786      -0.238708       0.273323      -0.439996       0.365666      -0.015508       0.151713       0.622219       0.017667       0.071008      -0.209177       0.249654      -0.633336       0.072947       0.141322       0.333345      -0.165535       0.035011       0.097019   \n4          0.287255      -0.117844       0.354066      -0.416070       0.345341       0.081981       0.127965       0.430397      -0.033595       0.146656      -0.145624       0.264575      -0.743011       0.066035       0.111831       0.342469      -0.121586      -0.025412       0.168367   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797       0.285773      -0.155253       0.305933      -0.367506       0.540473      -0.021083       0.066829       0.402238      -0.075639       0.109660      -0.123663       0.239306      -0.705540       0.060390       0.073319       0.389951      -0.127743      -0.055209       0.170106   \n5798       0.289968      -0.056845       0.354439      -0.385794       0.514765      -0.034586       0.081908       0.694056       0.012635       0.114672      -0.177682       0.200928      -0.724951       0.023766       0.051795       0.347250      -0.017165      -0.132733       0.114947   \n5799       0.298106      -0.230524       0.286361      -0.462133       0.462410       0.000655       0.154841       0.407353      -0.051175       0.152394      -0.191501       0.315363      -0.713991       0.023393       0.080166       0.292353       0.031207      -0.088986       0.029142   \n5800       0.173348      -0.082199       0.358428      -0.342664       0.371657       0.042953       0.130622       0.415231      -0.026164       0.173615      -0.139402       0.210499      -0.817762      -0.040684       0.111804       0.409951      -0.075350       0.051719       0.048335   \n5801       0.275216      -0.120936       0.361518      -0.371402       0.531414       0.032738       0.071400       0.516245      -0.113635       0.087816      -0.195519       0.306917      -0.882524       0.126712       0.042555       0.366912       0.031172      -0.120738       0.112366   \n\n      BERTEmbed_367  BERTEmbed_368  BERTEmbed_369  BERTEmbed_370  BERTEmbed_371  BERTEmbed_372  BERTEmbed_373  BERTEmbed_374  BERTEmbed_375  BERTEmbed_376  BERTEmbed_377  BERTEmbed_378  BERTEmbed_379  BERTEmbed_380  BERTEmbed_381  BERTEmbed_382  BERTEmbed_383  BERTEmbed_384  BERTEmbed_385  \\\n0         -0.053641      -0.010466      -0.311530      -0.232820       0.022677      -0.070380       0.228312       0.247126      -0.047978      -0.732083      -1.484927       0.258183       0.359196       0.495749       0.134124       0.201615      -0.158650      -0.109172       0.130224   \n1         -0.261697       0.051554      -0.294390      -0.102898      -0.025798       0.094722       0.215585       0.035600       0.065640      -0.693600      -1.572459       0.251762       0.318642       0.574410       0.121396       0.218621      -0.145323      -0.043381       0.151153   \n2         -0.148554       0.214363      -0.158922      -0.103673      -0.085503       0.199857       0.305532       0.222068      -0.073283      -0.889415      -1.420962       0.218933       0.352416       0.599981       0.098127       0.273358      -0.213906       0.023297       0.159789   \n3         -0.043437       0.211305      -0.400283      -0.257370       0.095195      -0.101903       0.287008       0.087937      -0.028010      -0.725253      -1.573878       0.223328       0.463709       0.323709       0.137103       0.198513      -0.173492      -0.067455       0.183557   \n4         -0.033360       0.066971      -0.284110      -0.227425       0.059063      -0.028301       0.208341       0.172644      -0.195995      -0.667109      -1.659434       0.234415       0.411392       0.557919       0.171800       0.130507      -0.081883      -0.085766       0.128735   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797      -0.089865       0.123567      -0.181661      -0.124836      -0.023484       0.244534       0.274888       0.183664      -0.052647      -0.804746      -1.542918       0.306663       0.391627       0.741156      -0.032830       0.094576      -0.022537       0.060180       0.105492   \n5798       0.017597       0.142115      -0.236726      -0.154013       0.126272       0.257089       0.212672       0.101434      -0.005463      -0.733822      -1.462212       0.298860       0.449988       0.733735      -0.047313       0.145938       0.087589      -0.005651       0.062469   \n5799      -0.092543       0.009028      -0.192571      -0.057543      -0.002781       0.109127       0.140147       0.191238       0.028455      -0.702601      -1.635913       0.195723       0.304195       0.470695      -0.003513       0.220291       0.008413       0.040587      -0.065108   \n5800       0.014924       0.132361      -0.348198      -0.190974      -0.102708       0.095096       0.235608       0.170968       0.016238      -0.762695      -1.575914       0.189854       0.332282       0.293242       0.073670       0.241289      -0.093960      -0.054013       0.085197   \n5801      -0.030130       0.181367      -0.267578      -0.156898      -0.088706       0.259204       0.217878       0.243013       0.069513      -0.753294      -1.555932       0.227741       0.396136       0.499281       0.065283       0.187489       0.014420       0.000720       0.163176   \n\n      BERTEmbed_386  BERTEmbed_387  BERTEmbed_388  BERTEmbed_389  BERTEmbed_390  BERTEmbed_391  BERTEmbed_392  BERTEmbed_393  BERTEmbed_394  BERTEmbed_395  BERTEmbed_396  BERTEmbed_397  BERTEmbed_398  BERTEmbed_399  BERTEmbed_400  BERTEmbed_401  BERTEmbed_402  BERTEmbed_403  BERTEmbed_404  \\\n0          0.076583       0.148866       0.102369       0.497895      -0.051056       0.102688      -0.061415      -0.181574       0.147057      -0.318179       0.168422       0.179673       0.135512       0.137194      -0.105513       0.090339       0.332361      -0.429137       0.738065   \n1          0.075572      -0.094515      -0.013944       0.373636      -0.027739       0.039750      -0.118055      -0.249140       0.197492      -0.402778       0.195925       0.229122       0.205297       0.186908      -0.126872       0.075641       0.112252      -0.250059       0.615707   \n2          0.117926       0.043708      -0.046526       0.415024      -0.190328       0.064324      -0.088316      -0.122251       0.275452      -0.467009       0.127770       0.199551       0.243181       0.146670       0.001147      -0.009912       0.333561      -0.365534       0.593835   \n3          0.096656       0.082774      -0.074274       0.505964      -0.171689       0.059159      -0.064126      -0.191974       0.280933      -0.286824       0.055742       0.164370       0.233401       0.110808      -0.109585       0.057272       0.326034      -0.300844       0.570521   \n4          0.082796       0.103305       0.057104       0.503316      -0.050694       0.051762      -0.043146      -0.236718       0.212253      -0.284349       0.225131       0.317554       0.233766       0.166256      -0.067030       0.125299       0.325762      -0.413970       0.624875   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797       0.188438       0.057571       0.074640       0.437863      -0.137878       0.034812      -0.141849      -0.247712       0.279302      -0.404679       0.172610       0.236249       0.231468       0.270060       0.060169       0.073054       0.194955      -0.459749       0.557014   \n5798       0.173531       0.093825      -0.029331       0.444125      -0.021245      -0.117827      -0.054896      -0.165349       0.231078      -0.454145       0.172870       0.157398       0.218177       0.180482      -0.245151       0.159684       0.412418      -0.476833       0.602345   \n5799       0.141092       0.120222      -0.052011       0.362969      -0.036765       0.036839      -0.079194      -0.276581       0.202655      -0.269324       0.148433       0.212523       0.383705       0.054178      -0.179690       0.011319       0.251478      -0.290414       0.648630   \n5800       0.129575       0.185716       0.003847       0.454327      -0.189133       0.090189      -0.102188      -0.075440       0.384681      -0.263391       0.044862       0.283937       0.098191       0.082367      -0.012841      -0.011989       0.335970      -0.274440       0.668942   \n5801       0.210455       0.000319      -0.045226       0.426808      -0.122421       0.120248      -0.226760      -0.119434       0.107869      -0.360259       0.178765       0.219603       0.330241       0.114662      -0.062618       0.069053       0.230423      -0.304907       0.581747   \n\n      BERTEmbed_405  BERTEmbed_406  BERTEmbed_407  BERTEmbed_408  BERTEmbed_409  BERTEmbed_410  BERTEmbed_411  BERTEmbed_412  BERTEmbed_413  BERTEmbed_414  BERTEmbed_415  BERTEmbed_416  BERTEmbed_417  BERTEmbed_418  BERTEmbed_419  BERTEmbed_420  BERTEmbed_421  BERTEmbed_422  BERTEmbed_423  \\\n0         -0.029155       0.107776       0.450315       0.228201      -0.107785       0.047060      -0.067315      -0.356229       0.025094      -0.003247       0.239771       0.146900       0.051395       0.281231       0.123686       0.131963      -0.114395      -0.121030       0.183903   \n1          0.055545       0.119774       0.347842       0.190807      -0.093280       0.112409      -0.100943      -0.259254       0.037487      -0.083349       0.245945       0.271640       0.169596       0.229249       0.082409       0.038823       0.000280      -0.066066       0.107624   \n2          0.239373       0.084423       0.394833       0.160417      -0.139481       0.061734      -0.153677      -0.277519       0.095881      -0.063011       0.218702       0.315401       0.182546       0.184343       0.117218       0.072643      -0.155542       0.177091       0.139776   \n3          0.077208      -0.011745       0.308210       0.215719      -0.132078       0.108649      -0.037369      -0.193766       0.028883       0.018203       0.161684       0.160041       0.021596       0.327838       0.142262       0.234510      -0.289800       0.097334       0.142094   \n4          0.131539       0.093488       0.330448       0.166375      -0.142191       0.048861      -0.120444      -0.253740       0.072407       0.067012       0.207322       0.190528       0.064146       0.251848       0.099032       0.165080      -0.137715      -0.140599       0.209397   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797       0.182896       0.156694       0.158557       0.142131      -0.224718       0.012433      -0.079857      -0.403267       0.034917      -0.190151       0.304608       0.197498       0.148168       0.146923       0.102384       0.227980       0.035478       0.012525       0.088056   \n5798       0.236185       0.057630      -0.057051       0.136654      -0.238345       0.143253      -0.011439      -0.291248       0.008720      -0.160638       0.189454       0.080644       0.140982       0.278438       0.168202       0.262578      -0.104650       0.037699       0.136130   \n5799       0.127239       0.121155       0.201968       0.302226      -0.139846       0.146931      -0.161217      -0.229362       0.017145       0.014194       0.181324       0.166731       0.208772       0.112288       0.012724       0.250854      -0.029290       0.164436       0.219871   \n5800      -0.021293       0.078891       0.444623       0.181930      -0.014003       0.063660      -0.121199      -0.429821      -0.061743      -0.004597       0.192542       0.180476       0.067924       0.292306       0.092897       0.079233      -0.142078       0.037863       0.170807   \n5801       0.201269       0.086544       0.303610       0.278336      -0.115036       0.096076      -0.113682      -0.229704      -0.037115       0.046386       0.272311       0.158997       0.098373       0.164840       0.107994       0.130272      -0.001865       0.055415       0.114537   \n\n      BERTEmbed_424  BERTEmbed_425  BERTEmbed_426  BERTEmbed_427  BERTEmbed_428  BERTEmbed_429  BERTEmbed_430  BERTEmbed_431  BERTEmbed_432  BERTEmbed_433  BERTEmbed_434  BERTEmbed_435  BERTEmbed_436  BERTEmbed_437  BERTEmbed_438  BERTEmbed_439  BERTEmbed_440  BERTEmbed_441  BERTEmbed_442  \\\n0         -0.183826       0.012366       0.325674      -0.062778       0.320075      -0.104069      -0.287190       0.000626       0.144869       0.005394       0.408010      -0.078483       0.338656       0.199368       0.276234       0.403440       0.105201       0.111741       0.130294   \n1         -0.265498      -0.134188       0.246832      -0.045114      -0.118571      -0.147457      -0.151012      -0.022420      -0.024002       0.108842       0.335836      -0.066827       0.171031       0.328363       0.267881       0.358730       0.233092       0.064519       0.063946   \n2         -0.155007      -0.156836       0.245269       0.033423      -0.042583      -0.166833      -0.126384       0.015218       0.185972       0.146797       0.399611      -0.221173       0.145568       0.157571       0.297717       0.424670       0.238460       0.090002       0.161389   \n3         -0.135536      -0.053005       0.185052      -0.084795       0.112072      -0.115877      -0.070810      -0.003475       0.091526       0.089123       0.296380      -0.057218       0.282161       0.147720       0.231796       0.231546       0.109525       0.208487       0.143153   \n4         -0.117788      -0.106100       0.333269      -0.041248       0.127098      -0.093145      -0.167996      -0.006999       0.056609       0.141041       0.481331      -0.103233       0.327625       0.272374       0.196827       0.363769       0.037420       0.105824       0.148034   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797      -0.237266      -0.013573       0.428837       0.010675      -0.011769      -0.206249      -0.093406      -0.055180       0.272245       0.113926       0.415245      -0.130399       0.292666       0.346836       0.326721       0.422867       0.221530       0.013407       0.104407   \n5798      -0.215785      -0.282512       0.302961      -0.053165       0.005802       0.060965      -0.098238      -0.040212      -0.090117       0.168087       0.398319      -0.121302       0.281986       0.317084       0.313597       0.358396       0.009464       0.033063       0.061557   \n5799      -0.215580      -0.198178       0.460069       0.042292      -0.025143      -0.035915      -0.283712      -0.114488       0.154105       0.107332       0.355283       0.014019       0.336870       0.219213       0.333101       0.308347       0.154799       0.130365      -0.012931   \n5800      -0.144208      -0.068382       0.274768      -0.031804       0.060594      -0.124615      -0.241654       0.050052       0.159044       0.055887       0.356343      -0.195894       0.323094       0.046594       0.206693       0.420075       0.145882       0.007916       0.158233   \n5801      -0.222612      -0.111380       0.273463       0.005236       0.048216      -0.162072      -0.169301      -0.034169       0.040331       0.215255       0.398604      -0.137986       0.154077       0.152590       0.188301       0.362347       0.127100       0.060738       0.090573   \n\n      BERTEmbed_443  BERTEmbed_444  BERTEmbed_445  BERTEmbed_446  BERTEmbed_447  BERTEmbed_448  BERTEmbed_449  BERTEmbed_450  BERTEmbed_451  BERTEmbed_452  BERTEmbed_453  BERTEmbed_454  BERTEmbed_455  BERTEmbed_456  BERTEmbed_457  BERTEmbed_458  BERTEmbed_459  BERTEmbed_460  BERTEmbed_461  \\\n0          0.073811      -0.160810       4.747331       0.072952       0.055920       0.006768       0.349222      -0.130795       0.379010      -0.010385       0.090603       0.393199      -0.103349       0.147315      -0.291891      -0.066541      -0.279493       0.305302       0.201402   \n1         -0.036409      -0.088903       4.735420       0.224308       0.247671       0.120584       0.165223      -0.192003       0.529650       0.013423      -0.024935       0.277555      -0.126389       0.087202      -0.320496       0.010809      -0.227202       0.341529       0.150292   \n2          0.013415      -0.171204       4.659614       0.319820       0.207910       0.019596       0.113093      -0.021126       0.397520       0.028851       0.038145       0.269388      -0.071700       0.085228      -0.310905      -0.075097      -0.396189       0.218256       0.225526   \n3          0.096822      -0.038644       4.589263       0.073934       0.274289       0.075938       0.610581      -0.103872       0.474489       0.106104      -0.147438       0.399179      -0.095195       0.144800      -0.423337      -0.159502      -0.319680       0.184307       0.261812   \n4         -0.010754      -0.015208       4.724798       0.138368       0.190422       0.003221       0.348576      -0.065356       0.328944      -0.051279       0.037169       0.442660      -0.079182       0.145942      -0.380691      -0.099134      -0.266771       0.314336       0.191153   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797       0.030790       0.004473       4.587753       0.243332       0.110125       0.082508       0.216538      -0.103398       0.499644       0.061743      -0.043593       0.285424      -0.119368       0.134653      -0.305073      -0.134592      -0.288831       0.482646       0.230063   \n5798       0.028780      -0.001849       4.551806       0.088864       0.324186       0.097335       0.344112      -0.047675       0.406895       0.207308      -0.037088       0.404700      -0.060503       0.111403      -0.275313      -0.039107      -0.246947       0.280948       0.148235   \n5799       0.004880      -0.049432       4.661571      -0.117565       0.313893       0.104418       0.422277      -0.288065       0.554124       0.167493       0.209744       0.141981      -0.156686       0.060455      -0.225565      -0.123161      -0.345784       0.573043       0.070739   \n5800      -0.011257      -0.126677       4.767896       0.133264       0.097943       0.119734       0.389167       0.017960       0.301269       0.151791       0.030799       0.473789      -0.204830       0.194116      -0.335194       0.007883      -0.459456       0.333442       0.128281   \n5801       0.041136       0.025562       4.755297       0.104469       0.359572       0.067171       0.349734      -0.193996       0.463586       0.094244       0.072032       0.337075      -0.065795       0.054828      -0.313711      -0.012513      -0.260075       0.425665       0.153244   \n\n      BERTEmbed_462  BERTEmbed_463  BERTEmbed_464  BERTEmbed_465  BERTEmbed_466  BERTEmbed_467  BERTEmbed_468  BERTEmbed_469  BERTEmbed_470  BERTEmbed_471  BERTEmbed_472  BERTEmbed_473  BERTEmbed_474  BERTEmbed_475  BERTEmbed_476  BERTEmbed_477  BERTEmbed_478  BERTEmbed_479  BERTEmbed_480  \\\n0          0.041830       0.200939       0.185602      -0.265424       0.502218      -0.028360      -0.168451      -0.105645      -0.079321      -0.047105       0.021278      -0.349123       0.029367       0.190332      -0.095118       0.012601      -0.382088       0.352415      -0.074742   \n1          0.071865       0.166022       0.127493      -0.122384       0.515608       0.017776      -0.269383      -0.076361      -0.196708      -0.073824       0.114449      -0.230122       0.152294       0.139875       0.101156       0.020042      -0.119836       0.389177      -0.135395   \n2          0.089454       0.199574       0.320235      -0.121477       0.634486       0.001248      -0.108045      -0.134644      -0.147585      -0.144309       0.155484      -0.133366       0.137158       0.125155      -0.200577      -0.099577      -0.337281       0.489089      -0.184713   \n3          0.030999       0.249410       0.222796      -0.170890       0.424784      -0.217573      -0.104660      -0.101065      -0.100737      -0.017755      -0.058625      -0.294678      -0.031522       0.184311      -0.138472      -0.015762      -0.423487       0.316276      -0.105167   \n4         -0.050169       0.161240       0.229753      -0.225471       0.489279      -0.081877      -0.146585      -0.173832      -0.183103      -0.082342       0.021002      -0.266867       0.138740       0.096133      -0.015366      -0.029622      -0.374682       0.414259      -0.090948   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797       0.267644       0.106669       0.168055      -0.120968       0.520531      -0.173883      -0.146917      -0.128240      -0.228523      -0.183027      -0.016809      -0.307935       0.234795       0.059595      -0.040782      -0.088659      -0.355450       0.449389      -0.062130   \n5798       0.080893       0.138986       0.339330      -0.127223       0.573909      -0.206516      -0.074858      -0.116951      -0.095827       0.018867       0.065639      -0.363526       0.109518      -0.032558       0.017930      -0.078727      -0.323117       0.471757      -0.207449   \n5799       0.023572       0.294897      -0.022959      -0.236122       0.465324      -0.121645      -0.267481      -0.087629      -0.226174      -0.137586       0.026345      -0.295932       0.064476       0.089143      -0.042295      -0.046266      -0.238839       0.281470       0.001068   \n5800      -0.003074       0.208963       0.325823      -0.093666       0.519885       0.007528      -0.207777      -0.171610      -0.269769      -0.122947      -0.042812      -0.189893       0.178181       0.167261      -0.035295      -0.016297      -0.466228       0.400818      -0.044528   \n5801       0.097730       0.182984       0.274651      -0.070109       0.506160      -0.084080      -0.289241      -0.109700      -0.218080      -0.122398       0.089358      -0.249233       0.068926       0.155156      -0.002891      -0.108785      -0.118666       0.297574      -0.016073   \n\n      BERTEmbed_481  BERTEmbed_482  BERTEmbed_483  BERTEmbed_484  BERTEmbed_485  BERTEmbed_486  BERTEmbed_487  BERTEmbed_488  BERTEmbed_489  BERTEmbed_490  BERTEmbed_491  BERTEmbed_492  BERTEmbed_493  BERTEmbed_494  BERTEmbed_495  BERTEmbed_496  BERTEmbed_497  BERTEmbed_498  BERTEmbed_499  \\\n0         -0.113953      -0.032978       0.584896       0.161829      -0.054128       0.538985       0.276367       0.136710      -0.199390       0.451597       0.319561       0.349828      -0.516844       0.000485      -0.179471       0.159981      -0.318736       0.053733       0.168461   \n1         -0.320353       0.068988       0.461596       0.101328      -0.042104       0.443206       0.194410       0.066657      -0.003343       0.383663       0.328565       0.650958      -0.660451      -0.044131      -0.091020       0.058883      -0.268196       0.027388       0.293735   \n2         -0.219138      -0.055203       0.415248       0.018134       0.163483       0.425787       0.293932      -0.061277      -0.036749       0.349015       0.257537       0.506231      -0.484376      -0.086010      -0.232735       0.130123      -0.299626       0.178947       0.333248   \n3         -0.209806       0.103387       0.440179       0.305836      -0.031058       0.429843       0.437899       0.098855      -0.366449       0.394186       0.430007       0.444887      -0.669890      -0.163727      -0.139011       0.039955      -0.354806       0.004578       0.112291   \n4         -0.127830      -0.053346       0.499038       0.171812      -0.047922       0.474412       0.269469       0.120755      -0.271437       0.359295       0.356456       0.413065      -0.531105      -0.067939      -0.269381       0.134331      -0.347513       0.075619       0.244737   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797      -0.260569      -0.006634       0.399993       0.040218      -0.007471       0.482334       0.200412       0.028204      -0.021496       0.396561       0.390499       0.654072      -0.567551      -0.032819      -0.232219       0.084525      -0.319877       0.190442       0.321887   \n5798      -0.244216      -0.023191       0.405933      -0.038485      -0.001030       0.430866       0.281027       0.116462       0.009130       0.253667       0.384996       0.290595      -0.673216      -0.124293      -0.053923       0.071955      -0.390182       0.080488       0.247643   \n5799      -0.169031       0.017892       0.347520       0.114027       0.026536       0.539894       0.258185       0.217490      -0.030520       0.407385       0.278295       0.330414      -0.487602      -0.046118      -0.106195       0.018743      -0.328588      -0.053319       0.127363   \n5800      -0.086911       0.057404       0.592008       0.096207      -0.142642       0.501376       0.291177       0.088048      -0.181443       0.470153       0.336471       0.586060      -0.397033      -0.036363      -0.106999       0.120221      -0.254843       0.058020       0.228707   \n5801      -0.213562       0.044780       0.422873       0.097924       0.078061       0.376828       0.193599       0.114676      -0.139354       0.426733       0.300406       0.556409      -0.505137      -0.129443      -0.098371       0.040871      -0.325016       0.066747       0.282382   \n\n      BERTEmbed_500  BERTEmbed_501  BERTEmbed_502  BERTEmbed_503  BERTEmbed_504  BERTEmbed_505  BERTEmbed_506  BERTEmbed_507  BERTEmbed_508  BERTEmbed_509  BERTEmbed_510  BERTEmbed_511  BERTEmbed_512  BERTEmbed_513  BERTEmbed_514  BERTEmbed_515  BERTEmbed_516  BERTEmbed_517  BERTEmbed_518  \\\n0         -0.083542      -0.128236       0.099722       0.067470       0.119408       0.163848      -0.216542       0.249147      -0.213316      -0.413827      -0.094823       0.005856      -0.065209       0.421940       0.087920      -0.088703       0.061222      -0.012150       0.514841   \n1          0.008397      -0.088674       0.139552       0.075708       0.099574       0.231444      -0.298517       0.207255      -0.166472      -0.382062      -0.107599      -0.138740      -0.054414       0.417538      -0.191485      -0.202802       0.042192       0.023265       0.540775   \n2          0.087834       0.071150       0.285669      -0.066506       0.115129       0.172095      -0.244898       0.172538      -0.266423      -0.298037      -0.065096      -0.147692      -0.164367       0.165593      -0.122191      -0.029296       0.136410       0.007887       0.600133   \n3         -0.081875      -0.075048       0.251497       0.137177       0.042746       0.233811       0.112302       0.166262      -0.189081      -0.584402      -0.099310      -0.153482      -0.085784       0.657140       0.058212      -0.069985       0.172562      -0.122488       0.499766   \n4         -0.087617      -0.105489       0.162576       0.053325       0.107291       0.131072      -0.161796       0.182106      -0.027949      -0.365878      -0.106784      -0.101995      -0.096707       0.455633       0.016582      -0.106859       0.047870      -0.118545       0.522409   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797      -0.010718      -0.052586       0.174991       0.134580       0.043581       0.093323      -0.222376       0.190514      -0.259734      -0.319978      -0.094615      -0.202123      -0.146689       0.343829      -0.132182      -0.128431       0.085141       0.012305       0.669596   \n5798      -0.105171      -0.139464       0.106522       0.039369       0.145788       0.028975      -0.202725       0.215933      -0.220481      -0.393871      -0.128812      -0.161213      -0.149792       0.406497      -0.071667      -0.118462       0.166328      -0.021144       0.613396   \n5799      -0.132542      -0.227554       0.057860       0.112070      -0.000287       0.030610       0.028930       0.198467      -0.303935      -0.429734      -0.318576      -0.224360      -0.187954       0.573480      -0.117913      -0.152062       0.116279      -0.055709       0.393670   \n5800      -0.120829       0.031817       0.174521       0.023215       0.060007       0.202050      -0.074929       0.286043      -0.148116      -0.487437      -0.210278      -0.154281      -0.062967       0.345353       0.013007       0.042138       0.055535      -0.071465       0.543413   \n5801      -0.055999      -0.051167       0.130796       0.071629       0.006048       0.187473      -0.163047       0.118924      -0.132957      -0.440336      -0.117296      -0.221083      -0.114240       0.503192      -0.111579       0.021600       0.062059      -0.028263       0.479920   \n\n      BERTEmbed_519  BERTEmbed_520  BERTEmbed_521  BERTEmbed_522  BERTEmbed_523  BERTEmbed_524  BERTEmbed_525  BERTEmbed_526  BERTEmbed_527  BERTEmbed_528  BERTEmbed_529  BERTEmbed_530  BERTEmbed_531  BERTEmbed_532  BERTEmbed_533  BERTEmbed_534  BERTEmbed_535  BERTEmbed_536  BERTEmbed_537  \\\n0         -0.047247      -0.166690      -0.115741      -0.286324      -0.092178      -0.678961       0.259359      -0.159371       0.254227       0.149190       0.167796       0.210738      -0.151450      -0.210756       1.435943      -0.078241      -0.114692       0.008090       0.467595   \n1         -0.000280      -0.092398      -0.095384      -0.215992      -0.106038      -0.676636       0.164287      -0.148349       0.110805       0.116059       0.173325       0.277992      -0.185510      -0.183296       1.302565       0.012073      -0.008259       0.021889       0.438934   \n2          0.049115      -0.159277      -0.125493      -0.228019      -0.131946      -0.667638       0.245047      -0.215789       0.237266       0.253201       0.245690       0.207296      -0.170030      -0.176699       1.260766       0.068533      -0.155212       0.188573       0.553603   \n3          0.023834      -0.063930      -0.054424      -0.394738       0.023029      -0.566956       0.322194      -0.174802       0.359565       0.191352       0.214928       0.081044       0.014249      -0.248099       1.306929       0.010150      -0.154674      -0.049906       0.441014   \n4         -0.014396      -0.184805      -0.137146      -0.316302      -0.093624      -0.666288       0.257855      -0.164358       0.212663       0.082727       0.181313       0.189990      -0.163090      -0.235879       1.271474      -0.066186      -0.128729      -0.005365       0.469518   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797       0.035358      -0.067230      -0.077602      -0.287588      -0.124050      -0.624496       0.215740      -0.245675       0.171061       0.169850       0.131098       0.243320      -0.197165      -0.150008       1.173477      -0.001061      -0.010695       0.073038       0.475230   \n5798       0.035741      -0.240043      -0.090992      -0.387077      -0.023804      -0.801698       0.247690      -0.174178       0.303614       0.107106       0.160636       0.390093      -0.105251      -0.252691       1.398387      -0.056694      -0.004260       0.077852       0.508024   \n5799       0.018342      -0.287295      -0.118957      -0.241666      -0.060184      -0.568609       0.229785      -0.164359       0.106208       0.025059       0.109468       0.230713      -0.120605      -0.185707       1.030239      -0.072780      -0.116536       0.109835       0.420646   \n5800       0.073271      -0.163895      -0.116848      -0.344962      -0.054594      -0.683111       0.220683      -0.256915       0.155769       0.165155       0.228100       0.164370      -0.162400      -0.093309       1.319607      -0.204972      -0.223999      -0.173336       0.466747   \n5801       0.037583      -0.183235      -0.087970      -0.276075      -0.017940      -0.705892       0.294718      -0.137707       0.134199       0.138118       0.127715       0.259978      -0.172032      -0.174915       1.196315      -0.005313      -0.054269      -0.028721       0.501213   \n\n      BERTEmbed_538  BERTEmbed_539  BERTEmbed_540  BERTEmbed_541  BERTEmbed_542  BERTEmbed_543  BERTEmbed_544  BERTEmbed_545  BERTEmbed_546  BERTEmbed_547  BERTEmbed_548  BERTEmbed_549  BERTEmbed_550  BERTEmbed_551  BERTEmbed_552  BERTEmbed_553  BERTEmbed_554  BERTEmbed_555  BERTEmbed_556  \\\n0          0.145489      -0.087178       0.481435      -0.047705       0.118655       0.455190      -0.040371       0.091085       0.016216       0.157984       0.027836       0.257078      -0.279486      -0.004686      -0.395863      -0.020518       0.082706      -0.445489       0.042649   \n1          0.177178      -0.159774       0.605235      -0.185618       0.083717       0.556896      -0.012802       0.076579      -0.019178       0.087928      -0.013053       0.340684      -0.164111       0.038193      -0.436777      -0.062536       0.094718      -0.480012       0.064211   \n2          0.021000      -0.220785       0.480360      -0.047067       0.076357       0.432700       0.008528       0.086129      -0.015649       0.200788       0.041094       0.397157      -0.256051      -0.006302      -0.502058       0.003792      -0.046427      -0.432073       0.089857   \n3          0.218925      -0.153914       0.488047      -0.005701       0.032783       0.438099      -0.177114       0.264958      -0.012098       0.172682       0.080806       0.314867       0.015568       0.052388      -0.425482      -0.033898       0.044292      -0.423019       0.104922   \n4          0.195297      -0.148033       0.540526      -0.038647       0.053315       0.487796      -0.068527       0.077622       0.021009       0.236534       0.055601       0.285885      -0.160817      -0.049537      -0.503420      -0.093137       0.011491      -0.493284       0.207891   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797       0.088670      -0.085849       0.470349      -0.007309       0.038967       0.458922       0.034546       0.127686       0.000078       0.219262       0.105941       0.465120      -0.190167      -0.095677      -0.425898       0.032250      -0.012627      -0.504825       0.212114   \n5798       0.251119      -0.285997       0.468437      -0.084137       0.060491       0.448465      -0.019801      -0.147670       0.010732       0.125005       0.080282       0.290036      -0.078184      -0.164123      -0.351651      -0.042004       0.071081      -0.441032       0.226924   \n5799       0.176525      -0.161021       0.563187      -0.015506       0.246615       0.554169      -0.086021       0.106225      -0.002149       0.049488      -0.074861       0.226262      -0.123194      -0.047453      -0.456172      -0.090524       0.097732      -0.551270       0.288022   \n5800       0.174668      -0.383195       0.527666       0.005899       0.108961       0.258122      -0.123884       0.086010       0.139231       0.236788       0.032440       0.209263      -0.147246       0.070680      -0.451074       0.073525       0.032705      -0.471250       0.193761   \n5801       0.083360      -0.264184       0.587294      -0.020360       0.097729       0.458523       0.031435       0.117152       0.067868       0.162279      -0.025534       0.341441      -0.088403      -0.025945      -0.288340       0.073795       0.104680      -0.528187       0.165488   \n\n      BERTEmbed_557  BERTEmbed_558  BERTEmbed_559  BERTEmbed_560  BERTEmbed_561  BERTEmbed_562  BERTEmbed_563  BERTEmbed_564  BERTEmbed_565  BERTEmbed_566  BERTEmbed_567  BERTEmbed_568  BERTEmbed_569  BERTEmbed_570  BERTEmbed_571  BERTEmbed_572  BERTEmbed_573  BERTEmbed_574  BERTEmbed_575  \\\n0          0.338311      -0.381562      -0.063263      -0.059470      -0.059927       0.297672      -0.069628      -0.407640      -0.082503       0.289098       0.337436      -0.082769       0.357320       0.176288       0.214432      -0.368707      -0.006962       0.433743       0.236393   \n1          0.595901      -0.420840      -0.078573      -0.044344      -0.094345       0.448299      -0.141775      -0.321848      -0.101493       0.042241       0.328914      -0.192891       0.483030       0.038726       0.311949      -0.353370      -0.042744       0.437307       0.138747   \n2          0.469271      -0.342020      -0.066909      -0.195322       0.057461       0.470980      -0.084404      -0.246583       0.020685       0.184780       0.290825      -0.261427       0.292839       0.070038       0.272943      -0.430346       0.071460       0.577150       0.060315   \n3          0.387087      -0.313701      -0.053402      -0.036802       0.057828       0.150301       0.020526      -0.413149       0.126196       0.307310       0.182301      -0.121925       0.189220       0.157032       0.136725      -0.461674      -0.020370       0.745425       0.150443   \n4          0.352552      -0.302255      -0.046455      -0.051519      -0.118091       0.251326      -0.070982      -0.426790       0.017510       0.127080       0.409236      -0.080476       0.285003       0.257981       0.041792      -0.331380      -0.024571       0.560636       0.156476   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797       0.476855      -0.380684       0.064843      -0.155434      -0.048308       0.410547      -0.091522      -0.325071      -0.056600       0.127995       0.509393      -0.102349       0.332908      -0.040991       0.108821      -0.328062      -0.019397       0.678357       0.112795   \n5798       0.514121      -0.331835       0.001260      -0.186533      -0.017505       0.353123       0.017999      -0.378622      -0.013584       0.069780       0.422661      -0.084303       0.404565       0.063142      -0.017066      -0.237108      -0.078840       0.743538       0.122300   \n5799       0.383723      -0.329955       0.030013      -0.066330       0.022795       0.353780      -0.121599      -0.254203       0.112318       0.251033       0.343733      -0.052206       0.339885      -0.014592       0.151311      -0.416623       0.003609       0.870064       0.265041   \n5800       0.423801      -0.320447      -0.001936      -0.077355       0.031141       0.391592      -0.190227      -0.397756       0.015962       0.286114       0.282336      -0.015202       0.438908       0.053051       0.230438      -0.459325       0.141847       0.486188       0.189290   \n5801       0.513850      -0.335499       0.026776      -0.173049      -0.011792       0.344168      -0.139736      -0.322970       0.063007       0.198765       0.285336      -0.104871       0.338772       0.141768       0.199322      -0.373704       0.005892       0.691010       0.159527   \n\n      BERTEmbed_576  BERTEmbed_577  BERTEmbed_578  BERTEmbed_579  BERTEmbed_580  BERTEmbed_581  BERTEmbed_582  BERTEmbed_583  BERTEmbed_584  BERTEmbed_585  BERTEmbed_586  BERTEmbed_587  BERTEmbed_588  BERTEmbed_589  BERTEmbed_590  BERTEmbed_591  BERTEmbed_592  BERTEmbed_593  BERTEmbed_594  \\\n0         -0.245224       0.358831       0.040904      -0.437277       0.061364      -0.360650       0.111130       0.309104      -0.351730      -0.155896       0.184967      -0.120664       0.107070      -0.267976       0.340889       0.591779       0.673692       0.400805       0.413043   \n1         -0.262991       0.233440      -0.178823      -0.372884      -0.049909      -0.436951       0.107153       0.160562      -0.262083      -0.193632       0.173114      -0.368331       0.188000      -0.444024       0.301579       0.668362       0.620575       0.437763       0.291784   \n2         -0.162282       0.127919      -0.041031      -0.262100       0.197708      -0.351543       0.309030       0.255049      -0.477788      -0.163560       0.198803      -0.487982       0.182566      -0.458819       0.327886       0.614070       0.611886       0.524146       0.424707   \n3         -0.224761       0.296795       0.013498      -0.309460       0.118734      -0.330776       0.074880       0.313029      -0.113355      -0.233489       0.215368       0.074380       0.071018      -0.170584       0.302396       0.560400       0.503578       0.258928       0.524561   \n4         -0.317845       0.302103       0.048535      -0.362336       0.088438      -0.399602       0.187124       0.268476      -0.284721      -0.087856       0.150417      -0.117420       0.160845      -0.296682       0.273953       0.572014       0.568337       0.387098       0.439211   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797      -0.170870       0.247348       0.202667      -0.443963       0.150961      -0.555595       0.167062       0.238220      -0.200563      -0.242487       0.193388      -0.307706       0.174820      -0.405816       0.403904       0.660097       0.552302       0.382159       0.294486   \n5798      -0.205030       0.319238       0.023861      -0.453573       0.186198      -0.312835       0.110523       0.206434      -0.258428      -0.181687       0.177046      -0.161115       0.122846      -0.356982       0.258704       0.550243       0.417440       0.360353       0.322013   \n5799      -0.280053       0.256732       0.207401      -0.444582       0.219045      -0.324177       0.179530       0.236902      -0.187423      -0.134858       0.135275      -0.198047       0.062975      -0.374228       0.195483       0.432166       0.604630       0.213389       0.416887   \n5800      -0.203692       0.314315      -0.015356      -0.358939       0.285941      -0.453243       0.167612       0.204932      -0.391239      -0.266462       0.154510      -0.238417       0.159891      -0.247684       0.226233       0.619110       0.569341       0.281572       0.477477   \n5801      -0.196880       0.251185       0.007133      -0.421356       0.123509      -0.427793       0.082607       0.210277      -0.272839      -0.106038       0.269046      -0.218318       0.118495      -0.310512       0.423977       0.607719       0.487851       0.288233       0.419006   \n\n      BERTEmbed_595  BERTEmbed_596  BERTEmbed_597  BERTEmbed_598  BERTEmbed_599  BERTEmbed_600  BERTEmbed_601  BERTEmbed_602  BERTEmbed_603  BERTEmbed_604  BERTEmbed_605  BERTEmbed_606  BERTEmbed_607  BERTEmbed_608  BERTEmbed_609  BERTEmbed_610  BERTEmbed_611  BERTEmbed_612  BERTEmbed_613  \\\n0         -0.057027       0.038008       0.179726       0.380631      -0.099256      -0.326425      -0.098433      -0.249388       0.335000       0.161901       0.151454      -0.124199      -0.039004       0.027825       0.268244      -0.021239      -0.209983      -0.060947       0.383124   \n1         -0.092909      -0.018777       0.270073       0.285350      -0.054565      -0.312149       0.000933      -0.081424       0.215595       0.180109       0.059430      -0.199825       0.016596       0.037411       0.251007      -0.100918      -0.117772      -0.010459       0.056066   \n2         -0.005190      -0.081319       0.278993       0.338263      -0.040779      -0.155405      -0.010572      -0.022277       0.479041      -0.011212      -0.034441      -0.177459       0.024584      -0.016893       0.140829       0.028988      -0.137176      -0.265032       0.289644   \n3          0.112997       0.078601       0.145657       0.240603      -0.030985      -0.358218      -0.130811      -0.310706       0.278885       0.160765      -0.041355      -0.080748      -0.089060      -0.202069       0.226148      -0.072207      -0.257342      -0.207745       0.418329   \n4          0.022400       0.082213       0.178233       0.297233      -0.095604      -0.361939      -0.065236      -0.194765       0.403799       0.063449       0.041826      -0.149846      -0.047050       0.010768       0.164774      -0.080460      -0.202783      -0.149669       0.379859   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797       0.020989      -0.010870       0.257566       0.252134      -0.117760      -0.213748       0.148738       0.028456       0.377452       0.239205      -0.124741      -0.279801      -0.054819       0.030798       0.141301      -0.031256      -0.121317      -0.124733       0.296470   \n5798       0.066126       0.154839       0.292147       0.244444      -0.108820      -0.356691       0.039248      -0.135377       0.392029       0.159210      -0.133916      -0.291248      -0.094988       0.039504       0.018454      -0.122758      -0.230232      -0.149548       0.280438   \n5799       0.012604      -0.006210       0.177151       0.329447       0.002061      -0.324079      -0.032845      -0.135431       0.275498       0.048453       0.105030      -0.149223      -0.079357      -0.132657       0.281119      -0.147471      -0.231247       0.074115       0.237074   \n5800      -0.017737       0.070347       0.256809       0.374816      -0.079692      -0.242586       0.044298      -0.281989       0.387108       0.023107       0.019294      -0.154033      -0.007949       0.009666       0.219392      -0.152540      -0.112072      -0.053614       0.355407   \n5801       0.096749       0.039959       0.247011       0.278812      -0.077872      -0.285308       0.043751      -0.157213       0.405377       0.148745      -0.055537      -0.140283      -0.049370      -0.017800       0.065636      -0.107775      -0.216198      -0.157520       0.196929   \n\n      BERTEmbed_614  BERTEmbed_615  BERTEmbed_616  BERTEmbed_617  BERTEmbed_618  BERTEmbed_619  BERTEmbed_620  BERTEmbed_621  BERTEmbed_622  BERTEmbed_623  BERTEmbed_624  BERTEmbed_625  BERTEmbed_626  BERTEmbed_627  BERTEmbed_628  BERTEmbed_629  BERTEmbed_630  BERTEmbed_631  BERTEmbed_632  \\\n0         -0.181260       0.178984       0.190001      -0.159826       0.107045      -0.238369       0.317041      -0.307818       0.065264       0.075966      -0.183574       0.121416       0.280752      -0.047025      -0.072381      -0.069134       0.102866       0.240492      -0.387272   \n1          0.021274       0.211695       0.142366      -0.200547      -0.080780      -0.200967       0.223285      -0.195979      -0.031363      -0.003380      -0.369168       0.245116       0.191714      -0.096014      -0.040704      -0.031535       0.134452       0.165713      -0.288285   \n2         -0.232542       0.121185       0.140518      -0.132441       0.078564      -0.243050       0.237742      -0.199846      -0.065695      -0.014940      -0.214697       0.142232       0.047358      -0.214465       0.066636      -0.093653       0.171615       0.161688      -0.287282   \n3          0.034823       0.071462       0.119447      -0.059441      -0.048216      -0.308524       0.334301      -0.368105      -0.035209       0.066770      -0.072121       0.173889       0.433051      -0.193934       0.040075      -0.137883       0.203224       0.358552      -0.277534   \n4         -0.184855       0.150104       0.207748      -0.278440       0.071606      -0.112504       0.321108      -0.361182       0.109440       0.031743      -0.189667       0.149004       0.292923      -0.003158      -0.080441      -0.127086       0.239413       0.244251      -0.357834   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797      -0.017738       0.064700       0.251342      -0.344373       0.064800      -0.049627       0.258808      -0.109924      -0.026413       0.071289      -0.261268       0.200874       0.076666      -0.064258       0.008165      -0.067243       0.224314       0.264509      -0.296599   \n5798      -0.058096       0.187801       0.027801      -0.321697       0.052298       0.022998       0.191597      -0.085138       0.028338       0.056786      -0.230278       0.198877       0.274416      -0.182979      -0.001415      -0.096938       0.198821       0.183084      -0.281436   \n5799       0.002784       0.092143       0.229385      -0.262094      -0.150538      -0.092701       0.318429      -0.150850      -0.155615       0.087423      -0.293090       0.104949       0.332403      -0.032639      -0.158297      -0.066522       0.215349       0.188424      -0.257978   \n5800      -0.207847       0.027902       0.155566      -0.047144       0.112756      -0.182509       0.404179      -0.145474       0.066371       0.111475      -0.147998       0.081463       0.169748      -0.195488      -0.160839      -0.012413       0.260353       0.280414      -0.403983   \n5801      -0.103377       0.036351       0.036444      -0.212506      -0.054762      -0.069451       0.182633      -0.157330      -0.063294       0.095219      -0.265767       0.235105       0.267084      -0.170242      -0.021914      -0.074998       0.136633       0.283889      -0.305920   \n\n      BERTEmbed_633  BERTEmbed_634  BERTEmbed_635  BERTEmbed_636  BERTEmbed_637  BERTEmbed_638  BERTEmbed_639  BERTEmbed_640  BERTEmbed_641  BERTEmbed_642  BERTEmbed_643  BERTEmbed_644  BERTEmbed_645  BERTEmbed_646  BERTEmbed_647  BERTEmbed_648  BERTEmbed_649  BERTEmbed_650  BERTEmbed_651  \\\n0          0.182156       0.334470      -0.329235       0.112086      -0.065010       0.175637       0.071752       0.064201       0.160761       0.134399       0.268696      -0.029933      -0.601953      -0.097186       0.085533       0.322962      -0.206732       0.430094       0.407679   \n1          0.108456       0.287496      -0.222331       0.122112      -0.065364       0.172508       0.088675       0.086672       0.104378       0.189148       0.154208      -0.040203      -0.602993      -0.122736       0.062129       0.015915      -0.141395       0.319665       0.389355   \n2          0.121923       0.463174      -0.316325       0.098219      -0.018035       0.223211       0.098358       0.174359       0.250949       0.217919       0.166736       0.087708      -0.568713      -0.020482       0.159302       0.187071      -0.040462       0.267023       0.275973   \n3          0.167971       0.185265      -0.316817       0.087062      -0.064222       0.088192       0.069077       0.215891       0.078315       0.213914       0.282584       0.018605      -0.441069      -0.051702       0.212115       0.166402      -0.094944       0.459076       0.270304   \n4          0.158927       0.299974      -0.271492       0.100035      -0.115619       0.154402       0.097091       0.079012       0.139044       0.174378       0.166514      -0.037346      -0.662391      -0.031776       0.058510       0.194450      -0.159812       0.448170       0.328815   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797       0.252199       0.417744      -0.354891       0.131328      -0.193734       0.225204       0.143845       0.115912       0.131098       0.184448       0.174881      -0.017394      -0.626449      -0.041126       0.149054       0.040721      -0.166991       0.285461       0.305882   \n5798       0.267579       0.346376      -0.365949       0.143158      -0.065634      -0.006935       0.042851       0.107270       0.093657       0.067380       0.124359      -0.029758      -0.462490       0.033878       0.279373       0.090543      -0.185027       0.562772       0.211780   \n5799       0.405168       0.340767      -0.291075       0.083339      -0.212573      -0.018753       0.049122       0.186532       0.170497       0.092862       0.199120      -0.128425      -0.608483      -0.163588       0.001773       0.271846      -0.197258       0.571889       0.346139   \n5800       0.257666       0.366892      -0.332332       0.206630      -0.129587       0.262763       0.099482       0.121180       0.104501       0.206057       0.201226      -0.049926      -0.465961      -0.117912       0.020375       0.210493      -0.102061       0.412956       0.280114   \n5801       0.225190       0.294437      -0.273334       0.122757      -0.099111       0.074864       0.087805       0.110155       0.109665       0.197439       0.249975      -0.095036      -0.567671      -0.089320       0.076010       0.128530      -0.195245       0.457287       0.374000   \n\n      BERTEmbed_652  BERTEmbed_653  BERTEmbed_654  BERTEmbed_655  BERTEmbed_656  BERTEmbed_657  BERTEmbed_658  BERTEmbed_659  BERTEmbed_660  BERTEmbed_661  BERTEmbed_662  BERTEmbed_663  BERTEmbed_664  BERTEmbed_665  BERTEmbed_666  BERTEmbed_667  BERTEmbed_668  BERTEmbed_669  BERTEmbed_670  \\\n0         -0.209427      -0.170119      -0.335401       0.116572       0.210395      -0.055914      -0.043767      -0.127778       0.783382       0.140414      -0.094320       0.007408       0.014933       0.313902       0.029507      -0.076028       0.322393      -0.310212      -0.617736   \n1         -0.153609      -0.262987      -0.375414       0.186913       0.225910      -0.110922      -0.125461      -0.077291       0.735426       0.099012      -0.132591       0.154569      -0.037251       0.234324      -0.017863      -0.182162       0.221957      -0.308723      -0.368446   \n2         -0.030010      -0.413160      -0.333432       0.055706       0.272102       0.045399      -0.124057      -0.183621       0.661158       0.075592       0.154313       0.112476      -0.082439       0.266568       0.154344      -0.168504       0.196354      -0.250716      -0.597610   \n3         -0.338150      -0.292271      -0.424333       0.248754       0.135993      -0.123694      -0.162628      -0.026158       0.545900       0.068664       0.073108      -0.033413       0.153956       0.400309       0.014063      -0.144829       0.423704      -0.310697      -0.565725   \n4         -0.298211      -0.175982      -0.378731       0.157533       0.266792      -0.096635      -0.078272      -0.080815       0.670782       0.118917      -0.152896       0.005151       0.045711       0.304243       0.061555      -0.135009       0.315377      -0.285532      -0.581321   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797      -0.096526      -0.234143      -0.406645       0.096393       0.232464      -0.059252      -0.064103      -0.088494       0.674842       0.143414      -0.174938       0.089183      -0.063448       0.303750       0.029114      -0.251200       0.172916      -0.411044      -0.545882   \n5798      -0.083107      -0.174525      -0.511897       0.115093       0.170689       0.019645      -0.026293      -0.215979       0.564337       0.188991      -0.289473      -0.034662      -0.056462       0.182796      -0.011095      -0.233039       0.319495      -0.223945      -0.550600   \n5799      -0.123787      -0.153672      -0.341038       0.144858       0.083497       0.014404       0.018678      -0.058541       0.727416       0.259999      -0.198122       0.040902       0.029725       0.345610       0.094131      -0.155435       0.433426      -0.392165      -0.310089   \n5800      -0.191353      -0.287512      -0.391526       0.228060       0.285010      -0.124472      -0.063915      -0.119393       0.694497       0.036722       0.071856      -0.063877       0.001684       0.382080       0.052258      -0.149866       0.377124      -0.222215      -0.619077   \n5801      -0.136541      -0.173828      -0.324554       0.134303       0.250992      -0.064830      -0.059560      -0.053925       0.656420       0.131131      -0.170839       0.055130      -0.047512       0.317247      -0.002407      -0.277433       0.327366      -0.426331      -0.539502   \n\n      BERTEmbed_671  BERTEmbed_672  BERTEmbed_673  BERTEmbed_674  BERTEmbed_675  BERTEmbed_676  BERTEmbed_677  BERTEmbed_678  BERTEmbed_679  BERTEmbed_680  BERTEmbed_681  BERTEmbed_682  BERTEmbed_683  BERTEmbed_684  BERTEmbed_685  BERTEmbed_686  BERTEmbed_687  BERTEmbed_688  BERTEmbed_689  \\\n0          0.272213       0.149387      -0.275052      -0.143151      -0.174631       0.247282      -0.062002       0.250701       0.099866      -0.041563      -0.085862       0.078959      -0.413587       0.169517       0.537831       0.206173      -0.105326       0.083155      -0.082530   \n1          0.291514       0.145127      -0.231798      -0.232929      -0.082045       0.282466      -0.147976       0.168857       0.217745       0.022542      -0.129517      -0.009451      -0.323145       0.094201       0.472913       0.221711      -0.037085       0.097374      -0.044385   \n2          0.413501       0.179719      -0.251343      -0.246134      -0.185203       0.371252       0.209646      -0.019791       0.210080       0.084468      -0.055473       0.027057      -0.461324       0.029137       0.354453       0.143857       0.041494       0.263278       0.087657   \n3          0.243562       0.038082      -0.259731      -0.160869      -0.130605       0.002855       0.132899       0.355293       0.119231       0.083701      -0.061923      -0.121270      -0.487989       0.239034       0.340356       0.198060       0.016782       0.073750      -0.219563   \n4          0.311918       0.115056      -0.187161      -0.242095      -0.140811       0.193934       0.059399       0.276174       0.140397      -0.001556      -0.006784       0.035666      -0.385703       0.174802       0.439088       0.195584      -0.048269       0.062359      -0.203821   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797       0.261899       0.188732      -0.250089      -0.161904      -0.198985       0.323969       0.132217       0.065788       0.173405      -0.039658      -0.138034      -0.010410      -0.425527       0.048467       0.462357       0.063406       0.028772       0.064880      -0.016006   \n5798       0.251502      -0.107099      -0.157845      -0.251505      -0.106310       0.352285       0.129397       0.228652       0.096468      -0.055256      -0.077870      -0.069502      -0.413190       0.212244       0.498257       0.240471      -0.034320       0.038215      -0.125672   \n5799       0.239597       0.195762      -0.073282      -0.149105      -0.063516       0.303675       0.072645       0.361633       0.030400      -0.193240       0.012310      -0.078675      -0.331328       0.343445       0.549831       0.290103       0.043345       0.096760      -0.080240   \n5800       0.366178       0.157134      -0.270881      -0.206389      -0.166507       0.160963       0.162765       0.092605       0.019058       0.006810      -0.135174      -0.019600      -0.420353       0.131304       0.561871       0.247528       0.066850       0.127000      -0.080651   \n5801       0.316842       0.194402      -0.248386      -0.131194      -0.086684       0.263930       0.026052       0.221463       0.151201      -0.030184      -0.087996      -0.107575      -0.367033       0.280705       0.489565       0.242375       0.052886       0.073727      -0.108327   \n\n      BERTEmbed_690  BERTEmbed_691  BERTEmbed_692  BERTEmbed_693  BERTEmbed_694  BERTEmbed_695  BERTEmbed_696  BERTEmbed_697  BERTEmbed_698  BERTEmbed_699  BERTEmbed_700  BERTEmbed_701  BERTEmbed_702  BERTEmbed_703  BERTEmbed_704  BERTEmbed_705  BERTEmbed_706  BERTEmbed_707  BERTEmbed_708  \\\n0          0.049865       0.218028      -0.094545      -0.081364       0.122886      -0.019323      -0.050918       0.046577       0.033605      -0.457211      -0.032003       0.265508      -0.114494       0.387637       0.075453       0.091503      -0.005730       0.246317      -0.082467   \n1          0.273708       0.228361       0.045839      -0.050821       0.183618       0.056735       0.095231       0.019847      -0.037542      -0.378556      -0.019202       0.266377      -0.038626       0.557122      -0.158079       0.054925       0.012482       0.249959      -0.021767   \n2          0.282488       0.182306       0.018609       0.048870       0.034924       0.117454       0.048706       0.018731      -0.084871      -0.438666      -0.068246       0.267072       0.090743       0.533925       0.108869      -0.075757      -0.152011       0.196671      -0.027047   \n3          0.073500       0.137802      -0.136318      -0.084467       0.098981      -0.038492      -0.054478      -0.034997       0.102692      -0.440833      -0.030499       0.294646      -0.061308       0.546649       0.045260       0.088310      -0.202147       0.303553      -0.151507   \n4          0.059232       0.215975      -0.070030       0.029931       0.101400      -0.025347      -0.042545       0.017275      -0.082978      -0.488424       0.019588       0.249976      -0.036452       0.487762       0.067219       0.025660      -0.067392       0.196855      -0.096831   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797       0.156429       0.201508       0.110440      -0.012298      -0.048098       0.047057       0.010535      -0.011116      -0.023430      -0.418387      -0.030550       0.148495      -0.026301       0.675363      -0.014671      -0.038551      -0.187886       0.172552      -0.068385   \n5798       0.266316       0.174048      -0.006858       0.166562       0.090929       0.088613       0.105961      -0.102771       0.004453      -0.393167      -0.180170       0.278069       0.137691       0.603098       0.069573       0.072448      -0.222114       0.266283      -0.130260   \n5799       0.038742       0.273721       0.105223      -0.144940       0.151577       0.046810      -0.096304       0.025512       0.089983      -0.470928       0.032236       0.239439      -0.034409       0.438450       0.103597       0.119883      -0.125388       0.235254      -0.173017   \n5800       0.147129       0.270234      -0.103363       0.000844       0.123820       0.036597      -0.056737       0.096311       0.077604      -0.522206      -0.005860       0.406634      -0.045904       0.498449       0.011239      -0.000838      -0.099138       0.137752      -0.095763   \n5801       0.296458       0.177088       0.032822       0.088047       0.052572       0.065842       0.047565      -0.067014      -0.112564      -0.467437      -0.078100       0.287546      -0.018603       0.567209      -0.071385       0.002748      -0.144241       0.286929      -0.044578   \n\n      BERTEmbed_709  BERTEmbed_710  BERTEmbed_711  BERTEmbed_712  BERTEmbed_713  BERTEmbed_714  BERTEmbed_715  BERTEmbed_716  BERTEmbed_717  BERTEmbed_718  BERTEmbed_719  BERTEmbed_720  BERTEmbed_721  BERTEmbed_722  BERTEmbed_723  BERTEmbed_724  BERTEmbed_725  BERTEmbed_726  BERTEmbed_727  \\\n0         -0.414326      -0.102766       0.559930      -0.024724       0.069124       0.234209      -0.104877      -0.267793       0.224398       0.196256      -0.106226       0.374463      -0.290358      -0.203963      -0.403375      -0.059965       0.462064       0.067607       0.144920   \n1         -0.337370      -0.160150       0.533608       0.011663       0.311485       0.421899       0.063289      -0.120987       0.239355       0.316279      -0.104481       0.417814      -0.309106      -0.254129      -0.275178       0.121118       0.447513       0.079621       0.206817   \n2         -0.203506      -0.059581       0.464162      -0.085020       0.263243       0.351539      -0.040017      -0.324080       0.172602       0.202878      -0.053347       0.405290      -0.090488      -0.154337      -0.334477       0.019493       0.443522       0.072783       0.020109   \n3         -0.386074      -0.163417       0.432547       0.067472       0.188812       0.178715      -0.062067      -0.089850       0.120208       0.227616       0.109611       0.561448      -0.397279      -0.239775      -0.376003      -0.201369       0.439500       0.114194       0.225814   \n4         -0.394196       0.005774       0.631414      -0.001073       0.126442       0.246264      -0.059893      -0.157777       0.239462       0.335225      -0.025558       0.492118      -0.297662      -0.276724      -0.403481      -0.100482       0.420888       0.131512       0.193989   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797      -0.286752      -0.073326       0.533121      -0.075095       0.299108       0.544477      -0.002998      -0.110569       0.150285       0.332477       0.018159       0.310728      -0.305854      -0.206219      -0.377258       0.248645       0.475760       0.126378       0.095324   \n5798      -0.219240      -0.105882       0.512538       0.036792       0.316452       0.575138       0.100380      -0.168109       0.300144       0.430823      -0.069484       0.438238      -0.348733      -0.201232      -0.235494       0.105221       0.406333       0.234754       0.174787   \n5799      -0.535687       0.048290       0.673012       0.112622       0.286671       0.374532      -0.131838      -0.038065       0.176448       0.403942      -0.103302       0.261344      -0.265923      -0.165292      -0.370454       0.072731       0.403832       0.110122       0.219321   \n5800      -0.411436      -0.142437       0.615729       0.056552       0.234512       0.408369      -0.038255      -0.212576       0.263245       0.209351       0.047378       0.435127      -0.341885      -0.063315      -0.395247      -0.124030       0.500349       0.108620       0.200862   \n5801      -0.269166      -0.107868       0.433616      -0.044366       0.296187       0.462635      -0.014226      -0.194532       0.160193       0.358865      -0.085263       0.454523      -0.255431      -0.225070      -0.337939       0.007435       0.447520       0.180202       0.188728   \n\n      BERTEmbed_728  BERTEmbed_729  BERTEmbed_730  BERTEmbed_731  BERTEmbed_732  BERTEmbed_733  BERTEmbed_734  BERTEmbed_735  BERTEmbed_736  BERTEmbed_737  BERTEmbed_738  BERTEmbed_739  BERTEmbed_740  BERTEmbed_741  BERTEmbed_742  BERTEmbed_743  BERTEmbed_744  BERTEmbed_745  BERTEmbed_746  \\\n0          0.016390      -0.258372      -0.100162       0.007905      -0.272773      -0.068892      -0.164473      -0.241873       0.136297       0.032911      -0.019837      -0.354281       0.222190      -0.311203       0.001657      -0.009683      -0.200231       0.637772       0.152364   \n1         -0.173267      -0.189410      -0.181768       0.021070      -0.309448       0.005398      -0.120456      -0.362979       0.030736       0.076042      -0.013623      -0.415654       0.190638      -0.297017      -0.152379      -0.072528      -0.054599       0.602690       0.295859   \n2         -0.160150      -0.340063      -0.162504      -0.049432      -0.215553      -0.109270       0.021254      -0.339571       0.028898       0.145408      -0.137000      -0.432440       0.056058      -0.514949      -0.074441      -0.031295       0.010599       0.570636       0.187400   \n3          0.041255      -0.251847      -0.066098       0.004132      -0.292236      -0.147804      -0.104149      -0.274381       0.220314       0.156473       0.093490      -0.493545       0.354638      -0.295340      -0.016712      -0.105776      -0.135300       0.557548       0.082171   \n4         -0.014339      -0.278737      -0.025549       0.035356      -0.265374      -0.119169      -0.093115      -0.274699       0.151397       0.157824       0.024121      -0.268482       0.248347      -0.186100      -0.014369      -0.050867      -0.168469       0.689579       0.146725   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797      -0.114514      -0.334674      -0.068975      -0.014995      -0.341405      -0.134235      -0.060644      -0.264637       0.083678       0.027673      -0.095689      -0.172272       0.101961      -0.207517       0.034880      -0.026079      -0.035831       0.668570       0.324005   \n5798      -0.203285      -0.300867      -0.111074      -0.081603      -0.368640      -0.108363      -0.000391      -0.250889       0.140267       0.119072      -0.195856      -0.071644       0.209413      -0.213366      -0.021730       0.036682       0.006657       0.759058       0.209435   \n5799      -0.050819      -0.326276      -0.176302       0.094549      -0.181081      -0.145063      -0.173627      -0.119632      -0.084858       0.136829      -0.150102      -0.236805       0.193063      -0.299987       0.049597      -0.011547       0.023055       0.704938       0.103297   \n5800       0.003457      -0.393931      -0.069050       0.066490      -0.147501      -0.083016      -0.099370      -0.309697      -0.011674       0.008094      -0.136677      -0.269840       0.087978      -0.218604       0.006561       0.062727      -0.196419       0.476781       0.156788   \n5801      -0.158013      -0.330933      -0.087565       0.088043      -0.256032      -0.124658      -0.146170      -0.309005       0.141499       0.119541      -0.066728      -0.395234       0.091152      -0.270415      -0.179587      -0.007913      -0.014999       0.625769       0.284496   \n\n      BERTEmbed_747  BERTEmbed_748  BERTEmbed_749  BERTEmbed_750  BERTEmbed_751  BERTEmbed_752  BERTEmbed_753  BERTEmbed_754  BERTEmbed_755  BERTEmbed_756  BERTEmbed_757  BERTEmbed_758  BERTEmbed_759  BERTEmbed_760  BERTEmbed_761  BERTEmbed_762  BERTEmbed_763  BERTEmbed_764  BERTEmbed_765  \\\n0         -0.076765      -0.076144      -0.025430       0.400906      -0.004340       0.103831      -0.071845      -0.126130       0.241650       0.188085       0.346959       0.119001       0.176695      -0.039616       0.095833       0.033078      -0.131427       0.048515      -0.047153   \n1         -0.077529      -0.229374       0.067749       0.321622      -0.181074      -0.047573      -0.211155      -0.117754       0.300636       0.058140       0.292613       0.120544       0.155199      -0.049755       0.152939      -0.055066      -0.050144       0.104712      -0.058793   \n2         -0.214750      -0.200343      -0.051809       0.349155      -0.062885      -0.122001      -0.177467      -0.062024       0.178993       0.101019       0.186958       0.157977       0.114238       0.074561       0.136963       0.093136      -0.036314       0.200261      -0.120719   \n3         -0.191544      -0.064381      -0.145588       0.318829       0.082383       0.174325      -0.002391      -0.214820       0.254752       0.116520       0.292421       0.104102       0.227521       0.002342      -0.019536      -0.041649      -0.000390       0.102694      -0.043032   \n4         -0.092388      -0.083103       0.019245       0.283348       0.074712       0.066673       0.012640      -0.098305       0.222184       0.195031       0.363898       0.077204       0.148679      -0.043145       0.072902       0.113459      -0.039636       0.154138      -0.079439   \n...             ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n5797      -0.296785      -0.292222       0.109131       0.228367       0.048022       0.002982      -0.136173      -0.114503       0.249840       0.213267       0.283380       0.108792       0.143121       0.063068       0.114636       0.087869       0.048760       0.256539      -0.102271   \n5798      -0.150627      -0.281938       0.108604       0.084329       0.238416       0.039098      -0.122804      -0.134536       0.243362       0.451409       0.190678       0.082955       0.194071       0.091210       0.221798       0.169295      -0.040467       0.197197      -0.028993   \n5799      -0.091895      -0.375727      -0.055390       0.169564       0.190825       0.260881      -0.111916      -0.179431       0.291061       0.243255       0.362094       0.010315       0.038279       0.076084       0.139378      -0.019313      -0.025637       0.159266      -0.130484   \n5800      -0.185793      -0.044427      -0.048074       0.399349       0.035793       0.137853      -0.111256      -0.155004       0.352759       0.130666       0.344784       0.169702       0.175369      -0.121725       0.040599      -0.032955      -0.053189       0.147725      -0.009660   \n5801      -0.228505      -0.258184       0.057018       0.215825       0.065390       0.021443      -0.115814      -0.214083       0.272588       0.334561       0.198665       0.166565       0.197325       0.050572       0.129849      -0.003030      -0.006511       0.203633      -0.124874   \n\n      BERTEmbed_766  BERTEmbed_767  \n0         -0.144548      -0.027911  \n1          0.008540       0.052572  \n2         -0.027865      -0.068774  \n3          0.091723      -0.132449  \n4         -0.028031      -0.112473  \n...             ...            ...  \n5797      -0.049987       0.000990  \n5798      -0.054743       0.039319  \n5799       0.000459      -0.151011  \n5800      -0.067827      -0.001753  \n5801      -0.044585      -0.119854  \n\n[5802 rows x 768 columns]"
          },
          "execution_count": 471,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_result_pheme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "vtIMlTiMUFzS",
        "fJjwums-kI2t",
        "lqDU7M-oP54i",
        "_HtUKy5HP8X8"
      ],
      "include_colab_link": true,
      "name": "_CLF_BERT.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit (conda)",
      "name": "python388jvsc74a57bd0b3e779290c3971bcb91630500d66c3c3cecd721489b4b277b4ac4fef67ef773c"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "metadata": {
      "interpreter": {
        "hash": "b3e779290c3971bcb91630500d66c3c3cecd721489b4b277b4ac4fef67ef773c"
      }
    },
    "orig_nbformat": 2,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07a9ca0cf13f462f830bba75a4910399": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08b32242d368498daa7828b7afeb2a9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "0a75ab30a15140db902caf400674c115": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "0ab9fc781a51454ca816230c73d55538": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d25491d17b242b38c658b3ad668de1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dcbf81848f9482ea0bcb415241d13bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10b4a6adb70440168442d2748c249915": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07a9ca0cf13f462f830bba75a4910399",
            "max": 843438,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b96f5ef0297f4936bd403f02198c50b8",
            "value": 843438
          }
        },
        "1a43d70de32d4def9a1fe6966d96650b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e80b1e80add4325a6750caeab4c54b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dcbf81848f9482ea0bcb415241d13bb",
            "max": 542529064,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b19a9345a08d4154aaa3aec5d34b7a13",
            "value": 542529064
          }
        },
        "283c37bb7a4c42c2853f77a0e82c9a0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c22c266bd5c4ccea6b245e4b990971c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f62faaa6cf947f48cde0b97d5fff4d7",
              "IPY_MODEL_fb8c67a56d8b4c4986c6d815cb4c6b3e"
            ],
            "layout": "IPY_MODEL_be000548455044719249d519495e514a"
          }
        },
        "2cb51beb13da4c3688008a052ab82ee3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fa3a91087fb4213b65d8aea738b3c9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3111d583f32540d3a18bb468741ff888": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_feb22d4ea5124d13aa49a849eee755f1",
            "max": 558,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d0cbaf5fa69439e836c0a49b53b3b42",
            "value": 558
          }
        },
        "375cc6ce64ce4791835ce34cb3bc2b8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a7e702d08a4446ea50b410f2e9843ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3111d583f32540d3a18bb468741ff888",
              "IPY_MODEL_d202a9f02ebf442e88967fd5d6ddd303"
            ],
            "layout": "IPY_MODEL_ce795ec9283d46b8927ddc7dc93a2d32"
          }
        },
        "3b53ece024be4582879ecfb989e30b75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ae373acd0d6406782cdd393667a5eae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e956c2c49a7648e1911ff94c3c403c9b",
              "IPY_MODEL_ab141a8f69d54292abcd5fb01104fdd0"
            ],
            "layout": "IPY_MODEL_0d25491d17b242b38c658b3ad668de1e"
          }
        },
        "68ddd0f509094751995c794a0dd9d3b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0e7b89c3e314dec92e6d355c5b8f99e",
            "placeholder": "​",
            "style": "IPY_MODEL_6b12e2ebcfcf4088a81abd6839a1fa06",
            "value": " 843k/843k [00:00&lt;00:00, 1.27MB/s]"
          }
        },
        "69415ec2eff448f09502f6bb35dd6345": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b12e2ebcfcf4088a81abd6839a1fa06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f2c1baae7c7462091f69527d989a792": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72f3f4bbc9d740d4b4c099f8890c84c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "772efbe2ccc143f4b182348720311d9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d0cbaf5fa69439e836c0a49b53b3b42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "7f62faaa6cf947f48cde0b97d5fff4d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd7dc5791f9142138fa0265c49d4be74",
            "max": 558,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9eb0aaa6736e43c58cc44ce1504e5366",
            "value": 558
          }
        },
        "86ae2530281341bcb7038f6fc347f8b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf606c73bf674f11b1a6d6db97fc5fff",
              "IPY_MODEL_f1759ca550bf44d0a33c3d86a4347154"
            ],
            "layout": "IPY_MODEL_772efbe2ccc143f4b182348720311d9e"
          }
        },
        "9c9dce7a03fb4e9bb773a8e49b399fcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e30225cf2364a548c06b6eecc8727df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9eb0aaa6736e43c58cc44ce1504e5366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "a0e9851bf5b64d13abb59af9b7918082": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_10b4a6adb70440168442d2748c249915",
              "IPY_MODEL_e5a6031cb1d04c4abd7ec69e91eabbe8"
            ],
            "layout": "IPY_MODEL_0ab9fc781a51454ca816230c73d55538"
          }
        },
        "ab141a8f69d54292abcd5fb01104fdd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f2c1baae7c7462091f69527d989a792",
            "placeholder": "​",
            "style": "IPY_MODEL_1a43d70de32d4def9a1fe6966d96650b",
            "value": " 1.08M/1.08M [00:00&lt;00:00, 4.02MB/s]"
          }
        },
        "acbcd81cab3b42539ac733b7c19098c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af99588ee18e413ba551a3ffd68861de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "b0e7b89c3e314dec92e6d355c5b8f99e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b19a9345a08d4154aaa3aec5d34b7a13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "b19ffd9730f54aa2aa484ba1e6e418d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b96f5ef0297f4936bd403f02198c50b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "be000548455044719249d519495e514a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be973816203644c69f9ba660c08b373c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c4a9e8b9afe5416b8e229a4f078dc238",
              "IPY_MODEL_68ddd0f509094751995c794a0dd9d3b0"
            ],
            "layout": "IPY_MODEL_b19ffd9730f54aa2aa484ba1e6e418d9"
          }
        },
        "c070a13922844983ad5ae16730544deb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4a9e8b9afe5416b8e229a4f078dc238": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_acbcd81cab3b42539ac733b7c19098c1",
            "max": 843438,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0a75ab30a15140db902caf400674c115",
            "value": 843438
          }
        },
        "c6ae66940c8d4d6cb1f1a9d1e66c8a2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdf8aa309771498786713193c162b7c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce795ec9283d46b8927ddc7dc93a2d32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf606c73bf674f11b1a6d6db97fc5fff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c9dce7a03fb4e9bb773a8e49b399fcc",
            "max": 1078931,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af99588ee18e413ba551a3ffd68861de",
            "value": 1078931
          }
        },
        "d202a9f02ebf442e88967fd5d6ddd303": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69415ec2eff448f09502f6bb35dd6345",
            "placeholder": "​",
            "style": "IPY_MODEL_72f3f4bbc9d740d4b4c099f8890c84c8",
            "value": " 558/558 [00:00&lt;00:00, 1.96kB/s]"
          }
        },
        "d4ed58d884a84191a2837daa47cba485": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e80b1e80add4325a6750caeab4c54b6",
              "IPY_MODEL_d82470e0c93e40bba3a66cbe425393bb"
            ],
            "layout": "IPY_MODEL_3b53ece024be4582879ecfb989e30b75"
          }
        },
        "d82470e0c93e40bba3a66cbe425393bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c070a13922844983ad5ae16730544deb",
            "placeholder": "​",
            "style": "IPY_MODEL_9e30225cf2364a548c06b6eecc8727df",
            "value": " 543M/543M [00:12&lt;00:00, 41.8MB/s]"
          }
        },
        "dcc1de2c1c1948e98c097472cea17686": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd7dc5791f9142138fa0265c49d4be74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5a6031cb1d04c4abd7ec69e91eabbe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_375cc6ce64ce4791835ce34cb3bc2b8c",
            "placeholder": "​",
            "style": "IPY_MODEL_2fa3a91087fb4213b65d8aea738b3c9a",
            "value": " 843k/843k [00:00&lt;00:00, 6.93MB/s]"
          }
        },
        "e956c2c49a7648e1911ff94c3c403c9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdf8aa309771498786713193c162b7c5",
            "max": 1078931,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_08b32242d368498daa7828b7afeb2a9d",
            "value": 1078931
          }
        },
        "f1759ca550bf44d0a33c3d86a4347154": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cb51beb13da4c3688008a052ab82ee3",
            "placeholder": "​",
            "style": "IPY_MODEL_c6ae66940c8d4d6cb1f1a9d1e66c8a2b",
            "value": " 1.08M/1.08M [00:00&lt;00:00, 6.78MB/s]"
          }
        },
        "fb8c67a56d8b4c4986c6d815cb4c6b3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_283c37bb7a4c42c2853f77a0e82c9a0e",
            "placeholder": "​",
            "style": "IPY_MODEL_dcc1de2c1c1948e98c097472cea17686",
            "value": " 558/558 [00:00&lt;00:00, 2.38kB/s]"
          }
        },
        "feb22d4ea5124d13aa49a849eee755f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}