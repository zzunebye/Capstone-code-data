{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_valid_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5b3ccc55088e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_valid_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_valid_X' is not defined"
     ]
    }
   ],
   "source": [
    "df_valid_X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/june/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/june/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob2 import glob\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "from nltk.corpus import stopwords as stp\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import gensim\n",
    "import gensim.models.word2vec as w2v\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 60)\n",
    "pd.set_option('display.max_colwidth', 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC\n",
    "1. Data Import\n",
    "    1. Feature Extraction\n",
    "2. Text Vectorization\n",
    "    2. Word2Vec\n",
    "    3. Doc2Vec\n",
    "4. Data Export as CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def cross_val_jsons(jsonFiles, isTest):\n",
    "    # output = [json for json in jsonFiles]\n",
    "    # print(random.choice(jsonFiles))\n",
    "    if (isTest == True):\n",
    "        test = random.choice(jsonFiles)\n",
    "        jsonFiles.remove(test)\n",
    "        train = [json for event in jsonFiles for json in event]\n",
    "        return train, test\n",
    "    else:\n",
    "        data = [json for event in jsonFiles for json in event]\n",
    "        return data\n",
    "\n",
    "def extract_data(datas):\n",
    "    tweet_data_frame = []\n",
    "    data_lists = []\n",
    "    isRumorLists = []\n",
    "    for index, dataset in enumerate(datas):\n",
    "        data_list = []\n",
    "        isRumorList = []\n",
    "        count = 0 # help var\n",
    "\n",
    "        for jsonFile in dataset:\n",
    "            count+=1\n",
    "            if jsonFile.find(\"non-rumours\") == -1:\n",
    "                isRumorList.append(1)\n",
    "            else:\n",
    "                isRumorList.append(0)\n",
    "\n",
    "            with open (jsonFile, 'r') as f:\n",
    "                for l in f.readlines():\n",
    "                    if not l.strip (): # skip empty lines\n",
    "                        continue\n",
    "                    json_data = json.loads(l)\n",
    "                    # print (json_data,\"\\n\\n\")\n",
    "                    data_list.append(json_data)\n",
    "\n",
    "        isRumorLists.append(pd.DataFrame(isRumorList, columns=['isRumor']))\n",
    "        data_lists.append(data_list)\n",
    "        tweet_data_frame.append(pd.DataFrame.from_dict(data_list))\n",
    "    return data_lists, isRumorLists, tweet_data_frame\n",
    "\n",
    "def printRumor(route):\n",
    "    if route.find(\"rumours\") == -1:\n",
    "        print('non-rumors')\n",
    "    else:\n",
    "        print('rumor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalratio(tweet_text):\n",
    "    uppers = [l for l in tweet_text if l.isupper()]\n",
    "    capitalratio = len(uppers) / len(tweet_text)\n",
    "    return capitalratio \n",
    "\n",
    "def tweets2tokens(tweet_text):\n",
    "    # Tokenizing\n",
    "    urls = []\n",
    "    tokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+','', tweet_text.lower()))\n",
    "\n",
    "    # Setting url value (whether the tweet contains http link) and filter http links\n",
    "    url=0\n",
    "    for token in tokens:\n",
    "        if token.startswith('http'):\n",
    "            url=1\n",
    "    tokens = [token for token in tokens if not token.startswith('http')]\n",
    "\n",
    "    ## Stemming\n",
    "    # porter = PorterStemmer()\n",
    "    # tokens = [porter.stem(token) for token in tokens]\n",
    "\n",
    "    # Filtering Stop words\n",
    "    # from nltk.corpus import stopwords\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # tokens = [token for token in tokens if not token in stop_words]\n",
    "\n",
    "    return tokens, url\n",
    "\n",
    "def getposcount(tokens):\n",
    "    postag = []\n",
    "    poscount = {}\n",
    "    poscount['Noun']=0\n",
    "    poscount['Verb']=0\n",
    "    poscount['Adjective'] = 0\n",
    "    poscount['Pronoun']=0\n",
    "    poscount['FirstPersonPronoun']=0\n",
    "    poscount['SecondPersonPronoun']=0\n",
    "    poscount['ThirdPersonPronoun']=0\n",
    "    poscount['Adverb']=0\n",
    "    poscount['Numeral']=0\n",
    "    poscount['Conjunction_inj']=0\n",
    "    poscount['Particle']=0\n",
    "    poscount['Determiner']=0\n",
    "    poscount['Modal']=0\n",
    "    poscount['Whs']=0\n",
    "    Nouns = {'NN','NNS','NNP','NNPS'}\n",
    "    Adverbs = {'RB','RBR','RBS'}\n",
    "    Whs = {'WDT','WP','WRB'} # Composition of wh-determiner(that,what), wh-pronoun(who), wh-adverb(how)\n",
    "    Verbs={'VB','VBP','VBZ','VBN','VBG','VBD','To'}\n",
    "    first_person_pronouns=['i','I','me','my','mine','we','us','our','ours'] #'i',\n",
    "    second_person_pronouns=['you','your','yours']\n",
    "    third_person_pronouns=['he','she','it','him','her','it','his','hers','its','they','them','their','theirs']\n",
    "\n",
    "    for word in tokens:\n",
    "        w_lower=word.lower()\n",
    "        if w_lower in first_person_pronouns:\n",
    "            poscount['FirstPersonPronoun']+=1\n",
    "        elif w_lower in second_person_pronouns:\n",
    "            poscount['SecondPersonPronoun']+=1\n",
    "        elif w_lower in third_person_pronouns:\n",
    "            poscount['ThirdPersonPronoun']+=1\n",
    "    \n",
    "    postag = nltk.pos_tag(tokens)\n",
    "    for g1 in postag:\n",
    "        if g1[1] in Nouns:\n",
    "            poscount['Noun'] += 1\n",
    "        elif g1[1] in Verbs:\n",
    "            poscount['Verb']+= 1\n",
    "        elif g1[1]=='ADJ'or g1[1]=='JJ':\n",
    "            poscount['Adjective']+=1\n",
    "        elif g1[1]=='PRP' or g1[1]=='PRON' or g1[1]=='PRP$':\n",
    "            poscount['Pronoun']+=1\n",
    "        elif g1[1] in Adverbs or g1[1]=='ADV':\n",
    "            poscount['Adverb']+=1\n",
    "        elif g1[1]=='CD':\n",
    "            poscount['Numeral']+=1\n",
    "        elif g1[1]=='CC' or g1[1]=='IN':\n",
    "            poscount['Conjunction_inj']+=1\n",
    "        elif g1[1]=='RP':\n",
    "            poscount['Particle']+=1\n",
    "        elif g1[1]=='MD':\n",
    "            poscount['Modal']+=1\n",
    "        elif g1[1]=='DT':\n",
    "            poscount['Determiner']+=1\n",
    "        elif g1[1] in Whs:\n",
    "            poscount['Whs']+=1\n",
    "    return poscount\n",
    "\n",
    "def contentlength(words):\n",
    "    wordcount = len(words)\n",
    "    return wordcount\n",
    "\n",
    "def extract_urls(entities_dicts):\n",
    "    if len(entities_dicts) < 1:\n",
    "        return 0,0,0\n",
    "\n",
    "    urls = []\n",
    "    urls_expanded = []\n",
    "\n",
    "    key = 'url'\n",
    "    key2 = 'expanded_url'\n",
    "    # print(len(entities_dict))\n",
    "    for i in entities_dicts:\n",
    "        urls.append(i[key])\n",
    "        urls_expanded.append(i[key2])\n",
    "    return 1, urls, urls_expanded\n",
    "\n",
    "def flatten_tweets(tweets):\n",
    "    \"\"\" Flattens out tweet dictionaries so relevant JSON is in a top-level dictionary. \"\"\"\n",
    "    tweets_list = []\n",
    "    total_tokens_l = []\n",
    "\n",
    "    # Iterate through each tweet\n",
    "    for tweet_obj in tweets:\n",
    "        output_f = dict()\n",
    "\n",
    "        output_f['text']= tweet_obj['text']\n",
    "        \n",
    "        urls_dicts = tweet_obj['entities']['urls']\n",
    "        # print(urls_dicts)\n",
    "\n",
    "        output_f['hasURL'], output_f['urls'], output_f['urls_expanded'] = extract_urls(urls_dicts)\n",
    "        \n",
    "        # print(type(tweet_obj['user']))\n",
    "        # print(tweet_obj['user'].contains_key('entities'))\n",
    "        if ('url' in tweet_obj['user']):\n",
    "            output_f['hasUserURL'] = 1\n",
    "            output_f['user_url'] = tweet_obj['user']['url']\n",
    "        elif ('entities' in tweet_obj['user']):\n",
    "            # output_f['user_entity'] = tweet_obj['user']['entities']['url']['urls']\n",
    "            # print(tweet_obj['user']['entities']['url']['urls'])\n",
    "            # output_f['user_url'] = tweet_obj['user']['entities']['expanded_url']\n",
    "            output_f['hasUserURL'] , _ , output_f['user_url'] = extract_urls(tweet_obj['user']['entities']['url']['urls'])\n",
    "        else:\n",
    "            # output_f['user_entity'] = None\n",
    "            output_f['user_url'] = 0\n",
    "            output_f['hasUserURL'] = 0\n",
    "\n",
    "        output_f['text_token'], output_f['isNotOnlyText'] = tweets2tokens(tweet_obj['text'])\n",
    "        total_tokens_l.extend(output_f['text_token']) # append the tokens to list of total tokens\n",
    "\n",
    "        '''POS Tagging'''\n",
    "        pos_dict=getposcount(output_f['text_token'])\n",
    "        output_f.update(pos_dict)\n",
    "\n",
    "        output_f['char_count'] = len(output_f['text'])\n",
    "        output_f['word_count'] = len(output_f['text_token'])\n",
    "\n",
    "        output_f['has_question'] = \"?\" in output_f[\"text\"]\n",
    "        output_f['has_exclaim'] = \"!\" in output_f[\"text\"]\n",
    "        output_f['has_period'] = \".\" in output_f[\"text\"]\n",
    "    \n",
    "        ''' User info'''\n",
    "        # Store the user screen name in 'user-screen_name'\n",
    "        # output_f['user-screen_name'] = tweet_obj['user']['screen_name']\n",
    "        \n",
    "        # Store the user location\n",
    "        # output_f['user-location'] = tweet_obj['user']['location']\n",
    "\n",
    "        acc_created = datetime.strptime(tweet_obj['user']['created_at'], '%a %b %d %H:%M:%S %z %Y')\n",
    "        tweet_created = datetime.strptime(tweet_obj['created_at'], '%a %b %d %H:%M:%S %z %Y')\n",
    "        age = (tweet_created - acc_created)\n",
    "        # print(type(timedelta.total_seconds(age)))\n",
    "\n",
    "        output_f['capital_ratio']=(capitalratio(tweet_obj['text']))\n",
    "\n",
    "        # features=(capitalratio(data_list[0]['user']))\n",
    "        output_f['tweet_count'] = np.log10(tweet_obj['user']['statuses_count'])\n",
    "        output_f['listed_count'] = np.log10(tweet_obj['user']['listed_count'])\n",
    "        output_f['follow_ratio'] = np.log10(tweet_obj['user']['followers_count'])\n",
    "        output_f['age'] = int(timedelta.total_seconds(age)/86400)\n",
    "        output_f['verified'] = tweet_obj['user']['verified']\n",
    "\n",
    "        tweets_list.append(output_f)\n",
    "\n",
    "    unk_tokens_l = list(set(total_tokens_l))\n",
    "    print(\"Number of total tokens appeared: {}\\nNumber of unique tokens appeared: {}\\n\".format(len(total_tokens_l), len(unk_tokens_l))) # number of tokens and unique tokens\n",
    "\n",
    "    return tweets_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing JSON Files and grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHEME dataset of social media rumours\n",
    "\n",
    "Used in:\n",
    "- Arkaitz Zubiaga, Maria Liakata, Rob Procter, Kalina Bontcheva, Peter Tolmie. Crowdsourcing the Annotation of Rumourous Conversations in Social Media. WWW Companion. 2015.\n",
    "   http://www.zubiaga.org/publications/files/www2015-crowdsourcing.pdf\n",
    "\n",
    "- Arkaitz Zubiaga, Maria Liakata, Rob Procter, Geraldine Wong Sak Hoi, Peter Tolmie. Analysing How People Orient to and Spread Rumours in Social Media by Looking at Conversational Threads. PLoS ONE 11(3): e0150989. 2016. http://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0150989\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Data of Root tweets) Train: 352 Test: 71\n",
      "(Data of Root tweets) Train_y: 352 Test_y: 71\n"
     ]
    }
   ],
   "source": [
    "charliehebdo_jsons = glob('../pheme-rumour-scheme-dataset/threads/en/charliehebdo/**/source-tweets/*.json') \n",
    "ferguson_jsons = glob('../pheme-rumour-scheme-dataset/threads/en/ferguson/**/source-tweets/*.json')\n",
    "germanwing_scrash_jsons = glob('../pheme-rumour-scheme-dataset/threads/en/germanwings-crash/**/source-tweets/*.json')\n",
    "ottawashooting_jsons = glob('../pheme-rumour-scheme-dataset/threads/en/ottawashooting/**/source-tweets/*.json')\n",
    "sydneysiege_jsons = glob('../pheme-rumour-scheme-dataset/threads/en/sydneysiege/**/source-tweets/*.json')\n",
    "putinmissing_jsons = glob('../pheme-rumour-scheme-dataset/threads/en/putinmissing/**/source-tweets/*.json')\n",
    "prince_toronto_jsons = glob('../pheme-rumour-scheme-dataset/threads/en/prince-toronto/**/source-tweets/*.json')\n",
    "ebolaessien_jsons = glob('../pheme-rumour-scheme-dataset/threads/en/ebola-essien/**/source-tweets/*.json')\n",
    "\n",
    "annotated_files = [charliehebdo_jsons, ferguson_jsons, germanwing_scrash_jsons, ottawashooting_jsons, sydneysiege_jsons, gurlitt_jsons, ebolaessien_jsons, putinmissing_jsons]\n",
    "\n",
    "# tweet_data_frame = [] # data_lists = [] # isRumorLists = []\n",
    "train, test = cross_val_jsons(annotated_files, True)\n",
    "data_lists, isRumorLists, tweet_data_frame = extract_data([train, test[0:100]])\n",
    "train, test = data_lists[0], data_lists[1]\n",
    "df_train_y, df_test_y = isRumorLists[0],isRumorLists[1]\n",
    "print(\"(Data of Root tweets) Train: {} Test: {}\".format(len(train),len(test)))\n",
    "print(\"(Data of Root tweets) Train_y: {} Test_y: {}\".format(len(isRumorLists[0]),len(isRumorLists[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHEME dataset for Rumour Detection and Veracity Classification - extension\n",
    "\n",
    "Used in:\n",
    "- 'All-in-one: Multi-task Learning for Rumour Verification'. (https://github.com/kochkinaelena/Multitask4Veracity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19275\n"
     ]
    }
   ],
   "source": [
    "all_jsons = glob('../PHEME/all-rnr-annotated-threads/**/**/source-tweets/*.json') \n",
    "\n",
    "# gurlitt_jsons = glob('../PHEME/all-rnr-annotated-threads/gurlitt-all-rnr-threads/**/source-tweets/*.json')\n",
    "# ebolaessien_jsons = glob('../PHEME/all-rnr-annotated-threads/ebola-essien-all-rnr-threads/**/source-tweets/*.json')\n",
    "# putinmissing_jsons = glob('../PHEME/all-rnr-annotated-threads/putinmissing-all-rnr-threads/**/source-tweets/*.json')\n",
    "# added_files = [gurlitt_jsons, ebolaessien_jsons, putinmissing_jsons]\n",
    "\n",
    "print(len(all_jsons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "charliehebdo_jsons = glob('../PHEME/all-rnr-annotated-threads/charliehebdo-all-rnr-threads/**/source-tweets/*.json') \n",
    "ferguson_jsons = glob('../PHEME/all-rnr-annotated-threads/ferguson-all-rnr-threads/**/source-tweets/*.json')\n",
    "germanwing_scrash_jsons = glob('../PHEME/all-rnr-annotated-threads/germanwings-crash-all-rnr-threads/**/source-tweets/*.json')\n",
    "ottawashooting_jsons = glob('../PHEME/all-rnr-annotated-threads/ottawashooting-all-rnr-threads/**/source-tweets/*.json')\n",
    "sydneysiege_jsons = glob('../PHEME/all-rnr-annotated-threads/sydneysiege-all-rnr-threads/**/source-tweets/*.json')\n",
    "gurlitt_jsons = glob('../PHEME/all-rnr-annotated-threads/gurlitt-all-rnr-threads/**/source-tweets/*.json')\n",
    "ebolaessien_jsons = glob('../PHEME/all-rnr-annotated-threads/ebola-essien-all-rnr-threads/**/source-tweets/*.json')\n",
    "putinmissing_jsons = glob('../PHEME/all-rnr-annotated-threads/putinmissing-all-rnr-threads/**/source-tweets/*.json')\n",
    "\n",
    "# annotated_files = [charliehebdo_jsons, ferguson_jsons, germanwing_scrash_jsons, ottawashooting_jsons, sydneysiege_jsons, gurlitt_jsons, ebolaessien_jsons, putinmissing_jsons]\n",
    "added_files = [gurlitt_jsons, ebolaessien_jsons, putinmissing_jsons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Data of Root tweets) Train: 390 Test: 390\n"
     ]
    }
   ],
   "source": [
    "# tweet_data_frame = [] # data_lists = [] # isRumorLists = []\n",
    "valid_data = cross_val_jsons(added_files, False)\n",
    "data_lists, isRumorLists, tweet_data_frame = extract_data([valid_data])\n",
    "X_valid = data_lists[0]\n",
    "y_valid = isRumorLists[0]\n",
    "print(\"(Data of Root tweets) Train: {} Test: {}\".format(len(X_valid),len(y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten and extract basic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total tokens appeared: 5637\n",
      "Number of unique tokens appeared: 1516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_valid_X = pd.DataFrame(flatten_tweets(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = pd.concat([df_valid_X,y_valid],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid[['has_question', 'has_exclaim', 'has_period','verified']] = df_valid[['has_question', 'has_exclaim', 'has_period','verified']].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling inf and NaN value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9174988197099399\n",
      "Before fill: Does the dataset contain NaN value? True\n",
      "After fill: Does the dataset contain NaN value? False\n"
     ]
    }
   ],
   "source": [
    "for dataset in [df_valid]:\n",
    "    dataset['listed_count'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    print(dataset['listed_count'].mean())\n",
    "    print(\"Before fill: Does the dataset contain NaN value? {}\".format(np.any(np.isnan(dataset['listed_count']))))\n",
    "    dataset['listed_count'].fillna(0,inplace=True)\n",
    "    print(\"After fill: Does the dataset contain NaN value? {}\".format(np.any(np.isnan(dataset['listed_count']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328, 34)"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid.drop(df_valid[df_valid['word_count'] < 10].index, inplace=True)\n",
    "df_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Word Vector model and Vectorize the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_ = w2v.Word2Vec.load('w2v_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = word2vec_.wv\n",
    "vocabs = word_vectors.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vectors = word_vectors.vectors # here you load vectors for each word in your model\n",
    "w2v_indices = {word: word_vectors.vocab[word].index for word in word_vectors.vocab} # here you load indices - with whom you can find an index of the particular word in your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec_.most_similar('liniers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(line): \n",
    "    words = []\n",
    "    \n",
    "    for word in line: # line - iterable, for example list of tokens \n",
    "        try:\n",
    "            w2v_idx = w2v_indices[word]\n",
    "        except KeyError: # if you does not have a vector for this word in your w2v model, continue \n",
    "            continue\n",
    "        words.append(list(w2v_vectors[w2v_idx]))\n",
    "        if not word:\n",
    "            words.append(None)\n",
    "\n",
    "        if len(line) > len(words):\n",
    "            continue\n",
    "    return np.asarray(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average of Vectors & Previous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328, 35)\n",
      "(328, 35)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "df_valid['text_token_vec'] = copy.deepcopy(df_valid['text_token'])\n",
    "print(df_valid.shape)\n",
    "\n",
    "for index, sentence in enumerate(df_valid['text_token_vec']):\n",
    "    df_valid['text_token_vec'][index] = vectorize(sentence).mean(axis=0)\n",
    "print(df_valid.shape)\n",
    "\n",
    "\n",
    "# df_valid_avg = pd.DataFrame(df_valid_avg.values.tolist()).add_prefix('token_avg') #.join(df)\n",
    "# df_valid_avg = pd.DataFrame(df_valid_avg.token_avg0.tolist())\n",
    "\n",
    "# df_valid[['text_token','text_token_vec']].head()\n",
    "avg = pd.DataFrame(df_valid['text_token_vec'].values.tolist()).add_prefix('token_avg') #.join(df)\n",
    "\n",
    "df_valid_avg = df_valid.join(avg).drop('text_token_vec',axis=1)\n",
    "df_valid.drop(['text_token_vec'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text                     0\n",
      "hasURL                   0\n",
      "urls                     0\n",
      "urls_expanded            0\n",
      "hasUserURL               0\n",
      "user_url               119\n",
      "text_token               0\n",
      "isNotOnlyText            0\n",
      "Noun                     0\n",
      "Verb                     0\n",
      "Adjective                0\n",
      "Pronoun                  0\n",
      "FirstPersonPronoun       0\n",
      "SecondPersonPronoun      0\n",
      "ThirdPersonPronoun       0\n",
      "Adverb                   0\n",
      "Numeral                  0\n",
      "Conjunction_inj          0\n",
      "Particle                 0\n",
      "Determiner               0\n",
      "Modal                    0\n",
      "Whs                      0\n",
      "char_count               0\n",
      "word_count               0\n",
      "has_question             0\n",
      "has_exclaim              0\n",
      "has_period               0\n",
      "capital_ratio            0\n",
      "tweet_count              0\n",
      "listed_count             0\n",
      "follow_ratio             0\n",
      "age                      0\n",
      "verified                 0\n",
      "isRumor                  0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_valid.isna().sum())\n",
    "# df_valid.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer the document vectors from trained Doc2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc2vec_model = Doc2Vec.load('./d2v_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid['text_token_doc'] = copy.deepcopy(df_valid['text_token'])\n",
    "\n",
    "for index, sentence in enumerate(df_valid['text_token_doc']):\n",
    "    df_valid['text_token_doc'][index] = Doc2vec_model.infer_vector(df_valid['text_token_doc'][index],steps=50)\n",
    "\n",
    "# df_test_X[['text_token','text_token_doc']].head()\n",
    "doc = pd.DataFrame(df_valid['text_token_doc'].values.tolist()).add_prefix('doc_vec')\n",
    "\n",
    "df_valid_doc = df_valid.join(doc).drop('text_token_doc',axis=1)\n",
    "# df_train_X_doc2.drop('text_token_doc', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X train: df_train_X -> df_train_X_doc | df_train_X_avg -> 둘다 text/text_token을 드랍해야함\n",
    "# y train: df_train_y\n",
    "# X test: df_test_X -> df_test_X_doc | df_test_X_avg\n",
    "# y test: df_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_avg.to_csv('./valid_avg.csv', index = False) # '''Word2Vec Averaged Vector Feature set'''\n",
    "df_valid_doc.to_csv('./valid_doc.csv', index = False) # '''Doc2vec Vector Feature set'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''Export data without word vector'''\n",
    "# data = pd.concat([df_train_X, df_test_X], ignore_index=True)\n",
    "# data2 = pd.concat([df_train_y, df_test_y], ignore_index=True)\n",
    "\n",
    "# data.to_csv('./X_basic.csv', index = False)\n",
    "# data2.to_csv('./y_basic.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rosetta",
   "language": "python",
   "name": "rosetta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}