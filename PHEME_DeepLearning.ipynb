{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On top of the Embeddedings gained from Bertweet, I added simple Neural network to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheme = pd.read_csv('./data/_PHEME_text_AVGw2v.csv').drop(['token'],axis=1)\n",
    "pheme_y = pd.read_csv('./data/_PHEME_target.csv').target\n",
    "pheme_event = pd.read_csv('./data/_PHEME_text.csv').Event\n",
    "\n",
    "ext = pd.read_csv('./data/_PHEMEext_text_AVGw2v.csv').drop(['token'],axis=1)\n",
    "ext_y = pd.read_csv('./data/_PHEMEext_text.csv').target\n",
    "ext_event = pd.read_csv('./data/_PHEMEext_text.csv').Event\n",
    "\n",
    "rhi = pd.read_csv('./data/_RHI_text_AVGw2v.csv').drop(['token'],axis=1)\n",
    "rhi_y = pd.read_csv('./data/_RHI_target.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data with BERTweet Embedding consists of 797 Dimensions: which are 767 Embeddings and 30 additional features\n",
    "\n",
    "### Other Dataset consists of 328 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Train and test data of Averaged Word2Vec: (5802, 200)/(5227, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the Train and test data of Averaged Word2Vec: {}/{}\".format(data.shape, test_data.shape))\n",
    "# print(\"Shape of the Train and test data of Doc2vec: {}/{}\".format(train_doc.shape, test_doc.shape))\n",
    "# print(\"\\nShape of the validation data of Avg: {}\".format(valid_avg.shape))\n",
    "# print(\"Shape of the validation data of Doc2vec: {}\".format(valid_doc.shape))\n",
    "# print(\"\\nShape of the data w/ BERTweet: {}\".format(df_bertweet.shape))\n",
    "# print(\"\\nShape of the validation data w/ BERTweet: {}\".format(df_valid_bertweet.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5802, 1) (5227, 1)\n"
     ]
    }
   ],
   "source": [
    "# Dropping Target values from the dataset\n",
    "# train_y = train_avg.isRumor\n",
    "# test_y = test_avg.isRumor\n",
    "# valid_y = valid_avg.isRumor\n",
    "# df_bertweet_y = df_bertweet.isRumor\n",
    "# df_valid_bertweet_y = df_valid_bertweet.isRumor\n",
    "# for dataset in [train_avg, test_avg, train_doc, test_doc, valid_avg, valid_doc, df_bertweet, df_valid_bertweet]:\n",
    "#     dataset.drop(['isRumor'], axis=1, inplace=True)\n",
    "\n",
    "# print(train.shape, test_y.shape, valid_y.shape, df_bertweet_y.shape, df_valid_bertweet_y.shape)\n",
    "print(y.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Base dataset are the baseline feature set to be inputted to the model\n",
    "# # Here, 4 features are dropped for their lack of predictive power\n",
    "\n",
    "# train_avg_base = train_avg.drop(['hasURL', 'hasUserURL', 'isNotOnlyText', 'char_count'],axis=1)\n",
    "# test_avg_base = test_avg.drop(['hasURL', 'hasUserURL', 'isNotOnlyText', 'char_count'],axis=1)\n",
    "# valid_avg_base = valid_avg.drop(['hasURL', 'hasUserURL', 'isNotOnlyText', 'char_count'],axis=1)\n",
    "# train_doc_base = train_doc.drop(['hasURL', 'hasUserURL', 'isNotOnlyText', 'char_count'],axis=1)\n",
    "# test_doc_base = test_doc.drop(['hasURL', 'hasUserURL', 'isNotOnlyText', 'char_count'],axis=1)\n",
    "# valid_doc_base = valid_doc.drop(['hasURL', 'hasUserURL', 'isNotOnlyText', 'char_count'],axis=1)\n",
    "# bertweet_base = df_bertweet.drop(['hasURL', 'hasUserURL', 'isNotOnlyText', 'char_count'],axis=1)\n",
    "# bertweet_valid_base = df_valid_bertweet.drop(['hasURL', 'hasUserURL', 'isNotOnlyText', 'char_count'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test-split을 이용한 기존 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "# # 위의 데이터들을 torchTensor로 변환한뒤 Unsqueeze한다.\n",
    "# # 이후 TensorDataset를 생성한다. (X, y 값을 담은 텐서들을 인자로 넘겨줌)\n",
    "\n",
    "# # tensor_x = torch.Tensor(np.array(data))\n",
    "# # tensor_y = torch.Tensor(np.array(y))\n",
    "# tensor_x = torch.tensor(data.values)\n",
    "# tensor_y = torch.tensor(y.values)\n",
    "\n",
    "# tensor_x = tensor_x.unsqueeze(1)\n",
    "# # tensor_y = tensor_y.unsqueeze(1)\n",
    "\n",
    "# data_dataset = TensorDataset(tensor_x,tensor_y)\n",
    "\n",
    "# 훈련 데이터 / 테스트 데이터를 torch.utils.data.random_split()를 통해서 나눠준다\n",
    "input_len = len(task1_dataset)\n",
    "test_ratio = 0.1\n",
    "test_size = int(input_len * test_ratio)\n",
    "train_size = input_len - test_size\n",
    "\n",
    "print(\"Length of the Inputs are: \",input_len, train_size, test_size)\n",
    "\n",
    "train_data, test_data = torch.utils.data.random_split(task1_dataset, (train_size, test_size))\n",
    "print(len(train_data), len(test_data))\n",
    "\n",
    "# 위에서 생성한 훈련/테스트 데이터를 각각 DataLoader를 호출해 데이터 로더를 생성한다.\n",
    "# 참고로 이 코드에서는 task1_dataset -> tensor_x/y -> train_avg_base/train_y를 사용하고 있다.\n",
    "task1_train_dataloader = DataLoader(train_data, batch_size=6, shuffle=True, num_workers=2)\n",
    "task1_test_dataloader = DataLoader(test_data, batch_size=6, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC_net을 생성 -> \n",
    "class FC_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FC_net, self).__init__() # 1*20\n",
    "        self.fc1 = nn.Linear(200, 130) # 420\n",
    "        self.fc2 = nn.Linear(130, 60)\n",
    "        self.fc3 = nn.Linear(60, 1)\n",
    "\n",
    "        self.drop_2 = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "        \n",
    "task1_model = FC_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTCLF 노트북을 참고한 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(pheme.values).float()\n",
    "val_inputs = torch.tensor(ext.values).float()\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(pheme_y.values)\n",
    "val_labels = torch.tensor(ext_y.values)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the Inputs are:\n",
      "Train\ttorch.Size([5802, 200])\n",
      "Val\ttorch.Size([485, 200])\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of the Inputs are:\\nTrain\\t%s\\nVal\\t%s\" %(train_inputs.size(), val_inputs.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class AvgW2VNet(torch.nn.Module):\n",
    "  def __init__(self, freeze_bert=True):\n",
    "    \"\"\"\n",
    "    In the constructor we construct three nn.Linear instances that we will use\n",
    "    in the forward pass.\n",
    "    \"\"\"\n",
    "    super(AvgW2VNet, self).__init__()\n",
    "\n",
    "    D_in, H, D_out = 200, 16, 2\n",
    "\n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.Linear(D_in, H),\n",
    "        nn.ReLU(),\n",
    "        # nn.Dropout(0.2),\n",
    "        nn.Linear(H, D_out),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    self.drop_2 = nn.Dropout(0.2)\n",
    "\n",
    "    if freeze_bert:\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "  def forward(self, input):\n",
    "    result = self.classifier(input)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.optim import Adam\n",
    "\n",
    "def initialize_model(epochs=200):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    model = AvgW2VNet(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    model.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    # loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    # scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "    #                                             num_warmup_steps=0, # Default value\n",
    "    #                                             num_training_steps=total_steps)\n",
    "    return model, optimizer, criterion\n",
    "    # return bert_classifier, optimizer, scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            print(logits)\n",
    "            print(b_labels)\n",
    "            loss = criterion(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)    # Set seed for reproducibility\n",
    "model, optimizer, criterion = initialize_model(epochs=100)\n",
    "# criterion = nn.BCELoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "tensor([[0.5271, 0.4975],\n",
      "        [0.5297, 0.4909],\n",
      "        [0.5199, 0.5000],\n",
      "        [0.5374, 0.4968],\n",
      "        [0.5359, 0.4928],\n",
      "        [0.5383, 0.5051],\n",
      "        [0.5366, 0.4905],\n",
      "        [0.5394, 0.4924],\n",
      "        [0.5347, 0.4994],\n",
      "        [0.5289, 0.4955],\n",
      "        [0.5479, 0.4989],\n",
      "        [0.5446, 0.4936],\n",
      "        [0.5286, 0.4958],\n",
      "        [0.5339, 0.4895],\n",
      "        [0.5302, 0.5000],\n",
      "        [0.5282, 0.4951],\n",
      "        [0.5387, 0.4988],\n",
      "        [0.5430, 0.5038],\n",
      "        [0.5220, 0.5002],\n",
      "        [0.5346, 0.4874],\n",
      "        [0.5353, 0.5009],\n",
      "        [0.5354, 0.4953],\n",
      "        [0.5405, 0.5069],\n",
      "        [0.5311, 0.4861],\n",
      "        [0.5293, 0.4981],\n",
      "        [0.5374, 0.5060],\n",
      "        [0.5326, 0.4970],\n",
      "        [0.5320, 0.4984],\n",
      "        [0.5370, 0.5031],\n",
      "        [0.5341, 0.4862],\n",
      "        [0.5260, 0.4958],\n",
      "        [0.5213, 0.4915]], grad_fn=<SigmoidBackward>)\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 1, 0, 1, 1, 0])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([32])) must be the same as input size (torch.Size([32, 2]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-a477b7f366f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-120-ca5d77f8bf9b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, val_dataloader, epochs, evaluation)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mbatch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         return F.binary_cross_entropy_with_logits(input, target,\n\u001b[0m\u001b[1;32m    630\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   2578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2580\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([32])) must be the same as input size (torch.Size([32, 2]))"
     ]
    }
   ],
   "source": [
    "train(model, train_dataloader, val_dataloader, epochs=100, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 300, 8, 2\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs.\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = AvgW2VNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "\n",
    "for t in range(500):\n",
    "  # Forward pass: Compute predicted y by passing x to the model\n",
    "  y_pred = model(x)\n",
    "\n",
    "  # Compute and print loss\n",
    "  loss = criterion(y_pred, y)\n",
    "  print(t, loss.item())\n",
    "\n",
    "  # Zero gradients, perform a backward pass, and update the weights.\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of FC_net(\n",
      "  (fc1): Linear(in_features=792, out_features=130, bias=True)\n",
      "  (fc2): Linear(in_features=130, out_features=60, bias=True)\n",
      "  (fc3): Linear(in_features=60, out_features=1, bias=True)\n",
      "  (drop_2): Dropout(p=0.2, inplace=False)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(task1_model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "# optimizer = optim.SGD(task1_model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(task1_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "train_accuracy = []\n",
    "\n",
    "prev_loss = 10\n",
    "PATH = \"./state_dict_BERT_fc.pt\"\n",
    "best_acc = 10.0\n",
    "num_epochs = 10\n",
    "\n",
    "val_corrects_list = []\n",
    "val_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "Train) Loss: 0.3190 Acc: 0.0042\n",
      "Epoch 1/9\n",
      "----------\n",
      "Train) Loss: 5.4705 Acc: 0.6325\n",
      "Epoch 2/9\n",
      "----------\n",
      "Train) Loss: 5.7149 Acc: 0.6568\n",
      "Epoch 3/9\n",
      "----------\n",
      "Train) Loss: 5.7213 Acc: 0.6568\n",
      "Epoch 4/9\n",
      "----------\n",
      "Train) Loss: 5.7149 Acc: 0.6568\n",
      "Epoch 5/9\n",
      "----------\n",
      "Train) Loss: 5.7213 Acc: 0.6568\n",
      "Epoch 6/9\n",
      "----------\n",
      "Train) Loss: 5.7149 Acc: 0.6568\n",
      "Epoch 7/9\n",
      "----------\n",
      "Train) Loss: 5.7149 Acc: 0.6568\n",
      "Epoch 8/9\n",
      "----------\n",
      "Train) Loss: 5.7213 Acc: 0.6568\n",
      "Epoch 9/9\n",
      "----------\n",
      "Train) Loss: 5.7213 Acc: 0.6568\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    task1_model.train()  # Set model to training mode\n",
    "    for i, data in enumerate(task1_train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        # inputs, labels = inputs.float(), labels.long()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = task1_model(inputs)\n",
    "\n",
    "        labels = labels.unsqueeze(1).float()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(outputs == labels.data)\n",
    "        # print(running_corrects)\n",
    "\n",
    "    epoch_loss = running_loss / train_size\n",
    "    epoch_acc = running_corrects.double() / train_size\n",
    "    train_loss.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_acc)\n",
    "\n",
    "    print('Train) Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "\n",
    "    # if epoch_loss < best_acc:\n",
    "    #     # print(\"prev_loss: {:.5f}\".format(prev_loss))\n",
    "    #     # print(\"loss: {:.5f}\".format(loss))\n",
    "    #     print(\"Saving the best model w/ loss {:.4f}\".format(epoch_loss))\n",
    "    #     torch.save(task1_model.state_dict(),PATH)\n",
    "    #     best_acc = epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the test dataset is: 68 %\n",
      "Loss of validation set: 5.20883\n"
     ]
    }
   ],
   "source": [
    "task1_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "test_loss = 0\n",
    "outputs_list = []\n",
    "y_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_loss = 0\n",
    "\n",
    "    for i, data in enumerate(task1_test_dataloader):\n",
    "        x, y = data\n",
    "        x, y = x.float(), y.long()\n",
    "        outputs = task1_model(x)\n",
    "        loss = criterion(outputs, y.unsqueeze(1).float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        outputs_list.append(predicted[:])\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).double().sum().item()\n",
    "        val_loss += loss.item()\n",
    "        y_list.append(y)\n",
    "\n",
    "print('Accuracy of the test dataset is: %d %%' % (100 * correct / total))\n",
    "print(\"Loss of validation set: {:.5f}\".format((val_loss / test_size)))\n",
    "acc = (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rosetta",
   "language": "python",
   "name": "rosetta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}