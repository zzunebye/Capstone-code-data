{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On top of the Embeddedings gained from Bertweet, I added simple Neural network to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Kernel is dead",
     "output_type": "error",
     "traceback": [
      "Error: Kernel is dead",
      "at f._sendKernelShellControl (/Users/june/.vscode-insiders/extensions/ms-toolsai.jupyter-2021.4.625511240/out/client/node_modules/@jupyterlab/services.js:3:483250)",
      "at f.sendShellMessage (/Users/june/.vscode-insiders/extensions/ms-toolsai.jupyter-2021.4.625511240/out/client/node_modules/@jupyterlab/services.js:3:483019)",
      "at f.requestExecute (/Users/june/.vscode-insiders/extensions/ms-toolsai.jupyter-2021.4.625511240/out/client/node_modules/@jupyterlab/services.js:3:485571)",
      "at w.requestExecute (/Users/june/.vscode-insiders/extensions/ms-toolsai.jupyter-2021.4.625511240/out/client/extension.js:24:209723)",
      "at _.executeCodeCell (/Users/june/.vscode-insiders/extensions/ms-toolsai.jupyter-2021.4.625511240/out/client/extension.js:49:703237)",
      "at _.execute (/Users/june/.vscode-insiders/extensions/ms-toolsai.jupyter-2021.4.625511240/out/client/extension.js:49:702911)",
      "at _.start (/Users/june/.vscode-insiders/extensions/ms-toolsai.jupyter-2021.4.625511240/out/client/extension.js:49:698873)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:97:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (/Users/june/.vscode-insiders/extensions/ms-toolsai.jupyter-2021.4.625511240/out/client/extension.js:49:712261)",
      "at async t.CellExecutionQueue.start (/Users/june/.vscode-insiders/extensions/ms-toolsai.jupyter-2021.4.625511240/out/client/extension.js:49:711801)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from __Preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/_PHEME_text_AVGw2v.csv')\n",
    "y = pd.read_csv('./data/_PHEME_target.csv')\n",
    "events = pd.read_csv('./data/_PHEME_text.csv').Event\n",
    "\n",
    "test_data = pd.read_csv('./data/_RHI_text_AVGw2v.csv')\n",
    "test_y = pd.read_csv('./data/_RHI_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0    charliehebdo\n1    charliehebdo\n2    charliehebdo\n3    charliehebdo\n4    charliehebdo\nName: Event, dtype: object"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.drop(['token'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset in [train_avg, test_avg, train_doc, test_doc, valid_avg, valid_doc, df_bertweet, df_valid_bertweet]:\n",
    "#     dataset.drop(['text','text_token','urls', 'urls_expanded','user_url'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data with BERTweet Embedding consists of 797 Dimensions: which are 767 Embeddings and 30 additional features\n",
    "\n",
    "### Other Dataset consists of 328 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Train and test data of Averaged Word2Vec: (5802, 200)/(5227, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the Train and test data of Averaged Word2Vec: {}/{}\".format(data.shape, test_data.shape))\n",
    "# print(\"Shape of the Train and test data of Doc2vec: {}/{}\".format(train_doc.shape, test_doc.shape))\n",
    "# print(\"\\nShape of the validation data of Avg: {}\".format(valid_avg.shape))\n",
    "# print(\"Shape of the validation data of Doc2vec: {}\".format(valid_doc.shape))\n",
    "# print(\"\\nShape of the data w/ BERTweet: {}\".format(df_bertweet.shape))\n",
    "# print(\"\\nShape of the validation data w/ BERTweet: {}\".format(df_valid_bertweet.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5802, 1) (5227, 1)\n"
     ]
    }
   ],
   "source": [
    "# Dropping Target values from the dataset\n",
    "# train_y = train_avg.isRumor\n",
    "# test_y = test_avg.isRumor\n",
    "# valid_y = valid_avg.isRumor\n",
    "# df_bertweet_y = df_bertweet.isRumor\n",
    "# df_valid_bertweet_y = df_valid_bertweet.isRumor\n",
    "# for dataset in [train_avg, test_avg, train_doc, test_doc, valid_avg, valid_doc, df_bertweet, df_valid_bertweet]:\n",
    "#     dataset.drop(['isRumor'], axis=1, inplace=True)\n",
    "\n",
    "# print(train.shape, test_y.shape, valid_y.shape, df_bertweet_y.shape, df_valid_bertweet_y.shape)\n",
    "print(y.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Base dataset are the baseline feature set to be inputted to the model\n",
    "# # Here, 4 features are dropped for their lack of predictive power\n",
    "\n",
    "# train_avg_base = train_avg.drop(['hasURL', 'hasUserURL', 'isNotOnlyText', 'char_count'],axis=1)\n",
    "# test_avg_base = test_avg.drop(['hasURL', 'hasUserURL', 'isNotOnlyText', 'char_count'],axis=1)\n",
    "# valid_avg_base = valid_avg.drop(['hasURL', 'hasUserURL', 'isNotOnlyText', 'char_count'],axis=1)\n",
    "# train_doc_base = train_doc.drop(['hasURL', 'hasUserURL', 'isNotOnlyText', 'char_count'],axis=1)\n",
    "# test_doc_base = test_doc.drop(['hasURL', 'hasUserURL', 'isNotOnlyText', 'char_count'],axis=1)\n",
    "# valid_doc_base = valid_doc.drop(['hasURL', 'hasUserURL', 'isNotOnlyText', 'char_count'],axis=1)\n",
    "# bertweet_base = df_bertweet.drop(['hasURL', 'hasUserURL', 'isNotOnlyText', 'char_count'],axis=1)\n",
    "# bertweet_valid_base = df_valid_bertweet.drop(['hasURL', 'hasUserURL', 'isNotOnlyText', 'char_count'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 데이터들을 torchTensor로 변환한뒤 Unsqueeze한다.\n",
    "# 이후 TensorDataset를 생성한다. (X, y 값을 담은 텐서들을 인자로 넘겨줌)\n",
    "\n",
    "# tensor_x = torch.Tensor(np.array(data))\n",
    "# tensor_y = torch.Tensor(np.array(y))\n",
    "tensor_x = torch.tensor(data.values)\n",
    "tensor_y = torch.tensor(y.values)\n",
    "\n",
    "tensor_x = tensor_x.unsqueeze(1)\n",
    "tensor_y = tensor_y.unsqueeze(1)\n",
    "\n",
    "data_dataset = TensorDataset(tensor_x,tensor_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(y_train.values)\n",
    "val_labels = torch.tensor(y_val)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the Inputs are:  5802 5222 580\n",
      "5222 580\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터 / 테스트 데이터를 torch.utils.data.random_split()를 통해서 나눠준다\n",
    "input_len = len(task1_dataset)\n",
    "test_ratio = 0.1\n",
    "test_size = int(input_len * test_ratio)\n",
    "train_size = input_len - test_size\n",
    "\n",
    "print(\"Length of the Inputs are: \",input_len, train_size, test_size)\n",
    "\n",
    "train_data, test_data = torch.utils.data.random_split(task1_dataset, (train_size, test_size))\n",
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 생성한 훈련/테스트 데이터를 각각 DataLoader를 호출해 데이터 로더를 생성한다.\n",
    "# 참고로 이 코드에서는 task1_dataset -> tensor_x/y -> train_avg_base/train_y를 사용하고 있다.\n",
    "task1_train_dataloader = DataLoader(train_data, batch_size=6, shuffle=True, num_workers=2)\n",
    "task1_test_dataloader = DataLoader(test_data, batch_size=6, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC_net을 생성 -> \n",
    "class FC_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FC_net, self).__init__() # 1*20\n",
    "        self.fc1 = nn.Linear(792, 130) # 420\n",
    "        self.fc2 = nn.Linear(130, 60)\n",
    "        self.fc3 = nn.Linear(60, 1)\n",
    "\n",
    "        self.drop_2 = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "        \n",
    "task1_model = FC_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of FC_net(\n",
      "  (fc1): Linear(in_features=792, out_features=130, bias=True)\n",
      "  (fc2): Linear(in_features=130, out_features=60, bias=True)\n",
      "  (fc3): Linear(in_features=60, out_features=1, bias=True)\n",
      "  (drop_2): Dropout(p=0.2, inplace=False)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(task1_model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "# optimizer = optim.SGD(task1_model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(task1_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "train_accuracy = []\n",
    "\n",
    "prev_loss = 10\n",
    "PATH = \"./state_dict_BERT_fc.pt\"\n",
    "best_acc = 10.0\n",
    "num_epochs = 10\n",
    "\n",
    "val_corrects_list = []\n",
    "val_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "Train) Loss: 0.3190 Acc: 0.0042\n",
      "Epoch 1/9\n",
      "----------\n",
      "Train) Loss: 5.4705 Acc: 0.6325\n",
      "Epoch 2/9\n",
      "----------\n",
      "Train) Loss: 5.7149 Acc: 0.6568\n",
      "Epoch 3/9\n",
      "----------\n",
      "Train) Loss: 5.7213 Acc: 0.6568\n",
      "Epoch 4/9\n",
      "----------\n",
      "Train) Loss: 5.7149 Acc: 0.6568\n",
      "Epoch 5/9\n",
      "----------\n",
      "Train) Loss: 5.7213 Acc: 0.6568\n",
      "Epoch 6/9\n",
      "----------\n",
      "Train) Loss: 5.7149 Acc: 0.6568\n",
      "Epoch 7/9\n",
      "----------\n",
      "Train) Loss: 5.7149 Acc: 0.6568\n",
      "Epoch 8/9\n",
      "----------\n",
      "Train) Loss: 5.7213 Acc: 0.6568\n",
      "Epoch 9/9\n",
      "----------\n",
      "Train) Loss: 5.7213 Acc: 0.6568\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    task1_model.train()  # Set model to training mode\n",
    "    for i, data in enumerate(task1_train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        # inputs, labels = inputs.float(), labels.long()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = task1_model(inputs)\n",
    "\n",
    "        labels = labels.unsqueeze(1).float()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(outputs == labels.data)\n",
    "        # print(running_corrects)\n",
    "\n",
    "    epoch_loss = running_loss / train_size\n",
    "    epoch_acc = running_corrects.double() / train_size\n",
    "    train_loss.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_acc)\n",
    "\n",
    "    print('Train) Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "\n",
    "    # if epoch_loss < best_acc:\n",
    "    #     # print(\"prev_loss: {:.5f}\".format(prev_loss))\n",
    "    #     # print(\"loss: {:.5f}\".format(loss))\n",
    "    #     print(\"Saving the best model w/ loss {:.4f}\".format(epoch_loss))\n",
    "    #     torch.save(task1_model.state_dict(),PATH)\n",
    "    #     best_acc = epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the test dataset is: 68 %\n",
      "Loss of validation set: 5.20883\n"
     ]
    }
   ],
   "source": [
    "task1_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "test_loss = 0\n",
    "outputs_list = []\n",
    "y_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_loss = 0\n",
    "\n",
    "    for i, data in enumerate(task1_test_dataloader):\n",
    "        x, y = data\n",
    "        x, y = x.float(), y.long()\n",
    "        outputs = task1_model(x)\n",
    "        loss = criterion(outputs, y.unsqueeze(1).float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        outputs_list.append(predicted[:])\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).double().sum().item()\n",
    "        val_loss += loss.item()\n",
    "        y_list.append(y)\n",
    "\n",
    "print('Accuracy of the test dataset is: %d %%' % (100 * correct / total))\n",
    "print(\"Loss of validation set: {:.5f}\".format((val_loss / test_size)))\n",
    "acc = (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rosetta",
   "language": "python",
   "name": "rosetta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}