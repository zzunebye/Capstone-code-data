{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On top of the Embeddedings gained from Bertweet, I added simple Neural network to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheme = pd.read_csv('./data/_PHEME_text_AVGw2v.csv').drop(['token'],axis=1)\n",
    "pheme_y = pd.read_csv('./data/_PHEME_target.csv').target\n",
    "pheme_event = pd.read_csv('./data/_PHEME_text.csv').Event\n",
    "\n",
    "ext = pd.read_csv('./data/_PHEMEext_text_AVGw2v.csv').drop(['token'],axis=1)\n",
    "ext_y = pd.read_csv('./data/_PHEMEext_text.csv').target\n",
    "ext_event = pd.read_csv('./data/_PHEMEext_text.csv').Event\n",
    "\n",
    "rhi = pd.read_csv('./data/_RHI_text_AVGw2v.csv').drop(['token'],axis=1)\n",
    "rhi_y = pd.read_csv('./data/_RHI_target.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data with BERTweet Embedding consists of 797 Dimensions: which are 767 Embeddings and 30 additional features\n",
    "\n",
    "### Other Dataset consists of 328 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Train and test data of Averaged Word2Vec: (5802, 200)/(5227, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the Train and test data of Averaged Word2Vec: {}/{}\".format(data.shape, test_data.shape))\n",
    "# print(\"Shape of the Train and test data of Doc2vec: {}/{}\".format(train_doc.shape, test_doc.shape))\n",
    "# print(\"\\nShape of the validation data of Avg: {}\".format(valid_avg.shape))\n",
    "# print(\"Shape of the validation data of Doc2vec: {}\".format(valid_doc.shape))\n",
    "# print(\"\\nShape of the data w/ BERTweet: {}\".format(df_bertweet.shape))\n",
    "# print(\"\\nShape of the validation data w/ BERTweet: {}\".format(df_valid_bertweet.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5802, 1) (5227, 1)\n"
     ]
    }
   ],
   "source": [
    "# Dropping Target values from the dataset\n",
    "# train_y = train_avg.isRumor\n",
    "# test_y = test_avg.isRumor\n",
    "# valid_y = valid_avg.isRumor\n",
    "# df_bertweet_y = df_bertweet.isRumor\n",
    "# df_valid_bertweet_y = df_valid_bertweet.isRumor\n",
    "# for dataset in [train_avg, test_avg, train_doc, test_doc, valid_avg, valid_doc, df_bertweet, df_valid_bertweet]:\n",
    "#     dataset.drop(['isRumor'], axis=1, inplace=True)\n",
    "\n",
    "# print(train.shape, test_y.shape, valid_y.shape, df_bertweet_y.shape, df_valid_bertweet_y.shape)\n",
    "print(y.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Base dataset are the baseline feature set to be inputted to the model\n",
    "# # Here, 4 features are dropped for their lack of predictive power\n",
    "\n",
    "# train_avg_base = train_avg.drop(['hasURL', 'hasUserURL', 'isNotOnlyText', 'char_count'],axis=1)\n",
    "# test_avg_base = test_avg.drop(['hasURL', 'hasUserURL', 'isNotOnlyText', 'char_count'],axis=1)\n",
    "# valid_avg_base = valid_avg.drop(['hasURL', 'hasUserURL', 'isNotOnlyText', 'char_count'],axis=1)\n",
    "# train_doc_base = train_doc.drop(['hasURL', 'hasUserURL', 'isNotOnlyText', 'char_count'],axis=1)\n",
    "# test_doc_base = test_doc.drop(['hasURL', 'hasUserURL', 'isNotOnlyText', 'char_count'],axis=1)\n",
    "# valid_doc_base = valid_doc.drop(['hasURL', 'hasUserURL', 'isNotOnlyText', 'char_count'],axis=1)\n",
    "# bertweet_base = df_bertweet.drop(['hasURL', 'hasUserURL', 'isNotOnlyText', 'char_count'],axis=1)\n",
    "# bertweet_valid_base = df_valid_bertweet.drop(['hasURL', 'hasUserURL', 'isNotOnlyText', 'char_count'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test-split을 이용한 기존 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "# # 위의 데이터들을 torchTensor로 변환한뒤 Unsqueeze한다.\n",
    "# # 이후 TensorDataset를 생성한다. (X, y 값을 담은 텐서들을 인자로 넘겨줌)\n",
    "\n",
    "# # tensor_x = torch.Tensor(np.array(data))\n",
    "# # tensor_y = torch.Tensor(np.array(y))\n",
    "# tensor_x = torch.tensor(data.values)\n",
    "# tensor_y = torch.tensor(y.values)\n",
    "\n",
    "# tensor_x = tensor_x.unsqueeze(1)\n",
    "# # tensor_y = tensor_y.unsqueeze(1)\n",
    "\n",
    "# data_dataset = TensorDataset(tensor_x,tensor_y)\n",
    "\n",
    "# 훈련 데이터 / 테스트 데이터를 torch.utils.data.random_split()를 통해서 나눠준다\n",
    "input_len = len(task1_dataset)\n",
    "test_ratio = 0.1\n",
    "test_size = int(input_len * test_ratio)\n",
    "train_size = input_len - test_size\n",
    "\n",
    "print(\"Length of the Inputs are: \",input_len, train_size, test_size)\n",
    "\n",
    "train_data, test_data = torch.utils.data.random_split(task1_dataset, (train_size, test_size))\n",
    "print(len(train_data), len(test_data))\n",
    "\n",
    "# 위에서 생성한 훈련/테스트 데이터를 각각 DataLoader를 호출해 데이터 로더를 생성한다.\n",
    "# 참고로 이 코드에서는 task1_dataset -> tensor_x/y -> train_avg_base/train_y를 사용하고 있다.\n",
    "task1_train_dataloader = DataLoader(train_data, batch_size=6, shuffle=True, num_workers=2)\n",
    "task1_test_dataloader = DataLoader(test_data, batch_size=6, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC_net을 생성 -> \n",
    "class FC_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FC_net, self).__init__() # 1*20\n",
    "        self.fc1 = nn.Linear(200, 130) # 420\n",
    "        self.fc2 = nn.Linear(130, 60)\n",
    "        self.fc3 = nn.Linear(60, 1)\n",
    "\n",
    "        self.drop_2 = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "        \n",
    "task1_model = FC_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTCLF 노트북을 참고한 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(pheme.values)\n",
    "val_inputs = torch.tensor(ext.values)\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(pheme_y.values)\n",
    "val_labels = torch.tensor(ext_y.values)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the Inputs are:\n",
      "Train\ttorch.Size([5802, 200])\n",
      "Val\ttorch.Size([485, 200])\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of the Inputs are:\\nTrain\\t%s\\nVal\\t%s\" %(train_inputs.size(), val_inputs.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class AvgW2VNet(torch.nn.Module):\n",
    "  def __init__(self, freeze_bert=True):\n",
    "    \"\"\"\n",
    "    In the constructor we construct three nn.Linear instances that we will use\n",
    "    in the forward pass.\n",
    "    \"\"\"\n",
    "    super(AvgW2VNet, self).__init__()\n",
    "\n",
    "    D_in, H, D_out = 768, 50, 2\n",
    "\n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.Linear(D_in, H),\n",
    "        nn.ReLU(),\n",
    "        # nn.Dropout(0.2),\n",
    "        nn.Linear(H, D_out)\n",
    "    )\n",
    "    self.drop_2 = nn.Dropout(0.2)\n",
    "\n",
    "    if freeze_bert:\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "  def forward(self, input):\n",
    "    logits = self.classifier(last_hidden_state_cls)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    classifier = AvgW2VNet(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 300, 8, 2\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs.\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = AvgW2VNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "\n",
    "for t in range(500):\n",
    "  # Forward pass: Compute predicted y by passing x to the model\n",
    "  y_pred = model(x)\n",
    "\n",
    "  # Compute and print loss\n",
    "  loss = criterion(y_pred, y)\n",
    "  print(t, loss.item())\n",
    "\n",
    "  # Zero gradients, perform a backward pass, and update the weights.\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of FC_net(\n",
      "  (fc1): Linear(in_features=792, out_features=130, bias=True)\n",
      "  (fc2): Linear(in_features=130, out_features=60, bias=True)\n",
      "  (fc3): Linear(in_features=60, out_features=1, bias=True)\n",
      "  (drop_2): Dropout(p=0.2, inplace=False)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(task1_model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "# optimizer = optim.SGD(task1_model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(task1_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "train_accuracy = []\n",
    "\n",
    "prev_loss = 10\n",
    "PATH = \"./state_dict_BERT_fc.pt\"\n",
    "best_acc = 10.0\n",
    "num_epochs = 10\n",
    "\n",
    "val_corrects_list = []\n",
    "val_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "Train) Loss: 0.3190 Acc: 0.0042\n",
      "Epoch 1/9\n",
      "----------\n",
      "Train) Loss: 5.4705 Acc: 0.6325\n",
      "Epoch 2/9\n",
      "----------\n",
      "Train) Loss: 5.7149 Acc: 0.6568\n",
      "Epoch 3/9\n",
      "----------\n",
      "Train) Loss: 5.7213 Acc: 0.6568\n",
      "Epoch 4/9\n",
      "----------\n",
      "Train) Loss: 5.7149 Acc: 0.6568\n",
      "Epoch 5/9\n",
      "----------\n",
      "Train) Loss: 5.7213 Acc: 0.6568\n",
      "Epoch 6/9\n",
      "----------\n",
      "Train) Loss: 5.7149 Acc: 0.6568\n",
      "Epoch 7/9\n",
      "----------\n",
      "Train) Loss: 5.7149 Acc: 0.6568\n",
      "Epoch 8/9\n",
      "----------\n",
      "Train) Loss: 5.7213 Acc: 0.6568\n",
      "Epoch 9/9\n",
      "----------\n",
      "Train) Loss: 5.7213 Acc: 0.6568\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    task1_model.train()  # Set model to training mode\n",
    "    for i, data in enumerate(task1_train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        # inputs, labels = inputs.float(), labels.long()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = task1_model(inputs)\n",
    "\n",
    "        labels = labels.unsqueeze(1).float()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(outputs == labels.data)\n",
    "        # print(running_corrects)\n",
    "\n",
    "    epoch_loss = running_loss / train_size\n",
    "    epoch_acc = running_corrects.double() / train_size\n",
    "    train_loss.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_acc)\n",
    "\n",
    "    print('Train) Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "\n",
    "    # if epoch_loss < best_acc:\n",
    "    #     # print(\"prev_loss: {:.5f}\".format(prev_loss))\n",
    "    #     # print(\"loss: {:.5f}\".format(loss))\n",
    "    #     print(\"Saving the best model w/ loss {:.4f}\".format(epoch_loss))\n",
    "    #     torch.save(task1_model.state_dict(),PATH)\n",
    "    #     best_acc = epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the test dataset is: 68 %\n",
      "Loss of validation set: 5.20883\n"
     ]
    }
   ],
   "source": [
    "task1_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "test_loss = 0\n",
    "outputs_list = []\n",
    "y_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_loss = 0\n",
    "\n",
    "    for i, data in enumerate(task1_test_dataloader):\n",
    "        x, y = data\n",
    "        x, y = x.float(), y.long()\n",
    "        outputs = task1_model(x)\n",
    "        loss = criterion(outputs, y.unsqueeze(1).float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        outputs_list.append(predicted[:])\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).double().sum().item()\n",
    "        val_loss += loss.item()\n",
    "        y_list.append(y)\n",
    "\n",
    "print('Accuracy of the test dataset is: %d %%' % (100 * correct / total))\n",
    "print(\"Loss of validation set: {:.5f}\".format((val_loss / test_size)))\n",
    "acc = (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rosetta",
   "language": "python",
   "name": "rosetta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}