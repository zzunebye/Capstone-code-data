{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"./data/_PHEME_text.csv\")\n",
    "data = pd.read_csv(\"./data/_PHEME_text_twtToken.csv\")\n",
    "target = pd.read_csv(\"./data/_PHEME_target.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>Event</th>\n      <th>token</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BREAKING: Armed man takes hostage in kosher gr...</td>\n      <td>charliehebdo</td>\n      <td>['breaking', 'armed', 'man', 'takes', 'hostage...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>#CharlieHebdo killers dead, confirmed by genda...</td>\n      <td>charliehebdo</td>\n      <td>['#', 'killers', 'dead', 'confirmed', 'gendarm...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Top French cartoonists Charb, Cabu, Wolinski, ...</td>\n      <td>charliehebdo</td>\n      <td>['top', 'french', 'cartoonists', 'charb', 'cab...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Police have surrounded the area where the #Cha...</td>\n      <td>charliehebdo</td>\n      <td>['police', 'surrounded', 'area', '#', 'attack'...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>PHOTO: Armed gunmen face police officers near ...</td>\n      <td>charliehebdo</td>\n      <td>['photo', 'armed', 'gunmen', 'face', 'police',...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                                text         Event  \\\n0  BREAKING: Armed man takes hostage in kosher gr...  charliehebdo   \n1  #CharlieHebdo killers dead, confirmed by genda...  charliehebdo   \n2  Top French cartoonists Charb, Cabu, Wolinski, ...  charliehebdo   \n3  Police have surrounded the area where the #Cha...  charliehebdo   \n4  PHOTO: Armed gunmen face police officers near ...  charliehebdo   \n\n                                               token  \n0  ['breaking', 'armed', 'man', 'takes', 'hostage...  \n1  ['#', 'killers', 'dead', 'confirmed', 'gendarm...  \n2  ['top', 'french', 'cartoonists', 'charb', 'cab...  \n3  ['police', 'surrounded', 'area', '#', 'attack'...  \n4  ['photo', 'armed', 'gunmen', 'face', 'police',...  "
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from csv import reader,writer\n",
    "import operator as op\n",
    "import string\n",
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reader - tfidf - knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'charliehebdo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9b7576b0acb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabstract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0minstitute_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstitute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0mscore_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mabstract\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabstract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'charliehebdo'"
     ]
    }
   ],
   "source": [
    "# #Read data from corpus\n",
    "r = reader(open(\"./data/_PHEME_text_twtToken.csv\",'r'))\n",
    "abstract_list = []\n",
    "score_list = []\n",
    "institute_list = []\n",
    "row_count = 0\n",
    "for row in list(r)[1:]:\n",
    "    institute,score,abstract = row[0], row[1], row[2]\n",
    "    if len(abstract.split()) > 0:\n",
    "      institute_list.append(institute)\n",
    "      score = float(score)\n",
    "      score_list.append(score)\n",
    "      abstract = abstract.translate(string.punctuation).lower()\n",
    "      abstract_list.append(abstract)\n",
    "      row_count = row_count + 1\n",
    "\n",
    "# print(\"Total processed data: \", row_count)\n",
    "\n",
    "# #Vectorize (TF-IDF, ngrams 1-4, no stop words) using sklearn -->\n",
    "# vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,4),\n",
    "#                      min_df = 0, stop_words = 'english', sublinear_tf=True)\n",
    "# response = vectorizer.fit_transform(abstract_list)\n",
    "# classes = score_list\n",
    "# feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# clf = neighbors.KNeighborsRegressor(n_neighbors=1)\n",
    "# clf.fit(response, classes)\n",
    "# clf.predict(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                        ngram_range=(1, 2), \n",
    "                        stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list(data['text'])\n",
    "rumor = list(target['target'])\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "            min_df=0.0,\n",
    "            analyzer=\"char\",\n",
    "            sublinear_tf=True,\n",
    "            ngram_range=(1,3),\n",
    "            max_features=5000\n",
    "        )\n",
    "\n",
    "X = vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF from BERT_Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/june/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Uncomment to download \"stopwords\"\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def text_preprocessing(s):\n",
    "    \"\"\"\n",
    "    - Lowercase the sentence\n",
    "    - Change \"'t\" to \"not\"\n",
    "    - Remove \"@name\"\n",
    "    - Isolate and remove punctuations except \"?\"\n",
    "    - Remove other special characters\n",
    "    - Remove stop words except \"not\" and \"can\"\n",
    "    - Remove trailing whitespace\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    # Change 't to 'not'\n",
    "    s = re.sub(r\"\\'t\", \" not\", s)\n",
    "    # Remove @name\n",
    "    s = re.sub(r'(@.*?)[\\s]', ' ', s)\n",
    "    # Isolate and remove punctuations except '?'\n",
    "    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', s)\n",
    "    s = re.sub(r'[^\\w\\s\\?]', ' ', s)\n",
    "    # Remove some special characters\n",
    "    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n",
    "    # Remove stopwords except 'not' and 'can'\n",
    "    s = \" \".join([word for word in s.split()\n",
    "                  if word not in stopwords.words('english')\n",
    "                  or word in ['not', 'can']])\n",
    "    # Remove trailing whitespace\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/_PHEME_text_twtToken.csv\")\n",
    "target = pd.read_csv(\"./data/_PHEME_target.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val =\\\n",
    "    train_test_split(data, target, test_size=0.1, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 438 ms, sys: 73 ms, total: 511 ms\n",
      "Wall time: 623 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Preprocess text\n",
    "tokens = list(data['token'])\n",
    "rumor = list(target['target'])\n",
    "X_train_preprocessed = X_train['token']\n",
    "X_val_preprocessed = X_val['token']\n",
    "# X_train_preprocessed = np.array([text_preprocessing(text) for text in X_train])\n",
    "# X_val_preprocessed = np.array([text_preprocessing(text) for text in X_val])\n",
    "\n",
    "# Calculate TF-IDF\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                         binary=True,\n",
    "                         smooth_idf=False)\n",
    "X_train_tfidf = tf_idf.fit_transform(X_train_preprocessed)\n",
    "X_val_tfidf = tf_idf.transform(X_val_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5221,)\n",
      "(5221, 64758)\n"
     ]
    }
   ],
   "source": [
    "print(X_train['token'].shape)\n",
    "print(X_train_tfidf.shape) # 훈련 데이터를 tfidf score로 표현한 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "def get_auc_CV(model):\n",
    "    \"\"\"\n",
    "    Return the average AUC score from cross-validation.\n",
    "    \"\"\"\n",
    "    # Set KFold to shuffle data before the split\n",
    "    kf = StratifiedKFold(5, shuffle=True, random_state=1)\n",
    "\n",
    "    # Get AUC scores\n",
    "    auc = cross_val_score(\n",
    "        model, X_train_tfidf, y_train, scoring=\"roc_auc\", cv=kf)\n",
    "\n",
    "    return auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "res = pd.Series([get_auc_CV(MultinomialNB(i))\n",
    "                 for i in np.arange(1, 10, 0.1)],\n",
    "                index=np.arange(1, 10, 0.1))\n",
    "\n",
    "best_alpha = np.round(res.idxmax(), 2)\n",
    "print('Best alpha: ', best_alpha)\n",
    "\n",
    "plt.plot(res)\n",
    "plt.title('AUC vs. Alpha')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('AUC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted probabilities\n",
    "\n",
    "# X_data = pd.concat([X_train_tfidf, X_val_tfidf], axis=0)\n",
    "# y_data = pd.concat([X_train_tfidf, X_val_tfidf], axis=0)\n",
    "\n",
    "X_test = test_data.text.values\n",
    "y_test = test_data.isRumor.values\n",
    "\n",
    "X_preprocessed = np.array([text_preprocessing(text) for text in X])\n",
    "X_tfidf = tf_idf.transform(X_preprocessed)\n",
    "X_test_preprocessed = np.array([text_preprocessing(text) for text in X_test])\n",
    "X_test_tfidf = tf_idf.transform(X_test_preprocessed)\n",
    "\n",
    "nb_model = MultinomialNB(alpha=1.8)\n",
    "nb_model.fit(X_tfidf, y)\n",
    "probs = nb_model.predict_proba(X_test_tfidf)\n",
    "\n",
    "# Evaluate the classifier\n",
    "evaluate_roc(probs, test_data.isRumor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rosetta",
   "language": "python",
   "name": "rosetta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}