{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from fetchData import fetchdata, cv_events\n",
    "import __MLP\n",
    "# from __MLP import getSamplers, convert_df_to_unsqueezed_tensor, train_sequential, clf_report\n",
    "import random\n",
    "\n",
    "import __Preprocessing\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/x.notebook.stdout": "No GPU available, using the CPU instead.\n"
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ffbfbf19a8fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))\n",
    "\n",
    "def test_data_process(X_test, y_test):\n",
    "    tensor_x1 = torch.Tensor(X_test.values).unsqueeze(1)\n",
    "    tensor_y1 = torch.Tensor(y_test.values).unsqueeze(1)\n",
    "    test_dataset = TensorDataset(tensor_x1,tensor_y1)\n",
    "\n",
    "    batch_size = 8\n",
    "\n",
    "    # train_sampler, test_sampler = __MLP.getSamplers(pheme_y, tensor_x2)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    data = next(iter(test_dataloader))\n",
    "    print(\"mean: %s, std: %s\" %(data[0].mean(), data[0].std()))\n",
    "\n",
    "    test_size = int(tensor_y1.size(0))\n",
    "\n",
    "    print(\"Test Size\",test_size)\n",
    "\n",
    "    # predict_batch\n",
    "    return test_dataloader, test_size\n",
    "\n",
    "\n",
    "def test_data_process(X_test, y_test):\n",
    "\n",
    "    tensor_x1 = torch.Tensor(X_test.values).unsqueeze(1)\n",
    "    tensor_y1 = torch.Tensor(y_test.values).unsqueeze(1)\n",
    "    test_dataset = TensorDataset(tensor_x1,tensor_y1)\n",
    "\n",
    "    batch_size = 8\n",
    "\n",
    "    # train_sampler, test_sampler = __MLP.getSamplers(pheme_y, tensor_x2)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    data = next(iter(test_dataloader))\n",
    "    print(\"mean: %s, std: %s\" %(data[0].mean(), data[0].std()))\n",
    "\n",
    "    test_size = int(tensor_y1.size(0))\n",
    "\n",
    "    print(\"Test Size\",test_size)\n",
    "\n",
    "    # predict_batch\n",
    "    return test_dataloader, test_size\n",
    "def predict(model, criterion, val_dataloader, val_size):\n",
    "    model.eval()\n",
    "    val_label_list = []\n",
    "    # val_preds_list = []\n",
    "    running_val_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        f1_running = 0\n",
    "        for j, val in enumerate(val_dataloader, 0):\n",
    "            val_x, val_label = val\n",
    "            val_x, val_label = val_x.float(), val_label.float()\n",
    "            val_outputs = model(val_x)\n",
    "            val_preds = val_outputs.squeeze(1) > 0.0\n",
    "            f1_running += (f1_score(val_label, val_preds,zero_division=True) * val_x.size(0))\n",
    "            v_loss = criterion(val_outputs, val_label.unsqueeze(1))\n",
    "            val_loss += (v_loss.item() * val_x.size(0))\n",
    "            val_corrects += torch.sum(val_preds == val_label)\n",
    "            val_label_list.append(val_label)\n",
    "            running_val_preds.append(val_preds)\n",
    "\n",
    "    running_val_preds = torch.cat(running_val_preds, 0)\n",
    "    val_label_list = torch.cat(val_label_list, 0)\n",
    "    val_corrects = val_corrects\n",
    "    val_loss = val_loss/val_size\n",
    "    val_acc = val_corrects.double().numpy() / val_size\n",
    "    f1_running /= val_size\n",
    "    print(\"accuracy_score:\\t\\t%.4f\" % val_acc)\n",
    "    print('Precision Score:\\t%.4f' % precision_score(val_label_list,running_val_preds))\n",
    "    print('Recall Score:\\t\\t%.4f' % recall_score(val_label_list,running_val_preds))\n",
    "    print(\"f1_score:\\t\\t%.4f\" % f1_running)\n",
    "    print(\"Test_loss:\\t\\t%.4f\" % val_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final\n",
    "pheme_sparse_final = pd.read_csv('./data/_PHEME_sparse.csv')\n",
    "pheme_y = pd.read_csv('./data/_PHEME_target.csv').target\n",
    "pheme_pos_final = pd.read_csv('./data/_PHEME_postags.csv')\n",
    "pheme_thread_final_avg = pd.read_csv('./data/_PHEME_thread_avg.csv')\n",
    "pheme_thread_final_std = pd.read_csv('./data/_PHEME_thread_std.csv')\n",
    "\n",
    "ext_pos_final = pd.read_csv('./data/_PHEMEext_postags.csv')\n",
    "ext_sparse_final = pd.read_csv('./data/_PHEMEext_sparse.csv')\n",
    "ext_y = pd.read_csv('./data/_PHEMEext_text.csv').target\n",
    "ext_thread_final_avg = pd.read_csv('./data/_PHEMEext_thread_avg.csv')\n",
    "ext_thread_final_std = pd.read_csv('./data/_PHEMEext_thread_std.csv')\n",
    "\n",
    "pheme_bert_simple_normal = pd.read_csv('./data/_PHEME_Bert_final_brackets.csv')\n",
    "pheme_bert_brackets_normal = pd.read_csv('./data/_PHEME_Bert_final_brackets_nrmzd.csv')\n",
    "ext_bert_simple_normal = pd.read_csv('./data/_PHEMEext_Bert_final_brackets.csv')\n",
    "ext_bert_brackets_normal = pd.read_csv('./data/_PHEMEext_Bert_final_brackets_nrmzd.csv')\n",
    "\n",
    "pheme_event = pd.read_csv('./data/_PHEME_text.csv')['Event']\n",
    "ext_event = pd.read_csv('./data/_PHEMEext_text.csv').Event\n",
    "pheme_AVGw2v = pd.read_csv('./data/_PHEME_text_AVGw2v.csv').drop(['token'],axis=1)\n",
    "ext_AVGw2v = pd.read_csv('./data/_PHEMEext_text_AVGw2v.csv').drop(['token'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pheme_sparse_final (5802, 28)\n",
      "Index(['emoji_count', 'URLcount', 'has_media', 'Skepticism', 'MentionCount',\n",
      "       'FirstPersonPronoun', 'SecondPersonPronoun', 'ThirdPersonPronoun',\n",
      "       'test_auxiliary', 'test_tentat', 'test_certain', 'Numeral',\n",
      "       'char_count', 'word_count', 'HashTag', 'has_question', 'has_exclaim',\n",
      "       'has_period', 'capital_ratio', 'retweet_count', 'tweet_count',\n",
      "       'listed_count', 'friends_count', 'follower_count', 'followers/friend',\n",
      "       'favourites_count', 'account_age_days', 'verified'],\n",
      "      dtype='object') \n",
      "\n",
      "pheme_pos_final (5802, 21)\n",
      "{('#', 'Hashtag'), ('t', 'verb incl. copula, auxiliaries'), (',', 'punctuation'), ('!', 'Interjection'), ('a', 'adjectivedeterminerother'), ('n', 'pre/postposition/subordinating conjunction'), ('d', 'nominal + verbal'), ('^', 'proper noun'), ('o', 'adverb'), ('p', 'nominal + possessive'), ('l', 'pronoun'), ('&', 'coordinating conjunction'), ('@', 'at-mention'), ('u', 'existential there, predeterminers'), ('x', 'discourse marker'), ('r', 'verb particle'), ('s', 'URL or email'), ('v', 'proper noun + possessive'), ('g', 'common noun')} \n",
      "\n",
      "pheme_thread_final (5802, 52)\n",
      "Index(['depth', 'SUM FriendsCount', 'AVG FriendsCount', 'AVG WordCount',\n",
      "       'SUM WordCount', 'AVG CharCount', 'AVG HashTag', 'SUM HashTag',\n",
      "       'Ratio HashTag', 'SUM Url', 'AVG Url', 'RATIO Url', 'SUM Mention',\n",
      "       'AVG Mention', 'Ratio Mention', 'AVG Statues', 'AVG Listed',\n",
      "       'AVG Follower', 'AVG followers/friend', 'AVG favorite', 'Tweets Count',\n",
      "       'Ratio Verified', 'SUM Verified', 'SUM RT', 'AVG RT', 'AVG AccAge',\n",
      "       'thread_time', 'AVG Emoji', 'RATIO Emoji', 'Ratio Media',\n",
      "       'RATIO Question', 'RATIO Exclaim', 'RATIO Period', 'AVG FPP', 'AVG SPP',\n",
      "       'AVG TPP', 'AVG Skepticism', 'Ratio Skepticism', 'test_auxiliary',\n",
      "       'test_tentat', 'test_certain', 'root_user_ratio', 'unique_user_ratio',\n",
      "       'unique_user_sum', 'NodeThreadCount_0', 'NodeThreadCount_25',\n",
      "       'NodeThreadCount_5', 'NodeThreadCount_75', 'NodeThreadRatio_0',\n",
      "       'NodeThreadRatio_25', 'NodeThreadRatio_5', 'NodeThreadRatio_75'],\n",
      "      dtype='object') \n",
      "\n",
      "ext_bert_simple_normal (485, 768)\n",
      "Index(['BERTEmbed_0', 'BERTEmbed_1', 'BERTEmbed_2', 'BERTEmbed_3',\n",
      "       'BERTEmbed_4', 'BERTEmbed_5', 'BERTEmbed_6', 'BERTEmbed_7',\n",
      "       'BERTEmbed_8', 'BERTEmbed_9',\n",
      "       ...\n",
      "       'BERTEmbed_758', 'BERTEmbed_759', 'BERTEmbed_760', 'BERTEmbed_761',\n",
      "       'BERTEmbed_762', 'BERTEmbed_763', 'BERTEmbed_764', 'BERTEmbed_765',\n",
      "       'BERTEmbed_766', 'BERTEmbed_767'],\n",
      "      dtype='object', length=768) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"pheme_sparse_final\", pheme_sparse_final.shape)\n",
    "print(pheme_sparse_final.columns, \"\\n\")\n",
    "print(\"pheme_pos_final\", pheme_pos_final.shape)\n",
    "x = zip(pheme_pos_final.columns.values, ['Interjection', 'Hashtag', 'coordinating conjunction', 'punctuation', 'at-mention', 'proper noun', 'adjective' 'determiner' 'other', 'nominal + verbal',\n",
    "    'common noun', 'pronoun', 'pre/postposition/subordinating conjunction', 'adverb', 'nominal + possessive', 'verb particle', 'URL or email', 'verb incl. copula, auxiliaries',\n",
    " 'existential there, predeterminers', 'proper noun + possessive', 'discourse marker'])\n",
    "print(set(x), \"\\n\")\n",
    "print(\"pheme_thread_final\", pheme_thread_final_avg.shape)\n",
    "print(pheme_thread_final_avg.columns, \"\\n\")\n",
    "print(\"ext_bert_simple_normal\", ext_bert_simple_normal.shape)\n",
    "print(pheme_bert_simple_normal.columns, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = cv_events(pd.concat([pheme_sparse, pheme_y, pheme_event],axis=1))\n",
    "# X = cv[3][1].drop(['target', 'Event'],axis=1)\n",
    "# y = cv[3][1].target\n",
    "# val_X = cv[3][0].drop(['target', 'Event'],axis=1)\n",
    "# val_y = cv[3][0].target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed = 42\n",
    "\n",
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataSize(tensor_x1, tensor_y1, tensor_x2, tensor_y2):\n",
    "    train_size = int(tensor_y1.size(0))\n",
    "    test_size = int(tensor_y2.size(0))\n",
    "\n",
    "    print(\"Variables)\\n\\tTrain:%s\\n\\tTest: %s\"%(tensor_x1.size(),tensor_x2.size()))\n",
    "    # print(\"\\tTargets:%s \\ %s\"%(tensor_y1.size()[0],tensor_y2.size()[0]))\n",
    "    print(\"Train Size\",train_size,\"Test Size\",test_size)\n",
    "    print()\n",
    "    return train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "   if hasattr(layer, 'reset_parameters'):\n",
    "    print(f'Reset trainable parameters of layer = {layer}')\n",
    "    layer.reset_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sparse_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(sparse_model, self).__init__()  # 1*20\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(28, 8),\n",
    "            nn.ELU(),\n",
    "            # nn.Linear(12, 8),\n",
    "            # nn.ELU(),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "        self.drop_3 = nn.Dropout(0.3)\n",
    "        self.drop_4 = nn.Dropout(0.4)\n",
    "        self.drop_2 = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv_rumor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-7a2b5efb8bf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv_pd_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_rumor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_sparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpheme_sparse_final\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpheme_event\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cv_rumor' is not defined"
     ]
    }
   ],
   "source": [
    "cv_pd_list = cv_rumor(model_sparse, pd.concat([pheme_sparse_final,pheme_event],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "233"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_process(dataset, criterion, events=pheme_event, target=pheme_y, epochs=20):\n",
    "    # cv_pd_list[?][0]은 Training cv_pd_list[?][1] Testing\n",
    "    cv_pd_list = []\n",
    "    data = pd.concat([dataset, pheme_event, pheme_y], axis=1)\n",
    "    NUM_EVENT = data.Event.unique().shape[0]\n",
    "    EVENTS = data.Event.unique()\n",
    "    results = {}\n",
    "\n",
    "    for i, d in enumerate(EVENTS):\n",
    "        df1, df2 = [x for _, x in data.groupby(data['Event'] != d)]\n",
    "        df1.reset_index(inplace=True, drop=True)\n",
    "        df2.reset_index(inplace=True, drop=True)\n",
    "        cv_pd_list.append([df2, df1])\n",
    "\n",
    "\n",
    "    for train, test in cv_pd_list:\n",
    "        print(\"Train: %s \\ Test: %s\" % (train.shape, test.shape))\n",
    "\n",
    "    print()\n",
    "\n",
    "    for index, fold in enumerate(cv_pd_list):\n",
    "\n",
    "        # DATA PREPARATION\n",
    "        train, test = fold\n",
    "        print(\"FOLD %d\\n----------------------------------------------------------------------------\" % (int(index)+1))\n",
    "        train_target = train.pop('target')\n",
    "        train.pop('Event')\n",
    "        test_target = test.pop('target')\n",
    "        test.pop('Event')\n",
    "\n",
    "        # train, test = scaleData(train, test)\n",
    "\n",
    "        tensor_x1, tensor_y1, tensor_x2, tensor_y2 = __MLP.convert_df_to_unsqueezed_tensor(\n",
    "            train.values, train_target, test.values, test_target.values)\n",
    "        train_dataset = TensorDataset(tensor_x1, tensor_y1)\n",
    "        test_dataset = TensorDataset(tensor_x2, tensor_y2)\n",
    "\n",
    "        batch_size = 16\n",
    "\n",
    "        train_sampler, test_sampler = __MLP.getSamplers(train_target, tensor_x2)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                    sampler=train_sampler, pin_memory=True, num_workers=0, worker_init_fn=_init_fn)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                    shuffle=False, pin_memory=True, num_workers=0, worker_init_fn=_init_fn)\n",
    "\n",
    "        data = next(iter(train_dataloader))\n",
    "        print(\"mean: %s, std: %s\" % (data[0].mean(), data[0].std()))\n",
    "\n",
    "        train_size, test_size = getDataSize(\n",
    "            tensor_x1, tensor_y1, tensor_x2, tensor_y2)\n",
    "\n",
    "        model = sparse_model()\n",
    "\n",
    "        # model_sparse = sparse_model()\n",
    "        # criterion = nn.BCEWithLogitsLoss()\n",
    "        # optimizer = optim.SGD(model_sparse.parameters(), lr=0.01, momentum=0.9)\n",
    "        # optimizer = optim.Adam(model_sparse.parameters(), lr=5e-5, eps=1e-8, weight_decay=1e-7)\n",
    "        optimizer = AdamW(model.parameters(),\n",
    "                        # lr=5e-5,    # Default learning rate\n",
    "                        lr=5e-5,    # Default learning rate\n",
    "                        eps=1e-8,    # Default epsilon value\n",
    "                        weight_decay=1e-6\n",
    "                        )\n",
    "\n",
    "        total_steps = len(train_dataloader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                    num_warmup_steps=0,  # Default value\n",
    "                                                    num_training_steps=total_steps)\n",
    "\n",
    "        PATH = \"./Model/sparse_fold_\"+str(index+1)+\".pt\"\n",
    "        print(f'PATH: {PATH}\\n')\n",
    "\n",
    "        training_acc = []\n",
    "        training_loss = []\n",
    "\n",
    "        # train_acc, train_loss, val_acc, val_loss_list = __MLP.train_sequential(model=model, num_epochs=epochs, patience=patience, criterion=criterion, optimizer=optimizer, scheduler=scheduler, train_loader=train_dataloader, train_size=train_size, test_loader=test_dataloader, test_size=test_size, PATH=PATH)\n",
    "\n",
    "        # Run the training loop for defined number of epochs\n",
    "        for epoch in range(0, epochs):\n",
    "\n",
    "            # Print epoch\n",
    "            print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "            # Set current loss value\n",
    "            current_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over the DataLoader for training data\n",
    "            for i, data in enumerate(train_dataloader, 0):\n",
    "\n",
    "                temp = []\n",
    "\n",
    "                # Get inputs\n",
    "                inputs, targets = data\n",
    "\n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Perform forward pass\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # temp.append(outputs)\n",
    "                # temp.append(inputs)\n",
    "                # temp.append(targets)\n",
    "\n",
    "                outputs = outputs.view(outputs.size(0), -1)\n",
    "\n",
    "                # Compute Prediction Outputs\n",
    "                # preds = outputs.squeeze(1) > 0.0\n",
    "                preds = outputs > 0.0\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == targets.data).data\n",
    "\n",
    "                # Perform backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Perform optimization and Scheduler\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                # Print statistics\n",
    "                # current_loss += loss.item() # 원본\n",
    "                # if i % len(train_dataloader) == len(train_dataloader)-1:\n",
    "                #     print('Loss after mini-batch %5d: %.3f' %\n",
    "                #           (i + 1, current_loss / i+1))\n",
    "\n",
    "                current_loss += loss.item() * inputs.size(0)\n",
    "                if i % len(train_dataloader) == len(train_dataloader)-1:\n",
    "                    print(\"Loss/ACC after mini-batch %5d: %.3f / %.4f\" %\n",
    "                        (i + 1, current_loss / train_size, running_corrects/train_size))\n",
    "\n",
    "            # temp.append(running_corrects)\n",
    "            # temp.append(running_corrects.double())\n",
    "            # epoch_acc = running_corrects.double() / train_size\n",
    "            epoch_acc = running_corrects / train_size\n",
    "            epoch_loss = running_loss / train_size\n",
    "            # temp.append(epoch_acc)\n",
    "            training_acc.append(epoch_acc)\n",
    "            training_loss.append(epoch_loss)\n",
    "            # print('Epoch {}/{}\\tTrain) Acc: {:.4f}, Loss: {:.4f}'.format(epoch+1,\n",
    "                                                                        #  epochs, epoch_acc, epoch_loss))\n",
    "        # Process is complete.\n",
    "        print('Training process has finished. Saving trained model.')\n",
    "\n",
    "        # Print about testing\n",
    "        print('**Starting TESTING**')\n",
    "\n",
    "        # Saving the model\n",
    "        # save_path = f'./model-fold-{fold}.pth'\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "\n",
    "        # Evaluation for this fold\n",
    "        correct, total = 0, 0\n",
    "        val_corrects=0\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Iterate over the test data and generate predictions\n",
    "            for i, data in enumerate(test_dataloader, 0):\n",
    "\n",
    "                # Get inputs\n",
    "                inputs, targets = data\n",
    "\n",
    "                # Generate outputs\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Set total and correct\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                outputs = outputs.view(outputs.size(0), -1)\n",
    "                preds = outputs > 0.0\n",
    "                # running_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += torch.sum(preds == targets.data).data\n",
    "\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "\n",
    "            # Print accuracy\n",
    "            print('Accuracy for fold %d: %d %%' % (index, 100.0 * correct / total))\n",
    "            print('F1 Score for fold %d: %d %%' %(index, f1_score(total, correct, zero_division=1)))\n",
    "            print('Accuracy-2 for fold %d: %d %%' % (index, 100.0 * val_corrects / total))\n",
    "            print('F1 Score-2 for fold %d: %d %%' %(index, f1_score(total, preds, zero_division=1)))\n",
    "            print('F1 Score-3 for fold %d: %d %%' %(index, f1_score(total, predicted, zero_division=1)))\n",
    "            print('--------------------------------')\n",
    "            results[index] = 100.0 * (correct / total)\n",
    "\n",
    "\n",
    "    # ---------------------------- Print fold results ---------------------------- #\n",
    "\n",
    "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {NUM_EVENT} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        sum += value\n",
    "    print(f'Average: {sum/len(results.items())} %')\n",
    "\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (3723, 30) \\ Test: (2079, 30)\n",
      "Train: (4659, 30) \\ Test: (1143, 30)\n",
      "Train: (5333, 30) \\ Test: (469, 30)\n",
      "Train: (4912, 30) \\ Test: (890, 30)\n",
      "Train: (4581, 30) \\ Test: (1221, 30)\n",
      "\n",
      "FOLD 1\n",
      "----------------------------------------------------------------------------\n",
      "mean: tensor(15906.8613), std: tensor(161537.8438)\n",
      "Variables)\n",
      "\tTrain:torch.Size([3723, 1, 28])\n",
      "\tTest: torch.Size([2079, 1, 28])\n",
      "Train Size 3723 Test Size 2079\n",
      "\n",
      "PATH: ./Model/sparse_fold_1.pt\n",
      "\n",
      "Starting epoch 1\n",
      "Loss/ACC after mini-batch   233: 14849.725 / 0.5068\n",
      "Starting epoch 2\n",
      "Loss/ACC after mini-batch   233: 13195.729 / 0.4829\n",
      "Starting epoch 3\n",
      "Loss/ACC after mini-batch   233: 9991.574 / 0.5026\n",
      "Starting epoch 4\n",
      "Loss/ACC after mini-batch   233: 8821.296 / 0.4808\n",
      "Starting epoch 5\n",
      "Loss/ACC after mini-batch   233: 9043.665 / 0.4902\n",
      "Starting epoch 6\n",
      "Loss/ACC after mini-batch   233: 8830.099 / 0.4867\n",
      "Starting epoch 7\n",
      "Loss/ACC after mini-batch   233: 6816.245 / 0.5012\n",
      "Starting epoch 8\n",
      "Loss/ACC after mini-batch   233: 6156.383 / 0.5173\n",
      "Starting epoch 9\n",
      "Loss/ACC after mini-batch   233: 5656.297 / 0.4875\n",
      "Starting epoch 10\n",
      "Loss/ACC after mini-batch   233: 5532.693 / 0.5034\n",
      "Starting epoch 11\n",
      "Loss/ACC after mini-batch   233: 5300.317 / 0.4754\n",
      "Starting epoch 12\n",
      "Loss/ACC after mini-batch   233: 4350.491 / 0.4883\n",
      "Starting epoch 13\n",
      "Loss/ACC after mini-batch   233: 4035.134 / 0.4878\n",
      "Starting epoch 14\n",
      "Loss/ACC after mini-batch   233: 3937.157 / 0.4897\n",
      "Starting epoch 15\n",
      "Loss/ACC after mini-batch   233: 4152.150 / 0.4843\n",
      "Starting epoch 16\n",
      "Loss/ACC after mini-batch   233: 3365.897 / 0.5087\n",
      "Starting epoch 17\n",
      "Loss/ACC after mini-batch   233: 3814.375 / 0.4993\n",
      "Starting epoch 18\n",
      "Loss/ACC after mini-batch   233: 3795.359 / 0.4902\n",
      "Starting epoch 19\n",
      "Loss/ACC after mini-batch   233: 3293.004 / 0.4980\n",
      "Starting epoch 20\n",
      "Loss/ACC after mini-batch   233: 3701.922 / 0.4937\n",
      "Training process has finished. Saving trained model.\n",
      "**Starting TESTING**\n",
      "Accuracy for fold 0: 77 %\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected sequence or array-like, got <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-179-8611590eca9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mcv_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpheme_event\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpheme_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-178-384e2776df67>\u001b[0m in \u001b[0;36mcv_process\u001b[0;34m(dataset, criterion, events, target, epochs)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;31m# Print accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy for fold %d: %d %%'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F1 Score for fold %d: %d %%'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy-2 for fold %d: %d %%'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mval_corrects\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F1 Score-2 for fold %d: %d %%'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[0mmodified\u001b[0m \u001b[0;32mwith\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mzero_division\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m     \"\"\"\n\u001b[0;32m-> 1068\u001b[0;31m     return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n\u001b[0m\u001b[1;32m   1069\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                        \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1190\u001b[0m     \"\"\"\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m     _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n\u001b[0m\u001b[1;32m   1193\u001b[0m                                                  \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m                                                  \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1461\u001b[0;31m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0m\u001b[1;32m   1462\u001b[0m                                     pos_label)\n\u001b[1;32m   1463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1272\u001b[0m                          str(average_options))\n\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1274\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m     \u001b[0;31m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;31m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \"\"\"\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \"\"\"\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rosetta/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected sequence or array-like, got <class 'int'>"
     ]
    }
   ],
   "source": [
    "class sparse_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(sparse_model, self).__init__()  # 1*20\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(28, 8),\n",
    "            nn.ELU(),\n",
    "            # nn.Linear(12, 8),\n",
    "            # nn.ELU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "        self.drop_3 = nn.Dropout(0.3)\n",
    "        self.drop_4 = nn.Dropout(0.4)\n",
    "        self.drop_2 = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "model = sparse_model()\n",
    "dataset = pheme_sparse_final\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "epochs = 20\n",
    "\n",
    "cv_process(dataset, criterion, events=pheme_event, target=pheme_y, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{}"
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (3723, 30) \\ Test: (2079, 30)\n",
      "Train: (4659, 30) \\ Test: (1143, 30)\n",
      "Train: (5333, 30) \\ Test: (469, 30)\n",
      "Train: (4912, 30) \\ Test: (890, 30)\n",
      "Train: (4581, 30) \\ Test: (1221, 30)\n",
      "\n",
      "FOLD 1\n",
      "----------------------------------------------------------------------------\n",
      "mean: tensor(51206.5273), std: tensor(638589.6875)\n",
      "Variables)\n",
      "\tTrain:torch.Size([3723, 1, 28])\n",
      "\tTest: torch.Size([2079, 1, 28])\n",
      "Train Size 3723 Test Size 2079\n",
      "\n",
      "PATH: ./Model/sparse_fold_1.pt\n",
      "\n",
      "Starting epoch 1\n",
      "Loss/ACC after mini-batch   233: 15368.260 / 0.4899\n",
      "Starting epoch 2\n",
      "Loss/ACC after mini-batch   233: 11438.299 / 0.5007\n",
      "Starting epoch 3\n",
      "Loss/ACC after mini-batch   233: 11925.357 / 0.5060\n",
      "Starting epoch 4\n",
      "Loss/ACC after mini-batch   233: 9597.950 / 0.5023\n",
      "Starting epoch 5\n",
      "Loss/ACC after mini-batch   233: 7925.050 / 0.5144\n",
      "Starting epoch 6\n",
      "Loss/ACC after mini-batch   233: 8651.734 / 0.5120\n",
      "Starting epoch 7\n",
      "Loss/ACC after mini-batch   233: 7384.062 / 0.5017\n",
      "Starting epoch 8\n",
      "Loss/ACC after mini-batch   233: 7047.151 / 0.5227\n",
      "Starting epoch 9\n",
      "Loss/ACC after mini-batch   233: 6876.622 / 0.5259\n",
      "Starting epoch 10\n",
      "Loss/ACC after mini-batch   233: 6769.078 / 0.5171\n",
      "Training process has finished. Saving trained model.\n",
      "**Starting TESTING**\n",
      "Accuracy for fold 0: 77 %\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "----------------------------------------------------------------------------\n",
      "mean: tensor(44724.0430), std: tensor(399370.9375)\n",
      "Variables)\n",
      "\tTrain:torch.Size([4659, 1, 28])\n",
      "\tTest: torch.Size([1143, 1, 28])\n",
      "Train Size 4659 Test Size 1143\n",
      "\n",
      "PATH: ./Model/sparse_fold_2.pt\n",
      "\n",
      "Starting epoch 1\n",
      "Loss/ACC after mini-batch   292: 18618.995 / 0.4765\n",
      "Starting epoch 2\n",
      "Loss/ACC after mini-batch   292: 15267.046 / 0.4737\n",
      "Starting epoch 3\n",
      "Loss/ACC after mini-batch   292: 11852.277 / 0.4683\n",
      "Starting epoch 4\n",
      "Loss/ACC after mini-batch   292: 11646.554 / 0.4619\n",
      "Starting epoch 5\n",
      "Loss/ACC after mini-batch   292: 9596.265 / 0.4778\n",
      "Starting epoch 6\n",
      "Loss/ACC after mini-batch   292: 8216.388 / 0.4649\n",
      "Starting epoch 7\n",
      "Loss/ACC after mini-batch   292: 7481.532 / 0.4713\n",
      "Starting epoch 8\n",
      "Loss/ACC after mini-batch   292: 6362.999 / 0.4761\n",
      "Starting epoch 9\n",
      "Loss/ACC after mini-batch   292: 6611.111 / 0.4664\n",
      "Starting epoch 10\n",
      "Loss/ACC after mini-batch   292: 5766.575 / 0.4789\n",
      "Training process has finished. Saving trained model.\n",
      "**Starting TESTING**\n",
      "Accuracy for fold 1: 75 %\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "----------------------------------------------------------------------------\n",
      "mean: tensor(75502.6172), std: tensor(1056831.)\n",
      "Variables)\n",
      "\tTrain:torch.Size([5333, 1, 28])\n",
      "\tTest: torch.Size([469, 1, 28])\n",
      "Train Size 5333 Test Size 469\n",
      "\n",
      "PATH: ./Model/sparse_fold_3.pt\n",
      "\n",
      "Starting epoch 1\n",
      "Loss/ACC after mini-batch   334: 6020.175 / 0.5440\n",
      "Starting epoch 2\n",
      "Loss/ACC after mini-batch   334: 5224.398 / 0.5368\n",
      "Starting epoch 3\n",
      "Loss/ACC after mini-batch   334: 7036.590 / 0.5693\n",
      "Starting epoch 4\n",
      "Loss/ACC after mini-batch   334: 5568.956 / 0.5717\n",
      "Starting epoch 5\n",
      "Loss/ACC after mini-batch   334: 5859.186 / 0.5682\n",
      "Starting epoch 6\n",
      "Loss/ACC after mini-batch   334: 5115.628 / 0.5667\n",
      "Starting epoch 7\n",
      "Loss/ACC after mini-batch   334: 4865.368 / 0.5727\n",
      "Starting epoch 8\n",
      "Loss/ACC after mini-batch   334: 4819.907 / 0.5697\n",
      "Starting epoch 9\n",
      "Loss/ACC after mini-batch   334: 4568.445 / 0.5593\n",
      "Starting epoch 10\n",
      "Loss/ACC after mini-batch   334: 5240.594 / 0.5689\n",
      "Training process has finished. Saving trained model.\n",
      "**Starting TESTING**\n",
      "Accuracy for fold 2: 49 %\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "----------------------------------------------------------------------------\n",
      "mean: tensor(78205.2734), std: tensor(1079964.2500)\n",
      "Variables)\n",
      "\tTrain:torch.Size([4912, 1, 28])\n",
      "\tTest: torch.Size([890, 1, 28])\n",
      "Train Size 4912 Test Size 890\n",
      "\n",
      "PATH: ./Model/sparse_fold_4.pt\n",
      "\n",
      "Starting epoch 1\n",
      "Loss/ACC after mini-batch   307: 5622.996 / 0.4988\n",
      "Starting epoch 2\n",
      "Loss/ACC after mini-batch   307: 1936.638 / 0.5232\n",
      "Starting epoch 3\n",
      "Loss/ACC after mini-batch   307: 1518.038 / 0.5440\n",
      "Starting epoch 4\n",
      "Loss/ACC after mini-batch   307: 1219.517 / 0.5509\n",
      "Starting epoch 5\n",
      "Loss/ACC after mini-batch   307: 1366.261 / 0.5460\n",
      "Starting epoch 6\n",
      "Loss/ACC after mini-batch   307: 1179.450 / 0.5434\n",
      "Starting epoch 7\n",
      "Loss/ACC after mini-batch   307: 911.131 / 0.5322\n",
      "Starting epoch 8\n",
      "Loss/ACC after mini-batch   307: 1162.978 / 0.5487\n",
      "Starting epoch 9\n",
      "Loss/ACC after mini-batch   307: 1209.817 / 0.5358\n",
      "Starting epoch 10\n",
      "Loss/ACC after mini-batch   307: 583.058 / 0.5460\n",
      "Training process has finished. Saving trained model.\n",
      "**Starting TESTING**\n",
      "Accuracy for fold 3: 47 %\n",
      "--------------------------------\n",
      "FOLD 5\n",
      "----------------------------------------------------------------------------\n",
      "mean: tensor(72604.8047), std: tensor(969858.0625)\n",
      "Variables)\n",
      "\tTrain:torch.Size([4581, 1, 28])\n",
      "\tTest: torch.Size([1221, 1, 28])\n",
      "Train Size 4581 Test Size 1221\n",
      "\n",
      "PATH: ./Model/sparse_fold_5.pt\n",
      "\n",
      "Starting epoch 1\n",
      "Loss/ACC after mini-batch   287: 24635.684 / 0.5027\n",
      "Starting epoch 2\n",
      "Loss/ACC after mini-batch   287: 11869.625 / 0.5003\n",
      "Starting epoch 3\n",
      "Loss/ACC after mini-batch   287: 1313.426 / 0.5278\n",
      "Starting epoch 4\n",
      "Loss/ACC after mini-batch   287: 479.443 / 0.5324\n",
      "Starting epoch 5\n",
      "Loss/ACC after mini-batch   287: 330.106 / 0.5115\n",
      "Starting epoch 6\n",
      "Loss/ACC after mini-batch   287: 191.158 / 0.5102\n",
      "Starting epoch 7\n",
      "Loss/ACC after mini-batch   287: 139.185 / 0.4855\n",
      "Starting epoch 8\n",
      "Loss/ACC after mini-batch   287: 115.176 / 0.4855\n",
      "Starting epoch 9\n",
      "Loss/ACC after mini-batch   287: 104.771 / 0.5023\n",
      "Starting epoch 10\n",
      "Loss/ACC after mini-batch   287: 92.828 / 0.5171\n",
      "Training process has finished. Saving trained model.\n",
      "**Starting TESTING**\n",
      "Accuracy for fold 4: 57 %\n",
      "--------------------------------\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 77.97017797017797 %\n",
      "Fold 1: 75.15310586176727 %\n",
      "Fold 2: 49.25373134328358 %\n",
      "Fold 3: 47.19101123595505 %\n",
      "Fold 4: 57.24815724815725 %\n",
      "Average: 61.36323673186822 %\n"
     ]
    }
   ],
   "source": [
    "# Class = {...}\n",
    "# model = sparse_model()\n",
    "dataset = pheme_sparse_final\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "# random.seed(42)\n",
    "# torch.backends.cudnn.deterministic=True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "#                                INSIDE FUNCTION                               #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "# cv_pd_list[?][0]은 Training cv_pd_list[?][1] Testing\n",
    "cv_pd_list = []\n",
    "data = pd.concat([dataset, pheme_event, pheme_y], axis=1)\n",
    "# results['feature'] = nameof(dataset)\n",
    "NUM_EVENT = data.Event.unique().shape[0]\n",
    "EVENTS = data.Event.unique()\n",
    "for i, d in enumerate(EVENTS):\n",
    "    df1, df2 = [x for _, x in data.groupby(data['Event'] != d)]\n",
    "    df1.reset_index(inplace=True, drop=True)\n",
    "    df2.reset_index(inplace=True, drop=True)\n",
    "    cv_pd_list.append([df2, df1])\n",
    "\n",
    "for train, test in cv_pd_list:\n",
    "    print(\"Train: %s \\ Test: %s\" % (train.shape, test.shape))\n",
    "\n",
    "print()\n",
    "\n",
    "for index, fold in enumerate(cv_pd_list):\n",
    "\n",
    "    # DATA PREPARATION\n",
    "    train, test = fold\n",
    "    print(\"FOLD %d\\n----------------------------------------------------------------------------\" % (int(index)+1))\n",
    "    train_target = train.pop('target')\n",
    "    train.pop('Event')\n",
    "    test_target = test.pop('target')\n",
    "    test.pop('Event')\n",
    "\n",
    "    # train, test = scaleData(train, test)\n",
    "\n",
    "    tensor_x1, tensor_y1, tensor_x2, tensor_y2 = __MLP.convert_df_to_unsqueezed_tensor(\n",
    "        train.values, train_target, test.values, test_target.values)\n",
    "    train_dataset = TensorDataset(tensor_x1, tensor_y1)\n",
    "    test_dataset = TensorDataset(tensor_x2, tensor_y2)\n",
    "\n",
    "    batch_size = 16\n",
    "\n",
    "    train_sampler, test_sampler = __MLP.getSamplers(train_target, tensor_x2)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                  sampler=train_sampler, pin_memory=True, num_workers=0, worker_init_fn=_init_fn)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                 shuffle=False, pin_memory=True, num_workers=0, worker_init_fn=_init_fn)\n",
    "\n",
    "    data = next(iter(train_dataloader))\n",
    "    print(\"mean: %s, std: %s\" % (data[0].mean(), data[0].std()))\n",
    "\n",
    "    train_size, test_size = getDataSize(\n",
    "        tensor_x1, tensor_y1, tensor_x2, tensor_y2)\n",
    "\n",
    "    model = sparse_model()\n",
    "\n",
    "    epochs = 10\n",
    "    patience = 20\n",
    "\n",
    "    # model_sparse = sparse_model()\n",
    "    # criterion = nn.BCEWithLogitsLoss()\n",
    "    # optimizer = optim.SGD(model_sparse.parameters(), lr=0.01, momentum=0.9)\n",
    "    # optimizer = optim.Adam(model_sparse.parameters(), lr=5e-5, eps=1e-8, weight_decay=1e-7)\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      # lr=5e-5,    # Default learning rate\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8,    # Default epsilon value\n",
    "                      weight_decay=1e-6\n",
    "                      )\n",
    "\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0,  # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "\n",
    "    PATH = \"./Model/sparse_fold_\"+str(index+1)+\".pt\"\n",
    "    print(f'PATH: {PATH}\\n')\n",
    "\n",
    "    training_acc = []\n",
    "    training_loss = []\n",
    "\n",
    "    # train_acc, train_loss, val_acc, val_loss_list = __MLP.train_sequential(model=model, num_epochs=epochs, patience=patience, criterion=criterion, optimizer=optimizer, scheduler=scheduler, train_loader=train_dataloader, train_size=train_size, test_loader=test_dataloader, test_size=test_size, PATH=PATH)\n",
    "\n",
    "    # Run the training loop for defined number of epochs\n",
    "    for epoch in range(0, epochs):\n",
    "        # Print epoch\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "        # Set current loss value\n",
    "        current_loss = 0.0\n",
    "        running_corrects = 0.0\n",
    "        running_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        # Iterate over the DataLoader for training data\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "\n",
    "            temp = []\n",
    "\n",
    "            # Get inputs\n",
    "            inputs, targets = data\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Perform forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # temp.append(outputs)\n",
    "            # temp.append(inputs)\n",
    "            # temp.append(targets)\n",
    "\n",
    "            outputs = outputs.view(outputs.size(0), -1)\n",
    "\n",
    "            # Compute Prediction Outputs\n",
    "            # preds = outputs.squeeze(1) > 0.0\n",
    "            preds = outputs > 0.0\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == targets.data).data\n",
    "\n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Perform optimization and Scheduler\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print statistics\n",
    "            # current_loss += loss.item() # 원본\n",
    "            # if i % len(train_dataloader) == len(train_dataloader)-1:\n",
    "            #     print('Loss after mini-batch %5d: %.3f' %\n",
    "            #           (i + 1, current_loss / i+1))\n",
    "\n",
    "            current_loss += loss.item() * inputs.size(0)\n",
    "            if i % len(train_dataloader) == len(train_dataloader)-1:\n",
    "                print(\"Loss/ACC after mini-batch %5d: %.3f / %.4f\" %\n",
    "                      (i + 1, current_loss / train_size, running_corrects/train_size))\n",
    "\n",
    "        # temp.append(running_corrects)\n",
    "        # temp.append(running_corrects.double())\n",
    "        # epoch_acc = running_corrects.double() / train_size\n",
    "        epoch_acc = running_corrects / train_size\n",
    "        epoch_loss = running_loss / train_size\n",
    "        # temp.append(epoch_acc)\n",
    "        training_acc.append(epoch_acc)\n",
    "        training_loss.append(epoch_loss)\n",
    "        # print('Epoch {}/{}\\tTrain) Acc: {:.4f}, Loss: {:.4f}'.format(epoch+1,\n",
    "                                                                    #  epochs, epoch_acc, epoch_loss))\n",
    "\n",
    "    # Process is complete.\n",
    "    print('Training process has finished. Saving trained model.')\n",
    "\n",
    "    # Print about testing\n",
    "    print('**Starting TESTING**')\n",
    "\n",
    "    # Saving the model\n",
    "    # save_path = f'./model-fold-{fold}.pth'\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    # Evaluation for this fold\n",
    "    correct, total = 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Iterate over the test data and generate predictions\n",
    "        for i, data in enumerate(test_dataloader, 0):\n",
    "\n",
    "            # Get inputs\n",
    "            inputs, targets = data\n",
    "\n",
    "            # Generate outputs\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Set total and correct\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "        # Print accuracy\n",
    "        print('Accuracy for fold %d: %d %%' % (index, 100.0 * correct / total))\n",
    "        print('--------------------------------')\n",
    "        results[index] = 100.0 * (correct / total)\n",
    "\n",
    "# ---------------------------- Print fold results ---------------------------- #\n",
    "\n",
    "print(f'K-FOLD CROSS VALIDATION RESULTS FOR {NUM_EVENT} FOLDS')\n",
    "print('--------------------------------')\n",
    "sum = 0.0\n",
    "for key, value in results.items():\n",
    "    print(f'Fold {key}: {value} %')\n",
    "    sum += value\n",
    "print(f'Average: {sum/len(results.items())} %')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2369.)\n",
      "tensor(2369., dtype=torch.float64)\n",
      "tensor(0.5171)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-8ac2b520dc32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(temp[0])\n",
    "print(temp[1])\n",
    "print(temp[2])\n",
    "print(temp[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 13.6247]],\n",
      "\n",
      "        [[ 85.3222]],\n",
      "\n",
      "        [[709.5609]],\n",
      "\n",
      "        [[ 86.6495]],\n",
      "\n",
      "        [[ 38.6009]]], grad_fn=<AddBackward0>)\n",
      "=> torch.Size([5, 1, 1])\n",
      "\n",
      "tensor([[ 13.6247],\n",
      "        [ 85.3222],\n",
      "        [709.5609],\n",
      "        [ 86.6495],\n",
      "        [ 38.6009]], grad_fn=<ViewBackward>)\n",
      "=> torch.Size([5, 1])\n",
      "\n",
      "tensor([[ 13.6247],\n",
      "        [ 85.3222],\n",
      "        [709.5609],\n",
      "        [ 86.6495],\n",
      "        [ 38.6009]], grad_fn=<SqueezeBackward1>)\n",
      "\n",
      "=> torch.Size([5, 1])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "            # Compute Prediction Outputs\n",
    "            preds = outputs.squeeze(1) > 0.0\n",
    "            \n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets.unsqueeze(1))\n",
    "\n",
    "예측값을 위해 아웃풋을 511 -> 51로 만들고\n",
    "loss를 위해 타겟값을 51 -> 511로 만든다. (아웃풋과 타겟값 매칭): 타겟 기본 크기는 5,1이며 아웃풋 기본 크기는 5,1,1.\n",
    "\n",
    "뷰를 사용하면 5,1로 맞춰지니, 출력되는 x를 5,1로 맞추면 outputs.squeeze(1)가 필요없어지고 타겟도 건드릴필요가 없음\n",
    "\n",
    "'''\n",
    "\n",
    "tsr = temp[0]\n",
    "print(tsr)\n",
    "print(f'=> {tsr.size()}\\n')\n",
    "print(tsr.view(tsr.size(0), -1))\n",
    "print(f'=> {tsr.view(tsr.size(0), -1).size()}\\n')\n",
    "print(f'{tsr.squeeze(1)}\\n')\n",
    "print(f'=> {tsr.squeeze(1).size()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size\n",
      "=> torch.Size([5, 1, 28])\n",
      "\n",
      "=> torch.Size([5, 28])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tsr = temp[1]\n",
    "print(f'input size')\n",
    "print(f'=> {tsr.size()}\\n')\n",
    "# print(tsr.view(tsr.size(0), -1))\n",
    "print(f'=> {tsr.view(tsr.size(0), -1).size()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target size\n",
      "=> torch.Size([5, 1])\n",
      "\n",
      "=> torch.Size([5, 1])\n",
      "\n",
      "torch.Size([5, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "tsr = temp[2]\n",
    "print(f'target size')\n",
    "print(f'=> {tsr.size()}\\n')\n",
    "# print(tsr.view(tsr.size(0), -1))\n",
    "print(f'=> {tsr.view(tsr.size(0), -1).size()}\\n')\n",
    "print(f'{tsr.unsqueeze(1).size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schdule\n",
    "- 61.36323673186822"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit (conda)",
   "name": "python388jvsc74a57bd0b3e779290c3971bcb91630500d66c3c3cecd721489b4b277b4ac4fef67ef773c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "b3e779290c3971bcb91630500d66c3c3cecd721489b4b277b4ac4fef67ef773c"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}